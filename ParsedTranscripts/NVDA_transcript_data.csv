ticker,quarter,word_count_prepared_remarks,word_count_qa_management,word_count_qa_analysts,prepared_remarks,qa_management,qa_analysts
NVDA,Q2 2017,1532,7725,1973,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the Second Quarter of FY17. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that today's call is being Webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until the 18th of August, 2016. The Webcast will be available for replay up until next quarter's Conference Call to discuss Q3 financial results. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without our prior written consent. During the course of this call, we may make forward-looking statements based on current expectations. These forward-looking statements are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All of our statements are made as of today, the 11th of August 2016, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find your reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Arnab. This quarter, we introduced our new family of Pascal-based GPUs, one of our most successful launches ever. We also benefited from both the winding adoption of deep learning and our expanding engagement with hyperscale data centers around the world as they apply deep learning to all the services they provide. Revenue continued to accelerate, rising 24% to a record $1.43 billion. We saw strong sequential and year-on-year growth across our four platforms. Gaming, professional visualization, data center, and automotive. Our business model based on driving GPU compute platforms into highly targeted markets is clearly succeeding. The GPU business was up 25% to $1.2 billion from a year ago.  The Tegra processor business increased 30% to $166 million. In Q2, our four platforms contributed nearly 89% of revenue, up from 85% a year earlier and 87% in the preceding quarter. They collectively increased 29% year-over-year. Let's begin with our gaming platform. Gaming revenue increased 18% year on year to $781 million reflecting the success of our latest integration of Pascal-based GPUs. Demand was strong in every geographic region. The Pascal architecture offers a number of amazing technological advances. It enables unprecedented performance and efficiencies for playing sophisticated AAA gaming titles and driving rich, immersive VR experiences. In our most successful launch ever, we introduced four major products. They are GeForce GTX1080, 1070, and 1060 for the enthusiast market and the Titan X, the world's fastest consumer GPU for deep learning development, digital content creation, and extreme gaming. Wired magazine called the GTX 1080 an unprecedented piece of electronic precision, one that performs Herculean feats of computational strength. Forbes called GTX1060, which brings a premium VR experience within reach of many, a fantastic product, and Hardware Canucks describes Titan X as a technological tour de force with frame rates that are simply mind-boggling. The GTX 1080, 1070, 1060, and Titan X are now in full production and available to consumers worldwide. VR's potential is on vivid display in a new, open source game that we released during the quarter. Available on Steam, NVIDIA VR Funhouse is an open source title created with our GameWorks SDK. It integrates physical simulation into VR along with amazing graphics and precise haptics that you feel like you're actually at a carnival. Moving to professional visualization, Quadro revenue grew to a record $214 million, up 22% year on year and up 13% sequentially. Growth came from the high end of the market for realtime rendering tools and mobile workstations. The M6000 GPU 24-gig launched earlier this year is drawing strong interest from a broad range of customers. Digital Domain, a leading special effects studio, is using Quadro to accelerate productivity for its work on films and commercials, which requires especially tight turnaround times. Engineering giant AECOM and the Yale school of architecture are using Quadro to accelerate their design and engineering workflows. Last month at SIGGRAPH conference, we introduced a series of new products that embed photo-realistic and immersive experience into workflows incorporating Iray a and VR. We launched the Pascal-based Quadro P6000, the most advanced workstation GPU, enabling designers to manipulate complex designs up to twice as fast as before. We demonstrated how deep learning is being brought to the realm of the industrial design to create better products faster.  And, we launched eight new and updated software libraries such as VRWorks 360 video SDK which brings panoramic video to VR. Moving to data center, revenue reached a record $151 million, more than doubling year on year and up 6% sequentially. This impressive performance reflects strong growth in supercomputing, hyperscale data centers, and grid virtualization. Interest in deep learning is surging as industries increasingly seek to harness this revolutionary technology. Hyperscale companies remain fast adopters of deep learning. Both for training and realtime inference, particularly for natural lingual processing, video, and image analysis. Among them are Facebook, Microsoft, Amazon, Ali Baba, and Bidoo.  Major cloud providers are also offering GPU computing for their customers. Microsoft Azure is now using NVIDIA's GPUs to provide computing and graphics virtualizations. During the quarter, we began shipping Tesla P100, the world's most advanced GPU accelerator based on the Pascal architecture. Designed to accelerate deep learning training, it allows application performance to scale up to 8 GPUs using our NV link interconnect. We also announced a variant of P100 based on PCI express that makes it suitable for a wide range of accelerated servers. At our GPU Technology Conference in April, we introduced DGX1, the world's first deep learning super computer.  Equipped with eight P100s in a single box, it provides deep learning performance that is equivalent to 250 traditional servers. It comes loaded with NVIDIA software and AI application developers. We are seeing strong interest in DGX1 from AR researchers and developers across academia, government labs, and large enterprises. Two days ago, Jen-Hsun hand-delivered the very first DGX1 production model to the open AI institute. They plan to use the system in part to build autonomous agents like chat box, cars, and robots.  Broader deliveries, will commence later this quarter. We will be talking more about deep learning later this year at regional versions of our GPU technology conference set for eight cities around the world. Among them Beijing, Amsterdam, Tokyo, and Seoul, as well as Washington DC. Our grid graphics virtualization business more than doubled in the quarter. Adoption is accelerating across a variety of industries, particularly automotive and AEC.  Among customers added this quarter was StatOil, a Norwegian Oil & Gas Company. Finally, in automotive, revenue increased to a record $119 million, up 68% year-over-year and up 5% sequentially, driven by premium infotainment and digital cockpit features in mainstream cars. Our effort to help partners develop self-driving cars continues to accelerate. We have started to ship our drive PX2 automotive super computer to the 80-plus companies using both our hardware and DRI work software to develop autonomous driving technologies. We remain on track to ship our autopilot solution based on the drive platform. Beyond our four platforms, our OEM and [IPA] business was $163 million, down 6% year on year in line with mainstream PC demands. Now, turning to the rest of the Income Statement, we had a record GAAP gross margins of 57.9% while non-GAAP gross margin was 58.1%. These reflect the strength of our GeForce gaming GPUs, the success of our platform approach, and strong demand for deep learning. GAAP operating expenses were $509 million, down 9% from a year earlier. Non-GAAP operating expenses were $448 million, up 6% from a year earlier. This reflects increased hiring in R&D and Marketing expenses, partially offset by lower legal fees. GAAP operating income for the Second Quarter was $317 million compared to $76 million a year earlier. Non-GAAP operating income was $382 million, up 65%. Non-GAAP operating margins improved 680 basis points from a year ago to 26.8%. Now, turning to the outlook for the third quarter of FY17,we expect revenue to be $1.68 billion, plus or minus 2%. Our GAAP and non-GAAP gross margins are expected to be 57.8% and 58%, respectively, plus or minus 50 basis points. GAAP operating expenses are expected to be approximately $530 million. Non-GAAP operating expenses are expected to be approximately $465 million. GAAP and non-GAAP tax rates for the third quarter of FY17 are both expected to be 21%, plus or minus 1%. Further financial details are included in the CFO commentary and other information available on our IR website. We will now open the call for questions. Operator, could you please poll for questions? Thank you.","Sure, Mark. Data center -- our data center business is comprised of three basic markets as you're alluding to. One of them is high-performance computing, and one could say that or characterize it as a traditional supercomputing market, very computationally intensive. A second market is grid which is our data center virtualization, basically graphics application virtualization. You could stream and serve any PC or any PC application from data center to any client device.  And, the third application is deep learning, and this is largely hyperscale data centers applying deep learning to enhance their applications to make them much smarter and much more delightful. The vast majority of the growth comes from deep learning by far, and the reason for that is because high performance computing is a relatively stable business. It's still a growing business, and I expect the high-performance computing to do quite well over the coming years. Grid is a fast-growing business.  I think Colette said that it was growing 100% year-over-year, but it's from a much smaller base, and deep learning is not only significant in size, it's also growing quite substantially. Yes, so we've talked extensively about the way we prepare for new process nodes over the last several years. For long-term NVIDIA followers, you might have recalled that 40-nanometer was a very challenging node for us.  And, with all of these challenges, it's an opportunity for us to improve our Company.  We've implemented a very rigorous process node preparation methodology, and it starts, of course, with some of the world's best process engineers, circuit design engineers, and process readiness teams.  We have a fantastic group dedicated to just getting process ready for us. And, the second part of it is how that process readiness is integrated throughout the entire Company so I'm really proud of the way that the Company executed on Pascal. 16-nanometer finfet is no trivial task, not to mention the speed of the memories that we use. It's the world's first G5x. We also ramped the world's first HBM2 memory and 3D memory stacking.  So, the number of technological challenges that we overcame in the ramp of Pascal is quite extraordinary.  I'm super-proud of the team. Going forward, we are going to continue to refine yields, and that is absolutely the case. However, we came into 16-nanometer with a great deal of preparedness, and so it's too early to guess what's going to happen to yields and margins long term.  But, we'll guide one quarter at a time. Yes, Toshi. I appreciate it. We are experiencing growth in all of our businesses. Our strategy of focusing on deep learning, self-driving cars, gaming, and virtual reality where markets -- these are markets where GPU makes a very significant difference is really paying off.  And, this quarter is really the first quarter where we saw growth across every single one of our businesses, and my expectation is that we're going to see growth across all of our businesses next quarter as well.  But, it's driven by the focus on these key markets and away from traditional commodity components businesses. I think one particular dynamic sticks out, and it's a very significant growth driver where we have an extraordinary position in.  It's deep learning. Deep learning, you may have heard is a  new computing approach. It's a new computing model and requires a new computing architecture, and this is where the parallel approach of GPUs is perfectly suited.  And, five years ago we started to invest in deep learning quite substantially, and we made fundamental changes and enhancements for deep learning across our entire stack of technology from the GPU architecture to the GPU design to the systems that GPUs connect into. For example, NVLink to other systems software that has been designed for it like [Kudi] and [N] and digits to all of the deep learning experts that we have now in our Company. The last five years, we've quietly invested in deep learning because we believe that the future of deep learning is so impactful to the entire software industry, the entire computer industry that we, if you will, pushed it all-in. Now, we find ourselves at the epicenter of this very important dynamic, and it is probably -- if there is one particular growth factor that is of great significance that would be deep learning. Sure.  Thanks a lot, Vivek. Let's see, on PC gaming, there's a few dynamics. Our installed base represents somewhere around 80 million active GeForce users around the world.  And, in fact, only about one-third has even upgraded to Maxwell, and we only started shipping Pascal half of this last quarter. And so, that gives you a sense of how much -- and Pascal is unquestionably the biggest leap we've ever made generationally in GPUs ever. It is not only high performance, it is also energy efficient, and it includes some really exciting new graphics technologies for VR and others.  I think Pascal is going to be enormously successful for us, and it comes at a time when the PC gaming marketplace is also quite different than the PC gaming market five years ago. One dynamic that's really quite powerful is that the production quality, the production content is much, much higher in video games today than ever.  And, the reason for that -- I've mentioned several times in previous calls -- is that the installed base of capable game platforms that are architecturally compatible, meaning that PlayStation 4 and Xbox 1 and PCs are essentially architecturally compatible.  And so, the footprint for developers has grown tremendously over previous generations. This is a dynamic that's relatively new.  And so, as a result, the quality of games go up which means that the consumption of GPU capability goes up with it.  I think  we're absolutely seeing that dynamic. I'm super-excited about the fact that the Next-Generation game console, the big boost, the 2x boost is coming just around the corner.  That's going to allow game content providers, game developers to aim even higher, and I think that that's going to support long-term expansion of our gross margins and ASPs of PC gaming. I would say that there's some other dynamics that are quite powerful as well as you know very well which is eSports is no longer just an interest. E-Sports is a full-force global phenomenon and very powerful in Asia in just about every developing country, and of course, the western world as well.  I think that on top of that, not only is VR off to a great start.  We're seeing some great content now.  But, some of the things that we introduced recently with Pascal, tapping into this grassroots but rather global interest in using video game as an art medium.  We introduced project NVIDIA Ansel which is the world's first in-game photography system.  It allows you to create virtual reality photographs, and it's just really, really amazing.  And so, you could use your video game, capture your amazing moments, share it in VR, or in high resolution with all of your friends.  There's a lot of different ways to enjoy games now, and the production value just continues to go up which is great for our platforms. And so, I think just to summarize your initial question, how much of the installed base has upgraded to Pascal -- very, very small, of course, because we just started production ramp.  But, even then, only one-third has upgraded to Maxwell, and so there's a pretty large, pretty significant upgrade opportunity ahead of us. Sure, thanks. Well, as you can imagine, we have a good pulse on the state of the industry, and we've been in this industry since the very beginning.  Deep learning was really ignited when pioneering researchers around the world discovered the use of GPUs to accelerate deep learning and made a practical -- made it even practical to use deep learning as an approach for developing software. The GPU was a perfect match because the nature of the GPU is a sea of small processors, not one big processor, but a whole bunch of small processors.  And, vitally, they're connected by this connecting tissue.  This connecting tissue inside our processor, connecting memory, connecting fabric.  That makes it possible for the processors to communicate with each other all simultaneously. That architectural innovation has been the source of our GPU computing initiative some 10 years ago. That invention has really been groundbreaking. And so, the GPU was really quite a perfect match for deep learning where neural nets are communicating neurons essentially -- inspired by neurons communicating with each other all simultaneously.  And so, the GPU was really quite a perfect match. If you look at deep learning today, five years later, I think that it's a foregone conclusion that deep learning is being infused into just about every single Internet service to make them smarter, more intelligent, more delightful to consumers.  And so, you could see that the hyperscale adoption of deep learning is not only broad, it's large scale and is completely global.  This new computing approach, we realized was going to be quite significant long term and so five years ago we started making quite significant investments across the entire stack of our Company. GPU computing is not just the GPU chip.  It's GPU architecture, it's the GPU design.  It's the GPU system.  All of the algorithms that run on top of it.  All of the tools that run on top of it.  The frameworks -- our collaborations with researchers all over the world.  And so, that collaboration and our investment has improved deep learning on GPUs dramatically in the last two generations when we started this, we were in Kepler.  Maxwell was some 10 times better, and Pascal is some 10 times better than Maxwell.  So, in just two generations, just five years time, we've improved deep learning by an enormous amount, and the GPU today is very unlike a GPU back in the good old days because of all of the work that we've done to it. Our strategy -- and this is where we are different not only the focus on the GPU and the expertise in parallel computing, but where we are really different, I would say, is our singular architecture approach to deep learning. We've essentially put all of our investment behind one architecture. We make this architecture available from hyperscale to data centers to workstations to notebooks to PCs, to cars, to embedded computers, to even a brand new, fully integrated, high performance computer in a box we call DGX, the NVIDIA DGX1.  So, there's so many different ways to gain access to the NVIDIA architecture, the NVIDIA platform for deep learning. It's just literally all over the place, all around you. It's available to you in retail stores and E-tail stores from OEMs in the cloud or even universities all over the world just in embedded computer kits.  So, our approach is quite singular and quite focused, and my sense is that our lead is quite substantial, and our position is very good.  But, we are not sitting on our laurels as you can tell, and for the last five years, we've been investing quite significantly.  And so, over the next several years, I think you're going to continue to see quite significant jumps from us as we continue to advance in this area. Well, I appreciate the question. Yes, we've just started this quarter shipping drive PX2.  Before I answer your question, let me tell you what drive PX2 is. Drive PX2, of course, is a processor. It's the drive PX2 version with just one single processor with just Parker and our Tegra processor.  And, optionally, with discrete GPUs, you could build a car with auto-pilot capability or an AI co-pilot capability all the way to self-driving car capability. And, it was able to do sensor fusion.  It was able to do SLAM, which is localization and mapping.  Detection, using deep neural nets of the environment.  In a surround matter, all of the cameras around the car, all feeding into the processor.  And, the drive PX processor doing realtime inferencing of surround cameras.  All the way to the actual planning and driving of the car -- all done in this one car computer, this one car AI super computer. And so, this quarter, we started them shipping to all of our partners and developers so that they can start developing their software and their systems around our computer and on top of our software stack. We have the intentions of shipping in volume production many of these, and it's hard to know exactly what everybody's schedule is.  But, it ranges everything from very soon to the next couple of years. And, developing a self-driving car is no -- it's a fairly significant undertaking.  Nobody does it for fun surely, and the question is maybe if I could frame the question just slightly differently, do I expect people to be building OEM cars? Or, do we expect them to be building shuttles that are maybe geofenced? Do we expect them to be building trucks? And, you know how many trucks are on the road and how much of the world's economy is built around trucking products all over the world to services of basically taxi as a service. The answer is that we're working with customers and partners across that entire range from cars that are sold to trucks to vans to shuttles to services. Well, first of all Founders edition -- I appreciate you asking that. Founders edition is engineered by NVIDIA, completely built by NVIDIA, and sold directly by NVIDIA and supported by NVIDIA. There's some people that -- some gamers and customers who would prefer to have a direct relationship with our Company. It's availability is limited, and it's engineered just at the highest possible level of quality.  And, we limit the production of it. The reason for that is because we have a network of partners who are much, much more able to take the NVIDIA architecture to every corner of the world literally overnight. We have a fair number of partners who blanket every single country on the planet as we know, and they can provide them in different sizes and shapes and styles and different thermal solutions and different configurations and different price points.  I think we believe that that diversity is one of the reasons why the NVIDIA GeForce platform is so popular, and it creates a lot of excitement in the marketplace and a lot of interesting and different diversified designs. So, I think those two strategies are harmonious with each other.  But, the key point is we built the Founders edition really as a way for some customers to be able to purchase directly and have a relationship directly with us.  But, largely, our strategy is to go to the market with a network of partners. As for gross margins, [the margins are the same]. Sure. Yes, first of all, the type of work loads in the data center has really changed. Back in the good old days, it largely ran database searches, but that has changed so much. It's no longer just about text. It's no longer just about data. The vast majority of what's going through the internet and what's going through data centers today as you know very well are images.  They are voice.  They are increasingly and probably one of the most important new data formats is live video. Live video, if you think about it for just a moment, it's live video.  So, it doesn't stay in the server, and it doesn't get recorded which means that if you want to enjoy that live video there needs to be a fair amount of artificial intelligence capability in the data center that's running realtime on their live video so that the person that might be interested the video stream that you're streaming knows who to alert and who to invite to come and watch the live video.  And so, if you think about data center traffic going forward, my sense is that the workload is going to continue to be increasingly high throughput, increasingly high multimedia content, and increasingly, of course, powered by AI and powered by deep learning.  And so, I think that's number one. The second is that the architecture of the data center is recognizably, understandably changing because the workload is changing. Deep learning is a very different workload than the workload of the past, and so the architecture -- it's a new computing model.  It recognized it needs a new computing architecture, and accelerators like GPUs are really well -- a good fit. So, now the other question is how much. It's hard to say. It's hard to say how much.  But, my sense is that it going to be alive, and without any predictions, it's going to be a lot more than we currently ship.  I think the growth opportunity for deep learning is quite significant. I think every hyperscale data center will be GPU-accelerated. It will be GPU-accelerated for training and GPU-accelerated for inferencing.  There may be other approaches, but I think using GPUs is going to be a very large part of that. And then, lastly, APUs. I guess my sense is for data centers, energy efficiency is such a vital part and although the work load is increasingly AI and increasingly live video and multimedia where GPUs can add a lot of value. There's still a lot of workload that is CPU-centric, and you still want to have an extraordinary CPU.  I don't think anybody would argue that Intel makes the world's best CPUs. It's not even close.  There's not even a close second, and so I think the artfulness of and the craftsman ship of Intel CPUs is pretty hard to deny.  For most data centers, if you have CPU workloads anyway, I think Intel's Xeons are hard to beat, and so that's my opinion anyway. Thanks a lot, Ian. We have taped out.  We have verified.  We have ramped every Pascal GPU.  That's right.   However, we have not introduced every one. Yes, Steve. I think it's all of that. We have to keep pushing VR and get the head mounts out to the world. I think ACC Vive -- they're doing a great job.  Oculus is doing a great job.  We track very carefully all of the head mounts that are going out there, and it's growing all time. Second, the content is really cool and people are really enjoying it.  And so, we've just got to get more content, and developers all over the world are jumping on to VR. It really is a great new experience.  But, it's not just games as you know. One of the areas where we have a lot of success, and we see a lot of excitement is in enterprise and industrial design, in medicine, medical imaging, and architectural engineering. We use it ourselves. We are doing a fair amount of design of our workspace, and we render everything using our photo realistic renderer called Iray, fully accelerated by our GPUs.  And then, we render it into VR, and we enjoy it, enjoy it completely in VR.  And, it's something else to be inside an environment that's photo-realistic to be rendered and completely enjoying in VR.  So, architectural engineering and construction is going to benefit from that. So, we see a lot of broad-based adoption of VR. Now, one of the things that we did which was really spectacular is the multi-resolution, multi-projection renderings of Pascal. It's the world's first GPU architecture that has the ability to render into multiple projections simultaneously instead of just one, and the reason for that is because the GPU back in the good old days was designed for displaying into one display. You have one keyboard.  You have one display.  But, that mode of computer graphics has really changed as we moved into the world of virtual reality and all kinds of interesting different display configurations and display types.  And so, multi-projection was a revolutionary approach to graphics, and Pascal introduced it.  You really benefit in VR. The second thing that we did was we integrated physics -- real world physics simulation into VR. The benefit is that without the laws of physics, as you know, you can't feel anything. Things don't collide. Things don't bounce. When you pick up something, you don't feel the haptics of it. We made the entire environment physically simulated,  and so as a result, you feel the entire environment.  When you tip a bottle of water over, it behaves like a bottle of water tipped over.  Balls behave the way balls behave, and things don't merge into each other.  That integration with haptics has completely revolutionized VR, we believe, and that's physics simulation is another thing. And so, I think our position in VR is really quite great, and I'm super-enthusiastic about the development of VR. I would say it's about 50% deep learning at the moment, and probably, call it, 35% -- one third is high performance computing.  Maybe more than that.  And, the rest of it is virtualization.  Going forward, which is part of your question, my sense is that deep learning would become a very significant part of that. The other thing to realize is that deep learning is not just for internet Service Providers for voice recognition and image recognition and face recognition and such. Deep learning is a way of using mathematics, using software to discover insight in a huge amount of data. And, the one place where we generate a huge amount of data is high-performance computing. Every single super computing center in the world is going to be moving towards deep learning, and the reason for that is because they generate a huge amount of data that they really have very little ability to comb through -- to sort through.  And now, with deep learning, they could discover really, really subtle insights in data that is hyper-dimensional.  That's the way to think about deep learning is it's really mathematics.  It's a new form of mathematics that is very, very powerful.  It's a new approach to software.  But, don't think of it as a market. I think every market is going to be a deep learning market. Every application is going to be a deep learning application, and every piece of software will be infused by AI long term. Yes.  Thanks, Harlan.  I appreciate the question. The team's been working really hard over the years to really change the way that computer aided design is done. Your observation is absolutely right, and it's coming from several different places. First of all, more and more of design is really about product design, industrial design, where the feeling of the product, the aesthetics of the product is just as important as the mechanical design of the product.  And whether you're talking about a building or just a consumer product or a car, we need to be able to simulate the aesthetics of it in a photo realistic way, using real material simulations. The computational load necessary to do that is just really quite extraordinary.  And we're now seeing one design package after another, whether it's [DeSo's] leading packages, Solid Work's leading packages, AutoDesk, Adobe the amount of GPU use has really, really increased and it's increasing quite dramatically. I think partly because, finally for all of the ISVs, of all the developers, not only is the market demand for earlier views of photo realistic designs an important decision criteria. They can also rely on the fact that great GPUs are available in just about every computer.  And so the pervasiveness of GPUs allows them to take advantage of the GPUs and to really trust that the software capabilities that they put into their packages, if they rely on GPUs, will have the benefits of GPUs there. And so I think that's the virtual cycle you're starting to see in design.  And so the investment that we made in the photo realistic renderings several years ago, the GPU acceleration of optics, this layer for path tracing that is used by just about every software package in the world, our continued evangelism of GPUs and its general purpose use, from computer graphics all the way to imaging, is something that I think is starting to see benefits. That's number one. Number two, Maxwell was the most energy efficient GPU ever made, until {Pascal. Maxwell was twice the energy efficiency of Kepler, and the amazing thing is that Pascal is twice the energy efficiency of Maxwell. But Maxwell made it possible for cinema-like designs in laptops and more elegant workstations and the ability to put more horsepower, more capability into any workstation because of power concerns. Maxwell made it possible for the entire industry to uplift the level of GPU that it uses. And I think that going forward, your last question is going forward, do we see -- how do we see Pascal? Pascal is in the process of ramping into workstations all over the world.  And so I think in the coming quarters, we're going to expect to see Pascal out there.  And my expectation is that the dynamics that I just described, which is software developers using more photo realistic capabilities, our inventioning of GPU accelerated photo rendering, I-ray and optics and NDL material description language, and then lastly, the energy efficiency of GPUs, those three factors combined is going to be  really helpful for workstations, and then last, VR. VR is coming, and in order to really enjoy these type of applications for design, you're going to need a pretty powerful GPU at this point. Yes, thanks a lot, Ross. Well, we play in a graceful, friendly and open way.  And I mean that rather seriously. We believe this. We believe that building an autonomous self-driving car is a pile of software and it's really complicated software. It's really, really complicated software and it's not like one company is going to do it. And it's also not logical that large, great companies who are refining their algorithms and the capabilities of their self-driving cars over the course of the next two decades can outsource to someone the self-driving car stack. We've always felt that self-driving cars is a software problem and that large companies need to be able to own their own destiny.  And that's the reason why PX2 is an open stack.  And it's an open platform.  So that every car company can build their self-driving car on top of it. Number one. Number two, the DRIVE PX2 architecture is scalable.  And the reason for that is because automatic braking and auto pilot on a highway and a virtual co-pilot and a completely autonomous self-driving car, a self-driving truck, a geofenced autonomous shuttle, all of these, a completely autonomous taxi, all of these platforms cannot be solved by one chip. It's just not even logical. The computation necessary to do it is so diverse. The more digits of accuracy or the more digits of precision towards safety that you would like to have in dealing with all of the unexpected circumstances, the more nines you would like to have, if you will, the more computation you have to do. Just as voice recognition, the amount of computation necessary for voice recognition over the last just four or five years has increased by 100 times.  But notice how precise and how accurate voice recognition has become.  And image recognition, video circumstance recognition, context recognition, all of that is going to require just an enormous amount of computation.  So we believe that scalable platforms is necessary, number two. And then number three. Detection, computer vision and detection, object detection, is just one tiny sliver of the entire autonomous driving problem. It's just one tiny sliver.  And we've always said that autonomous vehicles, self-driving cars, is really a AI computing problem. It's a computing problem because the processors needs not just detection, but also computation, the CPU matters, the GPU matters, the entire system architecture matters, and so the entire computation engine matters. Number two computation is -- computing is not just a chip problem. It's largely a software problem. And the body of software necessary for the entire system software stack, if you would, the operating system of a self-driving realtime computer, realtime super computer doesn't exist. Most super computers are best effort super computers. They run a job as fast as they can until they're done. But that's not good enough for self-driving cars. This super computer has to run in real time and it has to react at the moment that it sees that there's danger in the way, and best effort doesn't cut it. You need it to be a real time super computer.  And the world has never built a real time super computer before and that's what DRIVE PX2 is all about, a real time super computer for some round, autonomous AI. And so that's the focus that we have. That's the direction that we've taken.  And I think what you're seeing is that the market is starting to react to that. That maybe as they go further and further into autonomous driving, that they're discovering that the problems are related to the type of problems that we're seeing and that's the reason why DRIVE PX is a computer, not a smart camera. Well, as you know, deep learning is not just a internet service approach. Deep learning is really machine learning super charged. And deep learning is really about discovering insight in big data, in big unstructured data, in multi-dimensional data.  And that's what deep learning, that's what I've called it Thor's hammer that fell from the sky.  And it's amazing technology that these researchers discovered.  And we were incredibly, incredibly well prepared, because GPUs is naturally parallel. And we put us in a position to really be able to contribute to this new computing revolution. But when you think about it in the context that it's just, it's software development, it's a new method of doing software and it's a new way of discovering insight from data, what company wouldn't need it? So every medical, every life sciences company needs it. Every health care company needs it. Every energy discovery company needs it. Every E-tail, retail company needs it. Everybody has lots of data. Everybody has lots and lots of data that they own themselves. Every manufacturing company needs it. Every company that cares about security, every company that deals with a massive amount of customer data has the benefit, can benefit from deep learning.  And so when you frame it in that context, I think I would say that deep learning's market opportunity is even greater in enterprises than it is in consumer internet services. And that's exactly the reason why we built the NVIDIA DTX1.  Because most of these Enterprises don't have the expertise or simply don't have the willpower to want to build a super computing data center or a high performance computer. They would rather buy an appliance, if you will, with all of the software integrated and the performance incredibly well tuned, and it comes out of a box.  And that's essentially what NVIDIA DGX1 is.  It's a super computer in a box and  it's designed and tuned for high performance computing for deep learning. Well, our guidance is our best estimate.  And we'll know how everything turns out next quarter when we talk again.  But at some high level, I would agree with you that as we move further and further and more and more into our platform approach of business, where our platform is specialized and rich with software, that increasingly the value of the product that we bring has extraordinary enterprise value, that the benefits of using it is not just measured in frames per second but real TCO for companies and real cost savings as they reduce the number of server clusters, and real increases and real boosts in their productivity.  And so I think there's every reason to believe that long term, this platform approach can drive greater value.  But as for the next quarter, I think let's just wait and see how it goes. It's Vindra.  How are you? Yes, good question. There's no way to square and there's no reason to square, and you aren't going to find one answer.  And the reason why you aren't going to find one answer is because nobody knows exactly how to get it done. We all have intuitions and we all have beliefs about how we're going to be able to ultimately solve the long-term fully autonomous vehicle, that wherever I am, the car I step into, the automotive automobile we step into, is completely autonomous, and that it has AI inside and out, and it's just an incredible experience.  But we aren't there yet. And all of these companies have slightly -- not all -- but many companies have slightly different visions of the future. Some people believe that the path to the future is fully autonomous right away in a geofenced area that has been fully mapped in advance. Some people believe that you can use it just for highway auto pilot as a first starting point and work quickly towards fully autonomy, and some people believe the best way to do that is through shuttles and trucks.  So you see a lot of different visions out there.  And I think all of those visions are coming from smart people doing smart things and they're targeting different aspects of transportation. I think there's a fallacy that transportation in every single country in every single form is exactly the same. It just doesn't work that way.  And so there's technology insight and then there's market insight, and there's a technology vision versus your entry point.  And I think that's where all of the squaring doesn't happen. So you're solving for a simple equation that won't happen.  However there's one thing that we believe absolutely will happen. We are absolutely certain that AI will be involved in this endeavor, that finally with deep learning and finally with AI that we believe we have the secret sauce necessary to break these puzzles and to solve these puzzles over a period of time.  Number one. Number two.  We believe unquestionably that depending on the problem you want to solve, you need a different amount of computational capability. We believe unambiguously this is a software problem, and that for the largest of transportation companies, they need to own their own software in collaboration with you, but they aren't going to let you do it and keep it as a black box. We believe unambiguously that this is a computing problem, that this is a real time super computing problem., that it's not just about a special widget, but computation is necessary, processors, a computer system, systems software, and an enormous amount of operating system capability is necessary to build something like this. It is a massive software problem.  Otherwise, we would have done it already.  And so I think you'll see this year the beginnings of a lot of some very visionary and really quite exciting introductions.  But in the next year and the year after that, I think you'll see more and more and more.  I think this is the beginning and we're working with some really, really amazing people to get this done. No. Oh, automotive ASPs for self-driving cars will be much higher than infotainment. It's a much tougher problem. Every car in the world has infotainment. With the exception of some pioneering work or early, the best, the most leading edge cars today, almost no cars are self-driving.  So I think that the technology necessary for self-driving cars is much, much more complicated than lane keeping or adaptive cruise control or first generation, first and second generation ADAS. The problem is much, much more complicated. Brian, thank you very much.  First of all, I appreciate your comment. The team worked really, really hard.  And over the last several years, the last five years, all of the employees of NVIDIA have been pursuing a strategy that took until today, really, to show people that it really pays off and it's a very unique business model. It's a very unique approach. But I just want to congratulate all of the employees that have worked so hard to get us here. I appreciate the comment also about energy efficiency. In fact, energy efficiency is the single most important feature of processors today and going forward. And the reason for that is because every single environment that we're in is power constrained, every single environment. Even your PC, with 750 watts or 1,000 watts, is power constrained.  Because we could surely put more GPUs in there than 1,000 watts.  And so that's power constrained.  We're in environments where we only have one or two watts. It might be a drone and we need to be, we're completely power constrained, so energy efficiency is really, really important. We might be in a data center where we're doing deep learning and training we're training neuronets or we're inferencing neuronets.  And in this particular case, although the data center has a lot of power to provision, the number of GPUs that they want to use in it is measured in tens, tens and tens of thousands.  And so energy efficiency becomes the predominant issue. Energy efficiency, literally, is the most important feature of the processor. Now from there, from there, there are functionality and architectural features, that the architectural changes that we made in Pascal so that we could stay ahead of the deep learning research work and the deep learning progress, was groundbreaking and people are starting to discover the architectural changes that we put into Pascal and it's going to make a huge difference in the next several years of deep learning.  And so that's a feature and an architectural innovation. And then lastly, of course, there's all of the software that goes on top of the processor base. We call it GPU computing instead of just GPUs, because GPU computing is about computing, it's about software, it's systems, it's about the inner relationship of our GPU with the memories and all of the memories around the system and the networking and the inter connect and storage, and it's a large scale computing problem. It is also the highest throughput computing problem on the planet, which is the reason why we've been called upon by our nation to build the world's next two fastest super computers. High throughput computing is our company's expertise. High throughput computing from fundamental architecture to chip design to system design to system software to algorithms to computational mathematics, and all of the experts in all the various fields of science, that is the great investment that we made in the last five years.  And I think the results are really starting to show. Yes, thanks, Blayne. The return of capital continues to be an important part of our shareholder value message  But remember, it is still two parts of it. Part of it is still dividends and part of it has been our purchasing of stock. So as we continue to go forward, the dividend is definitely a long term perspective and we'll make sure that we can watch the dividend yield there to stay competitive and also looking at our profitability. Our share repurchase, we'll look at the opportunistic time for those repurchases and making sure that we're also doing that carefully, as well. And long-term use of capital, I would say this, that what NVIDIA is really rich with is we're rich with vision and creativity and the courage to innovate.  And that's one of the reasons why we start almost every conversation with anything by gathering our great people around the Company and seeing what kind of future we can invent for ourselves and for the world.  And so I think our use of capital is nurturing the employees that we have and providing them the platform to innovate and create the conditions by which they can be successful and do their live's work.  And so that's philosophically where we start. We aren't allergic to acquisitions and purchases, and we look all the time and we have the benefit of working with and partnering with companies large and small all over the world as we move the industry forward.  So we're surely open to that.  But our natural posture is always to invest in our people and invest in our own company's ability to invent the future. Yes, deep learning is a software approach, a new computing architecture, a new computing approach that the industry, that researchers have been developing for 20 years.  And it was only until five years ago when pioneer work was done on deep learning and on GPUs that really turbo charged it and gave the industry, if you will, a time machine that brought the future to the present.  And the power of deep learning is so great that this capability is expanding and people are discovering more ways to use it and more applications and new deep learning architectures, and the networks are getting bigger and deeper and more complicated.  And so I think that this area, this area is going to grow quite significantly. It represents a vast majority of our data center revenues recently, and my sense is that it's going to continue to be a significant part of it. So what was the second question?  Did I miss it? I think his question was really about data centers and deep learning, right? What's that? I think your question was regarding deep learning and the percentage of data center and how that has moved. Kevin, I appreciate the question. Tegra is at the core of all of our self-driving car initiatives. And so without Tegra, there would be no self-driving cars. So Tegra is the core of our self-driving car initiative, the computing platform for self-driving cars.  And DRIVE PX2 includes Tegra, as well as discrete GPUs of Pascal, but the core of it, the vast majority of the heavy lifting is done by Tegra, and we expect that going forward.  And so Tegra is incredibly important to us. Tegra is also the core of the processor of Jetson. Jetson is a platform that is designed for other embedded autonomous and intelligent machines.  And so you could imagine what kind of intelligent machines in the future will benefit from deep learning and AI, but robotics and drones and embedded applications inside buildings and cities.  There are all kinds of applications. I'm very, very optimistic about the future of Jetson, but at the core of that is also Tegra.  So think of Tegra as our computer on a chip, and it's our AI computer on a chip. Okay, may I -- I appreciate all the questions. Thank you all for joining us today. Our growth is really driven by several factors. Our focus on deep learning, self-driving cars, gaming, and VR, markets where GPU has been vital, is really starting to pay off. The second factor is that Pascal is the most advanced GPU ever created and we're incredibly excited about it. And we, this last quarter, we ramped it with enormous success.  And I'm so proud of the team for all of the preparation and the executions last quarter.  And the third is hyper scale adoption of deep learning is now widespread, is large scale and we're seeing it globally. Those are the several growth drivers ahead of us. As we go forward, we're also looking to sharing our many developments in deep learning AI with you. We're really just in the beginning of seeing  the actual growth of deep learning as we scale out into the market. Deep learning adoption is now widespread and is ramping at every hyper scale data center. It's a new computing model that requires a new computing architecture, one that GPU is perfectly suited for.  And the thing that we've done that I'm really delighted with is the strategy that started five years ago to optimize our GPU computing platform from end-to-end and optimize it for deep learning, at the processor level, at the architecture level, at the chip design level, systems and software and algorithms and a richness of deep learning experts at the Company, and the collaboration we have all over the world with researchers and developers has made it possible for us to continue to advance this field and this platform. And as a result of that our deep learning platform improved far more than anybody would have expected.  If you just projected it based on semiconductor physics, it would be nowhere near the level of speed up and step up that we got from generation to generation, from Kepler to Maxwell, we got 10x, from Maxwell to Pascal, we got another 10x.  And you can surely expect pretty substantial improvements and increases from us over the next several years. Where we really shine is not only as a fantastic platform for deep learning and the training of the networks, but it's also a fantastic platform to scale out. You can enjoy our platform, whether it's in cloud or in data center or in super computers and workstations and desk side PCs and notebook computers to cars to embedded computers, as I mentioned just now with Jetson. This is a one singular architecture approach, so the thoughtfulness and the care of the investment of the developers and the software programmers and researchers is really our preeminent concern.  And as we know, computing is about architecture and computing is about platform, and mostly, computing is about developers.  And we've been quite thoughtful about the importance it is to developers.  And as a result, developers all over the world, all over the industry, can use this singular architecture and get the benefits of their science and their applications as they scale and deploy their work. So that's it. We had a great quarter and I look forward to reporting our progress next quarter. Thank you all for joining us.","Hi. Thanks for taking my questions. First question on the data center business.  Can you all help us understand to what extent is the demand being driven by the deep learning applications versus the classic computationally intense design applications? That's very helpful, thank you.  And then, last question. On the new -- so, you're just starting to ship Pascal now.  I guess my understanding is that historically as you're shipping a new product, the yields have opportunity for improvement, and the more volume you ship the more you climb down the yield curve. What classically happens to here on the yield?  And, does that positively impact gross margins over the next three or four quarters? Thank you. Hi.  Thank you for taking my questions and congrats on a very strong quarter. Your Q3 revenue guide implies further acceleration on a year-over-year basis. Are there one or two end markets where you expect outsized growth?  Or, should we expect growth in the quarter to be broad-based? Thank you for taking my question, and congratulations on good growth and the execution. Jen-Hsun, the first question is tied to PC gaming, very strong trends. I was curious if you could quantify how much of your base has upgraded to Pascal?  And, have you noticed any change in the behavior of gamers in this upgrade cycle?   Whether it's the price or what part of the stack they are buying now?  And, how quickly they are refreshing versus what you might have seen in the Kepler and the Maxwell cycles? Hi, thanks for taking my questions.  Jen-Hsun, the first one if I could on the data center competitive landscape. Earlier this week, we saw one of your data center competitors make an acquisition of a smaller private Company, and I was wondering if you could talk a little bit more about how you view your position in the data center market with respect to machine learning AI?  And also, how your products are positioned from the high end or low end-type of machine learning application performance? Yes, thank you. I had a question on automotive. You mentioned that drive PX is now shipping to 80 car companies. Jen-Hsun, I'm curious, are the wins here similar in size and focused more on prototyping?  Or, are there opportunities here that could ultimately translate into full production wins and drive the automotive business disproportionately? Yes, thanks for taking the question. The first is just a follow-up on some of the gaming strength in the quarter with the Company launching the Founders addition availability of gaming products in the quarter.  Can you talk about how that went?  And, for those products, how gross margins compare to just chip bait sales that would go into a gaming card OEM? Yes, good afternoon. Thank you. Jen-Hsun, I wanted to ask a couple questions again on the data center business. The first being, we've done a little bit of work trying to estimate in our team what the long-term server attach rate for accelerators in general could be and for GPUs within that.  So, it would be really interesting to hear your perspectives on that?  And then, secondly, is there a market there for an APU-type product in the data center? I know you have project Denver and some other things going on from the CPU perspective, but is there a deep learning integrated CPU/GPU play that might open up more TAM long term for your Company that you are considering pursuing?  Thank you. Yes, thank you. Earlier you talked about taping out all of the Pascal products at this point. Are you with three products in the market, are you ceding the sub-$250 price point for cards to competition?  Or, is this something you can serve with older Maxwell product or some upcoming product? Thanks. Great, thanks a lot for the question. I just wanted to follow up a little bit on virtual reality.   You had talked a little bit about investments there.  I was just curious what reception you're getting at this point, and what's going to be in your mind the biggest driver getting that going?  Is it more headsets?  Or, more developers working on that?  Thank you. Hi.  Just on the data center side.  Jen-Hsun, you mentioned three key segments, HBC, grid, and deep learning. What percent of mix are those for the data center? Good afternoon. Solid job on the quarterly execution. You guys had really good growth in professional visualization, record revenues. I would have thought that most of the growth was being driven by the upcoming release of the Pascal based P5000 and P6000 family.  So I was sort pleasantly surprised that most of the demand was driven by your current generation M6000 family, which means obviously that the Pascal demand cycle is still ahead of you. Number one, is that a fair view?  And then what's driving the strong adoption of M6000, and if you haven't already released it, when do you expect to launch the Pascal-based 5000 and 6000 family?  Thank you. Hi, guys.  Thanks for letting me ask a question.  A couple for you, Jen-Hsun, on the automotive side. I guess the first part would be, we've seen in the recent months some partnerships being formed with some of your competitors and some of your customers, and we've seen some of those partnerships actually dissolve.   So how does NVIDIA play in this general ecosystem in forming partnerships or not?  And then the second part, if we put even just a rough year on it, when would you think the autonomous driving part of your automotive business would actually exceed the infotainment size of your automotive business? Thank you. Great.  Thank you so much. You talk about deep learning in the hyperscale environment, but it seems like you're getting some traction as well in the enterprise environment.  I know at least one IT department that we've talked to has been doing some implementation. Can you talk about your progress there and what does it take for you to build that presence within more traditional enterprises? Hello.  Thank you very much for squeezing me in. I had one question on gross margin, Jen-Hsun. Very big top line guidance, but yet gross margin is guided to flat. What is the reason?  And I understand it's not always perfectly correlated, margins should be going up that much, but is it pricing, is it yield? Because the mix also seems to be moving in the right direction, more [Pro Ves], more HPC and less of the OEM business. Thank you. Exactly. Good. A question, Jen-Hsun, on the DRIVE PX2. So my understanding, as you described it, it's one scalable architecture from the cockpit to ADAS to mapping to autonomous driving.  But I'm curious to see how that compares to the approach that some of your competitors are taking with respect to providing different solutions for different levels of the ADAS systems, whether it's level one, level two, level three, specifically with the V2x communication where, for level four autonomous driving, where you're going to need 6 to 20 different radar units, 3 to 6 different cameras, [lidar].  I'm trying to square how your approach is different from some of your competitors in the semiconductor space. Thanks for taking my question, guys.   So just circling back to the data center piece and the deep learning aspect, is there a change in ASPs you guys are seeing when you enter that market? So essentially there's going to be no margin change from the data center sales.  And I guess the same question in automotive, as well. Hi, guys. Thanks for squeezing me in. I think this will be the first, congrats, actually, on a pretty darn good quarter and amazing guidance. I want to come back to the difference of Pascal versus what would be otherwise competition from either Intel or AMD. There's been a fair amount of documentation talking about the power requirements or the power draw differences between Pascal versus Polaris.  And one would think that while that's important in gaming and it's gotten a lot of notice, it would actually be more important for these deep learning applications that we've been talking so much about over the past half hour, 45 minutes.  Can you maybe talk to that side of the design, not so much the horsepower, but maybe the power efficiency of it and what that means for when you scale it up into really big problems? Hey, guys.  Thanks for squeezing me in here and great execution on the quarter. Two related questions. One, Colette, just curious your view on the use of capital and buybacks.  Obviously, an accelerated one, only $9 million in the last quarter.  What's your view going forward? And then Jen-Hsun, maybe a bigger question in terms of ease of capital, whether you could talk about, you said CPU is not an area that you would want to go into, but obviously GPUs have legs.  So just curious, if you look around other areas, maybe in the data center, where you could also add value? Good afternoon. Thank you for squeezing me in. I guess, two quick questions.  The first one, thank you for breaking out deep learning as a percentage of the data center. Can you provide what that percentage was for the April quarter?  And then the follow-up question is if I look back over the last four quarters and I look at your implied guide, you're looking at roughly 50%  incremental operating margin. And curious if that's the right number you would underwrite here, or should we be thinking about improving mix, as well as maturing process and manufacturing at your foundry partners such that that could actually be higher as we look ahead?  Thank you. Yes, and it's the vast majority. Thanks for taking my question. Maybe I'll go to the other end of the spectrum and speaking of energy efficiency. Are you finding new opportunities for Tegra, aside from the infotainment in automotive? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q4 2016,2016,6772,1996,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for fourth quarter of 2016. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that today's call is being webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until the 24th of February 2016. The webcast will be available for replay up until next quarter's conference call to discuss Q1 financial results. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without our prior written consent. During the course of this call, we may make forward-looking statements based on current expectations. These forward-looking statements are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, the 17th of February 2016, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO Commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Arnab. Revenue reached a record in the fourth quarter, totaling $1.4 billion, up 12% from a year earlier, up 7% sequentially, and above our outlook of $1.3 billion. Our full-year revenue crossed above $5 billion to a record $5.01 billion, which was up 7% from the previous year. Quarterly growth was broad-based, with expansion across each of our four market platforms -- gaming, professional visualization, datacenter and automotive. Pacing gains were our GTX gaming platform, our datacenter platform, powered by deep learning, growing adoption and automotive. Viewed from a reporting segment perspective, Q4 GP revenue grew 10% to $1.18 billion from a year earlier. Tegra Processor revenue was up 40% to $157 million. NVIDIA's strategy remains sharply focused on creating platforms for our key markets. Our progress stems from our success in creating strong products that are targeted at growth markets. In Q4, our four market platforms contributed more than 85% of revenue, up from 78% a year earlier. Our growth platforms collectively increased 23% year over year. First let's start with our gaming platform. Gaming revenue grossed 25% year on year to $810 million, with good momentum carrying forward from the previous quarter. Maxwell-based GeForce GTX processors continue to lead our gaming growth, combined with growing anticipation for VR and the launch of holiday blockbuster games. The GeForce GTX 970 GPU stands as the world's most popular graphics card on the Steam gaming platform. And we continue to get strong traction with our GeForce GTX GPUs that power gaming notebooks. That includes the recently launched GTX 980 for notebooks, which has enabled a new category -- enthusiast-class VR-capable gaming notebooks. Excitement is growing around VR gaming, a key theme of last month's Consumer Electronics Show. We unveiled there our GeForce GTX VR Ready program to help gamers choose the best hardware for an immersive VR experience. And Oculus, which has now opened pre-orders for the Rift headsets, has exclusively certified GeForce GTX systems as being ready for VR. GeForce sales are driven by the launch of great gaming titles, and that again proved true this past holiday season. Fallout 4 was among the standouts, recording more than $750 million in sales in its first 24 hours. Other major hits were Star Wars Battlefront, Call of Duty: Black Ops 3 and Rainbow Six. We remain pleased with the continued success of GeForce Experience, our gaming platform that automatically optimizes your PC settings for each game and downloads the greatest game-ready drivers. At the end of January, just 2.5 years after its introduction, GeForce Experience subscribers stood at $76 million, up 37% on the year. Moving to professional visualization. Quadro revenue increased 7%, both sequentially and year over year, to $203 million. The refresh cycle of workstations continued to improve during the quarter, driven in part by new workstation configurations in the market. While VR is often portrayed as a consumer play, we are also excited by its potential in enterprise, particularly in areas such as medicine, architecture, education, and product design. Audi now has 20 virtual showrooms, with several hundred expected later this year, that lets customers experience new models, customize them in real-time, and take them for a virtual spin. In a very different application, a startup called Surgical Theater uses flight simulator technology and multiple GPUs to allow surgeons to use VR to fly through a patient's anatomy and rehearse complicated procedures before making the first cut. In datacenter, inclusive of Tesla and GRID, revenue rose 18% sequentially to a record $97 million, up 10% year on year. This reflects the extraordinary rise of deep learning, a field in which we are now engaged with nearly 3,500 companies and organizations, as well as growth in the number of high-performance computing applications that are GPU-accelerated. During the quarter, we launched key products for this market. And a number of the partners provided updates to their own work in this area that underscores the central role of the accelerated GPU platform. A key development came during November's Supercomputing 2015 Conference, with the release of the latest list of the worlds top 500 fastest supercomputers. It showed that more than 100 of these systems are now using accelerators. Two-thirds of these use NVIDIA accelerators, up 50% on the year. For hyper-scaled datacenters, we announced a platform that lets web services companies accelerate machine learning. It consists of both the NVIDIA Tesla M40, the most powerful accelerator designed for training deep [nolla] networks, and the NVIDIA Tesla M4, a low-power small form-factor accelerator for machine learning inference. Web services companies have enthusiastically embraced this trend. Shortly after our hyper-scale announcement, Facebook disclosed that it will use the Tesla M40s to power its next-generation computing system for machine learning applications. And earlier this month, AliCloud, Alibaba's cloud computing business, announced it will work with us to promote China's first GPU-accelerated, cloud-based, high-performance computing platform. They joined other web services giants embracing GPUs for machine learning. During the quarter, Google outsourced its TantraFlow deep learning framework, which can be accelerated on GPUs. Microsoft Computational Network Toolkit was integrated with Azure's GPU Lab, enabling neural nets for speech recognition that are up to 10 times faster than their predecessors. And IBM revealed that its Watson systems are now using GPUs. Progress continues to be made in our GRID virtualization platform, which enables companies to deliver graphs-rich applications to employees on any device, anywhere. More than 100 companies are participating in an accelerated deployment program. Turning to automotive. Automotive revenue was a record $93 million, up 18% sequentially, and up 68% year over year. One of the biggest stories at CES was the introduction of our NVIDIA DRIVE PX 2 self-driving car platform, which utilizes artificial intelligence to address the profoundly complex challenge of autonomous driving. As many of you saw, DRIVE PX 2 is a supercomputing platform the size of a lunchbox that processes 24 trillion deep learning operations a second, and delivers 8 teraflops of processing power, equivalent to that of 150 [metso pros]. It is a flexible platform that automotive developers can scale from one to four processors. And it can utilize passive cooling or integrate seamlessly with the water-cooling systems of self-driving EVs. Capable of fusing data from cameras, lidar, radar and ultrasonic sensors, it creates a full 360-degree understanding of what is happening around the vehicle. It localizes the vehicle on an HD map, and it determines a safe path forward for using deep learning techniques. Volvo, well-known for its safety and reliability, will be the first to develop DRIVE PX 2, using it as the brain force fleet of 100 self-driving cars to be publicly available next year in its hometown of Gothenburg, Sweden. Just a couple weeks ago, the first autonomous shuttle, the WEpod, incorporating our deep learning platform, took its inaugural trip on public roads in the Netherlands, where it can be summoned with a smart phone app. The DRIVE PX 2 launch generated enormous interest around the world from car makers, Tier 1 suppliers, and others. We are now collaborating with more than 70 companies that are developing self-driving car technologies. Finally, our OEM and IP business was $198 million, up 3% sequentially, driven by the seasonal demand for notebooks. Now turning to the rest of the income statement. GAAP quarterly gross margin was 56.5%. Non-GAAP gross margin was a record 57.2%, slightly above our outlook. GAAP and non-GAAP gross margins increased from a year ago. GAAP operating expenses for the fourth quarter were $539 million, inclusive of $34 million of restructuring and other charges. Non-GAAP operating expenses, including litigation charges, were $445 million, in line with our outlook. For full FY16, our non-GAAP operating expenses were $1.72 billion, including litigation costs. Our focus on rigorous execution and enhancing efficiencies enabled our core operating expenses to remain flat from FY15, as we focused on expanding operating margins. GAAP operating income for the fourth quarter was $252 million. Non-GAAP operating income was $356 million, up 26% from $283 million a year earlier. GAAP net income was $207 million. GAAP earnings per diluted share were $0.35, including $0.04 of restructuring and other charges. Non-GAAP net income was $297 million. Non-GAAP earnings per diluted share were $0.52, an increase of 21% year over year. Now turning to some key balance sheet items. At the end of Q4, our cash and marketable securities balance was just over $5 billion. During the quarter, we paid $62 million in cash dividends, and we closed our accelerated repurchasing agreement with an additional 4.3 million shares returned. As a result, we have turned to shareholders an aggregate of $800 million in FY16, meeting our intention we communicated at the start of the fiscal year. Over the past four years, we've returned more than $3 billion to shareholders representing 98% of our free cash flow. As part of our ongoing commitment to deliver shareholder value through capital return, our intention is to return $1 billion in FY17 through quarterly cash dividends and share repurchases. For FY16, revenue reached a record $5 billion, up 7%. Our growth platforms increased 26% year on year. Non-GAAP gross margin was a record 56.8%, up 100 basis points on the year. Non-GAAP operating income grew 18% to $1.13 billion, with operating margin expansion up more than 200 basis points to 22.5%. Non-GAAP EPS grew 18%. Now turning to the outlook for the first quarter of FY17. We remain excited about our business prospects. Gaming remains strong and eSports, VR, and new exciting games will lift it further. GP-accelerated datacenters are expanding in both HPC and cloud, driven by the growth of deep learning. And autonomous driving continues to move forward. We have excellent positions in each of these growth markets. We expect revenue for the first quarter of 2017 to be $1.26 billion, plus or minus 2%. Our GAAP and non-GAAP gross margins are expected to be 57.2% and 57.5%, respectively, plus or minus 50 basis points. GAAP operating expenses are expected be approximately $500 million. Non-GAAP operating expenses are expected to be approximately $445 million. GAAP and non-GAAP tax rates for the first quarter of FY17 are both expected to be 19%, plus or minus 1%. Further financial details, including the CFO Commentary and revenue by market platforms are available on our IR website. We will now open the call for questions. We ask you that you limit your questions to just two. Ash, would you please poll for questions at this time?","Yes, thanks a lot, Vivek. First of all, I think you captured the essence of it in your question. GeForce is really not a chip business anymore; it is really a gaming platform business. When you think about it from a gaming platform business, it has to be thought of in the context of the low gaming ecosystem and the gaming industry. It is $100 billion large. When you think about it that way and you drive the business that way and you create value that way, I think the prospects for our growth there is still quite significant. There are several different ways we can grow the market. First of all, when we introduce new game platforms -- and this last couple of years, we've introduced Maxwell, it's the most successful gaming platform we have ever introduced. The installed base of about 100 million GeForce gamers in the world has an opportunity to upgrade to a new platform. Another reason why we can grow is because the production value continues to increase, the graphics richness continues to increase. And we do that by inspiring the industry, providing a technology that helps it include our technology in a much easier way. And the way that we do that is called GameWorks. All the physical simulations, all of the visual simulations, all the lighting simulations and all of the things that makes games beautiful today are easy to include by just supporting GameWorks. It has been an enormous success for us. And of course, developing companies are still doing incredibly well. There are many countries around the world that are just starting to get into PC gaming. Southeast Asia is growing incredibly. And then not to mention that gaming is no longer just gaming. Gaming is all about sports now. But we're starting to see a new culture and a new dynamic in gaming even beyond that. It's really becoming a platform by which people could share, and a platform by which they could artistically express themselves. If you look at some of these games today, it's something you enjoy well-beyond just playing the game. You use the game as an editor to tell stories. And so these games like GTA 5, it's just fantastic for telling stories. And so you can see now that the gaming platform is going beyond games, it's going beyond sports, and now it is a creative platform. So there are just all these wonderful ways that the game industry has continued to be vibrant. And my sense is that we're going to continue to grow with it. Well, we have to be mindful of competition. And there's a lot of ways to compete, there's a lot of different ways to bring value. And surely what you describe is one way to compete. Those aren't really the segments in a market that we'll address. The way we think about infotainment is, there are segments of the infotainment market -- surely the parts of the market that we serve incredibly well -- with the richness of the displays, the number of the displays. And how the displays are going to be used in the coming years are going to continue to expand. You know that display costs are coming down, and outlet displays are becoming cheaper. There are so many different ways to bring visualization into the car, to enhance the driving experience. You can also imagine how artificial intelligence technology can change the way infotainment systems are even used. One of our strategies of course, is to bring artificial intelligence technology to enhance how the driver communicates with the car. So there is all kinds of new technologies that we're going to deploy into the infotainment system, leveraging our expertise in deep learning. The part of the market that you are starting to talk about is the segment of the market that we really introduced into the marketplace, which is the autonomous driving computer. We started talking about it several years ago -- I think it was like three years ago, at CES, when I introduced the DRIVE PX. Where we imagined that in the future, a car would also have a supercomputer inside that is powered by deep learning, that is powered by artificial intelligence. And that takes in the sensor input continuously from the car, what's surrounding the car. And infer from it the appropriate thing to do. That vision three years ago seemed a little bit, if you will, outer space. But I think that it is very clear now that the technology that we are bringing to bear, deep learning, is really the best approach for helping car companies go beyond ADAS, which is going to be a commodity in the coming years, as you can imagine. Go beyond ADAS, and move towards assisted driving to full autonomous driving. So I think that we can add a lot of value there. PX2 was really invented to allow OEMs to scale that entire range, from assisted driving all the way to fully assisted driving. And that is one of the reasons why we can support one chip all the way up to four chips, from passive cooling all the way up to integrating directly into the self-driving EV water-cooling system, that is quite available for most EV cars with liquid cooling. So I think that our strategy there is going to work out quite well. We add a lot of value. It is very algorithm-rich, it's very software-rich. And I think our DRIVE PX platforms is really quite state-of-the-art. Actually, I'm just trying to figure out what your question was. Let me see. I guess I'd be reluctant to announce anything today, but there are semi-custom businesses that people need our help on. And we are open for business to help select partners develop proprietary systems that leverages the wealth of technologies that we have, whether it is in visual computing or deep learning or supercomputing. So we can create systems and products and services that the world has never had before. That's an area that I think is of interest to us. It's an area that we will likely see a lot more success in the future. But there is not much to really announce today. I appreciate that question. There are a lot of moving parts in our gross margins. Of course, our enterprise business is richer in gross margins. Our consumer business tends to be lower in gross margins. But at the highest level, the way I would think of our Company's gross margins, it's the nature of our business model is changing. If you think about our business model, a long time ago, it  used to be a chip business, but today we are a differentiated specialty platform business. What I mean by platform business is, it's of course the chips, the systems, but it's largely about the differentiated software that's on top of it. So increasingly, you're going to find that our business is software-rich, it's services-rich. And if that is the case, one would think that our business model would become increasingly of that nature. And I think you're just seeing the reflection of that. As our Company continues to move towards our differentiated platforms -- which was, call it, 50% just a few years ago, and is now reaching some 80% now -- as we move into these specialty differentiated platforms, the software content is just much, much higher. And our customers who work with us are not buying chips for their systems, for their commodity systems, but they are looking for a platform to solve a particular problem. And the problems that we would help solve, the solution that we bring to bear, is so high-valued, that I think that increasingly you should expect that -- well, you should hope, and I hope myself, that our gross margins continue to move along with the change in our business model. Yes, Hans, thank you. Our high-performance computing business uses an architecture we call accelerated computing. Accelerated computing is a model of computing that we invented almost 10 years ago. It's a very unique way of computing, and it takes advantage of the strength of the CPU, as well as the advantage of the world's most parallel processor, called the GPU. It is very software-intensive, it is very mathematics-intensive, it's very algorithm-intensive. It's a problem that, when applied to some verticals, can accelerate computing dramatically. We see accelerations of 5x, 10x, 20x, quite normally, quite regularly. And the way that you translate this benefit to a customer is that it reduces their costs. Instead of building a supercomputer that may cost as much as $500 million, this supercomputer would be the world's best at $100 million. That's a pretty substantial reduction in expenses. The power-build that they would spend on a regular basis would be dramatically reduced. So datacenters and supercomputing centers save an enormous amount of money. On the other hand, the researchers see a substantial boost in their application throughput. So that is one of the reasons why accelerated computing is doing really well. We are seeing a couple of different drivers for accelerated computing and high-performance computing. Our datacenter business -- accelerated computing itself for supercomputing applications, whether it is weather simulation or molecular dynamic simulation -- continues to grow. The killer app that we are starting to see -- and we have been cultivating for several years now, and it's now really turning the corner and going into turbo-charge and growth -- is deep learning. Almost in every field of science, as well as for web services companies, artificial intelligence helps them wade through, comb through just massive mounds and oceans of data to discover insight. So deep learning and using artificial intelligence technology across all fields of science -- I'm super-excited about the work that's going to be done in medicine. It's really going to see some great adoption. I think we mentioned that in just a couple of years ago, we had 100 companies working with us in the area of deep learning, and now it has ballooned to 3,500. That is quite a large-scale growth. It is in industries all the way from life sciences to supercomputing of course, to web services of course, to even industrial. And the application for industrial would be Internet of Things. All of these sensors all around the world collecting data needs artificial intelligence software, deep learning software, to review insights. In terms of our positioning relative to the competition, this is an area that we have a real advantage, and we have a real advantage for several reasons. The incremental cost to our Company, the incremental cost of engineering accelerated computing into our normal course of running our GPU  business, GeForce business, is incremental. So the system, entire system, the 10,000 people in our Company, can quite easily, if you will, continue this rhythm. And quite a high-velocity rhythm, of bringing out new GPUs that are great for accelerated computing and great for gaming of course, and great for workstations. And it's a natural course of doing work. This is not an adjunct business to us. This is our core business. At the core, that is our fundamental advantage, that it's a singular motion, singular execution, singular investment, singular architecture, incredibly leveraged, and the execution, as a result, is just absolutely flawless. So I think in the end that, that's our greatest advantage. Absolutely. Well, Cray is a very important partner of ours. The thing that is really exciting for me, is to see them transition their business -- not transition, but transform their business, from one that is really focused on supercomputing centers to one that is also working on big data. This is an area where we can add a lot of value to them. We have a lot of expertise in this area. And as they continue to evolve their market footprint beyond supercomputing centers and now into large enterprises, I think they can find a lot of success. They are seeing a lot of this success in this area. Big data analytics is square in their bull's eye, and I'm quite excited for them in the work that they are doing there. They are good customer for us, a great partner of ours and I'm excited to see their ongoing success. The thing we are all seeing is that big-data analytics, the most powerful weapon for big-data analytics has recently been discovered. Deep learning is just a fantastic new computing model. It is able to discover insight that is provably now superhuman. Its dimensionality in thinking through data is unrivaled in any approach that we have learned in the past. And that's one of the reasons why industries all over the world, from life sciences to industry to manufacturing to supercomputing, are jumping on the deep learning bandwagon. I think their adoption of Tesla, the NVIDIA GPU, is going to be quite a success for them. Just as a backdrop, we litigated against Samsung last year. The expenses was what you are referring to. At the core of it, fundamentally, philosophically, we believe that it is inappropriate and it's wrong for Samsung to use NVIDIA's technology -- technology that has cost us billions of dollars to invent and use it without compensating us. At the core, I just think that is just wrong. And we think it's wrong, and that's the reason why we decided to litigate to sue Samsung. The ITC has passed it's early decisions, and we disagree with them, we are disappointed by them. It's unfortunate that the business courts couldn't see through the obviously complex data associated with the technology. But we are disappointed by it. We have appealed for a review, and hopefully, in the near term, we will discover what the ITC will do. But I still believe that it was the wrong thing to do, for Samsung to use technology that companies who are specialized in these fields invent, and to use it without compensation. And I'm disappointed with the decision from the ITC, but so be it. Next year, we have plenty of things to go invest in, and we have plenty of growth drivers. You know that we have four powerful growth drivers in our Company. Gaming is one, VR is another, artificial intelligence and self-driving cars. And we have plenty of growth drivers to go focus our Company on. Two questions there. First of all, our pipeline. We've talked about our pipeline several times. We've shipped probably 5 million, 6 million cars. We have another 20 million, 25 million cars to ship in our pipeline. So these are design wins that took quite a few years to have won, and quite a few years of engineering to ramp into production. So we have a pretty good visibility of the pipeline and the opportunities that are ahead of us. Probably there is some market dynamics that's helpful to some of the design wins, the segments that we serve. Of course, at the time, a long time ago, it's hard to tell. But it's very clear now that the computerization of cars is a highly desirable end-user feature. The partners that we worked with, the car companies we worked with, to computerize their cars, whether it is Audi or Tesla, whose cars are heavily computerized. Their growth prospects in the coming years are quite good. So I think that's one. We have a clear view of the pipeline, and I think the mega-trends of the computerization of cars is in our favor. Now you mention -- secondarily, we introduced this platform called DRIVE PX. It is our economist-driving car computer platform. And the recent success of ADAS has really inspired just about every car company in the world to look beyond ADAS and what's beyond ADAS is self-driving cars. It could be partially assisted, it could be mostly assisted, and it could be completely assisted. And in each one of those levels of autonomy, a different amount of computation would have to be deployed. We've created a scalable architecture that allows car companies to develop cars that are partially assisted, all the way to completely assisted. We are working with quite a large number of customers now. Car companies, startup companies, companies that are largely cloud-based and have an enormous amount of data that they could transform into an automotive service, transportation-as-a-service. And so we are working with a whole lot of different types of companies, and I think this is going to be an area of quite a significant industrial revolution. And arguably quite a gigantic society good in the long term. So anyways, we are working on a lot of projects there. Sure, thanks. I think part of the answer is, I'm not sure. Part of the answer is, I'm not sure. So with that as a disclaimer, let me tell you why I'm so enthusiastic about it. There are many problems that computer science has been trying to solve which, algorithmically, is just impossible to solve. There is no known way of a human-described algorithm that completely captures the noisy and long tail of society. And it could be almost any problem. It can be weather-related-type problems. It could be market-related-type problems. It could be all kinds of purchasing-related challenges, and all kinds of data. It could be life sciences, as we know that the human body is not in a perfect condition all the time, that randomness that plays a role in understanding molecular science. There are so many different types of areas where there is no simple Newtonian physics equation that can describe the nature. So in that particular case, using an enormous amount of data to train the neural net, to train software, to rewrite the software, if you will, using an enormous amount of computation, is a pretty exciting computation model. I think this is a brand-new computing model, one that is going to augment the traditional model of symbolics and computer programming. This is going to be a data-driven type of computing model. In this particular case, GPU accelerated computing is really quite ideal and the computing model that we invented some ten years ago is really quite ideal. How big is it? I think that it could be quite significant. And we are starting to see, of course, the type of companies that are jumping on top of the deep learning bandwagon. They are great companies, from Google to Facebook to Vidu to IBM to Alibaba, to just about every hyperscale web services company in the world, is jumping on this. Because they have enormous amounts of data and it has very, very long tails. And traditional segmentation is too contrived of an approach to find great insight. Now that the companies with a great deal of web-based data, cloud-based data, have already starting to engage in this area, they are starting to implement artificial intelligence into one application after another. I think we already heard them announce that it's very likely they will put artificial intelligence into every single application they have. We are starting to see this sweep across industries. The automotive industry, of course, has the longest tail, as the world is a very noisy place. And in order to create a car that can navigate through it, the long tail of a very complicated world has to be handled somehow. Writing software programs is just not going to do it. So using an ongoing learning artificial intelligence network could be exactly the solution for it. Life sciences, industries, manufacturing, supercomputing, financial services -- the list goes on and on. And we are seeing a lot of enthusiasm. Before everybody can use deep learning, they have to train a network. And this is an area where we have a great deal of expertise. This year, as you know, we also announced our first hyperscale inferencing engine. It's our first end-to-end training-to-inferencing -- inferencing is predictions, the application of the network. So from training all the way to inferencing, we now have a complete architecture that is architecturally compatible. The Tesla M40 is for training, and the Tesla M4 is for inferencing. The M4 is a little, tiny credit card-sized GPU, and very low-power, incredibly energy-efficient. And you can connect it into just about any hyperscale datacenter in the world. And we are sampling customers now. The results are quite exciting. Customers are very enthusiastic about it. I think we could dramatically reduce the cost of datacenters all over the world, as they start to ramp up artificial intelligence in their everyday workload. Yes, Steven, thanks so much for the question. Our inventory levels that we are holding here, they are definitely going to swing a bit in terms of the mix, in terms of our platforms. But what we have right now, we do have a very healthy level of inventory. And we have a great team of people managing all of those different pieces, both for the channel, for our partners, and definitely for what we need to ship going forward. So I don't think we look at a number to exactly optimize in any single one quarter, as we do make sure that we are prepared for the platforms coming down the pipeline, as well as what customers need. But you are correct, it is probably at a fairly healthy low level at this time. Sure. Here is my guess. I think long term, training will be half of the overall market. And the reason for that is because training is so heavyweight. And in the long term -- well, not long term now -- you're training your network constantly. You create a network. You want to improve this network as fast as you can, because you have so much valuable data and so much insight that you can go after. And you deploy the network for inferencing, which collects brand-new data. And the world looks completely different to you. You now collect that data, and you use that data to train your network. I think that network training is going to be continuous basis, and we're seeing that, absolutely. Also there are more types of networks. The type of networks that are being created, the rate of revolution, the rate of innovation of networks -- network styles, network types, network configurations, network depths. It is happening every single week. I'm actually not exactly sure how you would design a custom chip for it. Which explains why there are only two chips today that are successful in inferencing. One of them is the Intel Xeon, and the other is the Tesla M40 and M4. So I think the ability to adapt to new algorithms quickly, it's really quite vital as we go through the next several years of this artificial intelligence revolution. There is just so much algorithms being developed, and I think you guys are reading about it constantly -- new breakthroughs in AI, new breakthroughs in network design. At the moment, I just really don't know how someone would settle down and design a custom chip for it. I happen to believe that long term, artificial intelligence is not a chip. Artificial intelligence is a computing model. And a computing model needs processors. And processors are programmable. And these programmable processors need to have rich software development environments around it, and these platforms need to be available all over the world. Today the NVIDIA accelerated computing platform is available in a PC, in a work station, in a laptop, in the cloud, in a car, in robots, in embedded environments, and it's all exactly the same architecture. I think that, that's really one of our advantages, that we have the ability to be adaptable, programmable, and yet we are available in literally every single computing platform form-factor you can imagine. And the accessibility of NVIDIA's architecture is literally global, worldwide and within reach for anybody. Yes, first of all, I appreciate your question. I think, first of all, it's just the guidance, and it's our best view today, it's our most prudent view today. And as you know, although there are many things we know, the world is a very uncertain place. There's a few things that we do know. The gaming market is quite vibrant. It continues to be quite vibrant. We monitor it literally every day, every week. And we monitor it all over the world, and it remains quite vibrant. In the coming months, there are some really wonderful games that are coming out, that we think are going to be spectacularly successful, whether it is The Division or Tomb Raider or -- the list goes on. So I think the gaming market appears to be quite vibrant. Our automotive business is vibrant, and the work that we do in self-driving is really gaining traction and capturing the imagination of just about every car company around the world. Our deep learning work and supercomputing work, our high-performance computing work is accelerating. In a lot of ways, I understand where you're coming from. But we don't want to ignore seasonality. Q1 is Q1, and we recognize that the market is uncertain. We will see how it plays out. At the end of the quarter, we'll come and report it again. Those are really good questions. We monitor our installed base pretty carefully. Currently in the installed base, we basically have three architectures still in operation. We have the Maxwell architecture, and we have the Kepler architecture, and the Tesla architecture. All of those -- oh, excuse me, the Fermi architecture. Those architectures are all running in the installed base at the moment. We have managed upgrade about one-third of the installed base. Meanwhile, it is the case that ASPs of our chief use are going up, because the graphics richness and the graphics production value is going up. The quality of games -- because the market for games is so high, game developers can really create much more beautiful games, and take the risk to do that. The developing markets are growing. The number of genres, like eSports, of games are growing. So there's a lot of different growing vectors. Meanwhile, all of that is on top of our desire to upgrade our installed base, so they can enjoy games the way that it ought to be enjoyed. I think there is still a fair amount of growth opportunity ahead of us, and we will monitor it carefully and report it once a quarter. Yes, Deepon, thanks for the question. I was delighted myself. I will just put that out there. We worked really hard to improve our Quadro business. The team works incredibly hard. We invented a new technology for rendering called Iray. It's the world's first physically modeled photo-realistic renderer that is accelerated by GPU. The results of it is really quite remarkable, and they continue to add new capabilities to it. We, this last quarter, also benefited from the enthusiasm and excitement around VR. And we have VR SVKs and collaborations with just about every ISB in the world that is working on VR. So I think there's a lot of good reasons to be enthusiastic about Quadro. We don't believe for a moment that the design quality and design production value of movies or games or architecture or manufacturing will continue. We believe absolutely that it's going to continue to improve. And visual realism and the productivity of the engineers that are involved, the artists that are involved, needs to continue to increase. So we think this is going to be a vibrant growth area ahead. I think that what drove recently the uptick on the OEMs refreshing workstation cycle. And I think we should enjoy some of that for the coming quarters. But I still think long term, the real opportunity, surely the market is there. We know that more and more of design and creativity is done digitally. So at the core, the market is there. The opportunity for us is to bring new forms of rendering, new forms of design. And as you can see, Iray is for rendering, and VR is for interacting with the design. These type of capabilities require just an enormous amount of GPU capability. And I think at the core, that is going to be our long-term growth drivers. Sure. For some reason, I still tend to believe that what drives the gaming market is great games. And I think that there is some evidence that the continued release of great games and great production value games, the vibrancy of eSports, the fact that eSports is really not just for competition, but drives the dynamic of sharing and social. Those kind of factors continue to drive our gaming business. I'm quite enthusiastic about the developing markets. Southeast Asia, for one, is really starting to adopt PC gaming quite rapidly. It is a market that is extremely underserved. India is a market that is extremely underserved. They are underserved because broadband Internet hasn't been available to those marketplaces until just recently. And there are surely demographics in these markets that would love to jump onto gaming. Who doesn't need a PC? So almost every market develops around PCs quite rapidly. I think the way of enjoying games is so affordable by adding a GeForce GTX to a PC that you already own, it's the most affordable form of entertainment, if you think about it that way. Most of eSports are free to play anyhow. So much of it is, anyhow. So it is a wonderfully affordable way to enjoy entertainment. So I think the Southeast Asia, India, are really quite exciting developing markets. Yes. There are really two questions in your question; they are both good. One question is, how do I feel about VR and its impact on gaming? The second part is, how will VR impact our business? Let me tell you the second part first. We are not forecasting and not assuming any upside in VR. However, there's no way but good that VR will bring to our business. And we will take it a day at a time. We believe that it's going to be an exciting growth driver. We believe that is going to be helpful to our high-end GPU business. But when the time comes, it will be a nice bonus. So we're going to run our business as if we don't count on it. However, obviously we care very deeply about it, because we think that the experience of VR is quite amazing. Anybody who has tried it is surprised how immersive it is, how it takes you away from where you are and into another world. You are really suspended in disbelief, and it's as close to a holodeck as we have ever experienced. So we believe strongly that VR is going to be fantastic for entertainment, it's going to be fantastic for games. We also believe that it's going to be fantastic for all of our pro-business. I wouldn't be surprised if the segment of our business that it helps the earliest may very well be our professional business. The reason for that is because there are many applications that are mission-critical. Even though the headsets are not free, it's quite affordable. And for people who have power walls and use large displays, VR is actually an incredible cost reduction. Almost anybody can now have a virtual reality cave, which cost tens of thousands of dollars. Anyone can now have a power wall, which costs tens of thousands of dollars. And now for just a few hundred dollars, have all the benefits of that. So I think that you could tell that I'm very enthusiastic about it. We are developing a lot of fresh technology and new enabling technology to make it possible. We are working with all the market leaders to develop the market and cultivate the market. And then from a financial perspective, we will just see how it plays out. My sense is that it's going to be a really nice bonus. Thanks a lot, Chris. Well, my sense is that it will stabilize, and let me tell you why. Our OEM business is not about gaming, because it is our gaming business. And the OEMs are not about design, because that's our design business. But the thing that some OEMs do is, they include our technology to differentiate a mainstream platform from a premium platform. So by adding NVIDIA's technology, you turn a commodity PC into a premium PC. And the experience is better, the performance is better, and everything just works. So there is a real benefit in using it as a premium multimedia PC, if you will. So we'll probably see continued interest in doing that. And we are delighted by that. We don't count on it, but we're delighted by it. So my sense is that it will likely stabilize. I don't know if I have the precise granularity by geography, except for just a few countries. But let me address it overall. The installed base takes a couple two or three years to upgrade. And on the lower end, three to four years. On the higher end, one to two years. And so overall I guess, because the lower-end products are higher in volume, it would probably weigh the overall average to call it three-plus years. But the ASP, of course, if you look at it from an ASP perspective, it's a little bit different -- it would probably drift up. The rate of upgrade appears to be increasing. I might also explain that the ASP of our overall portfolio increasing. I think the reason why it's increasing is because the size of the gaming market has now grown to a level that developers can take a fair amount of risk to add a rich content, rich production value in graphics, which they didn't used to be able to do. They now have the benefit of a large installed base, a PlayStation an Xbox, a Nintendo and PCs, that they could actually create content that is really, really quite beautiful and technologically rich. Which drives up the adoption of higher-end GPUs, because you need higher-end GPUs to process it. I think that is quite a significant factor. I think the other factor is that the game consoles, although nowhere near the performance level of our high-end GPUs, on average, is higher in terms of capability than our average installed base. That is actually terrific news for us, because a PC gamer who wants to enjoy games that are adapted from game consoles, which all of them are, now would have to upgrade their GPUs to enjoy at least a game console experience. So I think that bottom half of our installed base has a real opportunity to enjoy at least a game console experience for just $150. For a $150 graphics card, you can get experience that is superior to a game console. That is quite an amazing value. That, I think, is also another reason to spur adoption. There are two different questions in there. One question is, what is the minimum requirement for VR today? Using today's graphics card as an example, the GTX 970, which is the most popular graphics card in the world, is the min spec for the Oculus Rift. And the reason why, of course, is because Oculus and their PC focus wants to have the best possible experience for the early adopters of VR, and I think that is a really prudent strategy. You want to delight all of your early adopters with the best possible experience. But the way to think about it long term is that, as the market continues to grow and more content comes and VR moves into the mainstream, there is no question in my mind that our $100 and $150 GeForce GTX cards will, in the future, be able to play VR just perfectly. So this is not a question about the availability of technology or the cost of technology. As we know, technology continues to advance, and whatever experience today will continue to get more affordable long term. Whatever the Sony PlayStation does, I think is just fantastic either way. What we would like to do is get people excited about VR. And in the final analysis, there are TV gamers and then there are -- there are console gamers and there are PC gamers. They are different genres, and they are different applications and different styles, and very largely different customers. So I think we are just enthusiastic about VR, period. And over time, the technology will more and more affordable. Sure. I appreciate the question. It's really a good question. This is really a matter of philosophy. And philosophically, this is how we see the world. We believe the self-driving car is not a solved problem. I say that as a statement of fact. I don't think anybody would dispute it. I also believe that self-driving cars is a field that's going to require the technological muscle of a very, very large industry, and that no one company with a few hundred employees is going to solve it all by themselves. The idea that a unsolved problem of such incredible daunting levels, that an entire computer industry is in the process of trying to solve, could possibly be a closed system tied around a chip, seems illogical to me. That's number one. Number two, I believe that long term, our car company, the soul of the car company is the driving experience of that car. The soul of the company is the safety record of that car. The soul of the company is the functionality of that car. In the future, the functionality, the safety, the driving experience of the car is going to be largely software-defined. It's going to be artificial intelligence network-defined. I just can't imagine great companies like BMW and Mercedes and Audi and all of the world -- Toyota, and the list goes on -- and many great companies that are emerging into this marketplace. I just can't imagine that these companies would somehow outsource the soul of their car to a chip company. That is a second philosophical belief. So what we have decided to do is to create an automotive autonomous car computing platform and all of the rich software that is necessary to enable this incredibly high-throughput computer to behave in a really energy-efficient way and cost-effective way. And to be able to apply our deep learning expertise, so that these cars can benefit from artificial intelligence to solve these really complicated world problems. And that by partnering with every single car company in the world, that together we might be able to solve this incredibly daunting challenge, and hopefully bring some society good. So that is our approach as the open platform. And it starts really from a philosophical approach. Now, that philosophical approach results in a very substantial technological difference. Notice that our platform is completely programmable. We have rich tools. We know that developers all over the world can easily buy themselves a GeForce TITAN and write CUDA applications. And those CUDA applications will tomorrow run on a DRIVE PX just seamlessly. So I think that is a wonderful way for designers all over the world to be able to develop software -- which is really hard to do right now -- and then quickly deploy it into the car. So our strategy is just very, very different, and that is our approach. And my sense is, at the moment, it appears to be quite a good approach. Okay, I really appreciate that question. NVIDIA is the world leader in visual and accelerated computing, which is helping to create exciting growth markets like VR, AI, and self-driving cars. Which will transform many industries and positively impact the future of society. Our strategy is to leverage one core investment into four growth markets -- gaming, professional visualization, datacenter and auto. And it's delivering results and gaining momentum. Our goal is to balance investments to capture the enormous opportunity ahead, while maintaining a keen focus on improving near-term financial performance. I also want to remind everyone that our annual GPU Technology Conference will take place April 4th through the 7th in San Jose. We will be focusing on VR, artificial intelligence and autonomous driving. We will also be holding Analyst Day there, and I look forward to seeing all of you. Thank you.","Thanks for taking my question, and congrats on the very good growth and execution. My first question, Jen-Hsun, is on the gaming segment. For the last two years, it has grown at over 30% a year. I don't know many other multi-billion-dollar businesses and semis that are growing at this pace. The question really is, how sustainable is this growth? I understand the drivers. But could you help us ballpark that going forward over the next two or three years? Do you think of this as a 10%, 15%, 5% growth opportunity? Any guidance there would be extremely helpful. Got it. And as a follow-up, Jen-Hsun, on your automotive business, as you move from entertainment systems that are graphics-intensive to more advanced computing systems, do you think it changes the competitive environment? And what I am referring to is, we have seen Qualcomm and others enter the segment, and they are making the case they can integrate a lot of different [piece] parts and tie back to their processor. And I'm trying to draw a parallel with what you had on the smartphone side, where you were specializing in one part, but others could integrate other parts and become more successful. Is there going to be a similar situation in orders? Or do you think we should read it in a different way as to how your competitive situation can evolve in orders as computing becomes a bigger part of the application? Thank you for taking my questions. The first question, on the TDS business, how -- I'm sorry, the Tegra development services, you noted that, that was an important driver of growth. Can you give us a little bit more color on that business? How big is that? What end markets are you working with? What are you helping customers do? Can you hear me? Okay, fair enough. And second question on the gross margins. They've gone over the last three years from the low-50%s, pushing through 57% now. At what point do these [asset tow out]? How should we be thinking about that? Thank you. Great, thanks, and congratulations, guys. Jen-Hsun, can you give us an update on the high-performance compute side of the business? How will Pascal compare to the upcoming other solutions in the market, specifically Intel's Knights Landing? And I have a follow-up. Thanks. Very helpful, thanks, Jen-Shun. Hi, this is Gabriel Ho calling in for Ambrish. Thanks for taking my question. I want to follow up on your Tesla business. I think in a recent earnings call, Cray actually reiterated its expectation of over 50% of this $25 million revenue in the fourth quarter of this calendar year. And I think they cited DRIVE as one of three supercomputers that actually use Pascal. So how should we think about the benefit to your Tesla business? Thanks. As a follow-up, [your spend] seems to spend about $90 million in legal expense. How should we think about in FY17? And also, and can you give an update where you are in the case of [Qualcomm] and Samsung? Good afternoon, thank you for taking my first question. I guess first question, on the auto side. I'm trying to get my arms around how we should think about growth year in calendar 2016 off this 75% growth in 2015. If you could parse between your backlog for infotainment and your outlook there, as well as what kind of ramp you see with product development contracts on the ADAS side? Hi, thanks for taking my questions. Jen-Hsun, the first one for you, if I could. In terms of the deep learning, machine learning opportunity, I was wondering if you could help quantify sort of the longer-term silicon TAM for the opportunity, both in datacenter and automotive? And I guess more near term, any thoughts on what kind of development revenues could be generated for these machine learning-type platforms in the near term? Okay, I appreciate that color, Jen-Hsun. And as a quick follow-up, I have one for Colette on the inventory levels. Colette, just given where inventories ended for the quarter, it's -- on a days and a dollars basis, it's roughly comparable to a number of quarters back, when revenues were about 20% to 30% lower than where it is today. Is this a new level that the Company can continue operating at, just based on supply chain efficiencies? Or is there some -- a seasonal volatility in December or any other kind of average ASP of the [prox] that you are carrying? Thanks. Hi, Jen-Hsun. One question on deep learning. I was wondering if you could talk about relative opportunity sizes in training and inference part of the deep learning? Clearly you have a strong position on the training side. I would love to hear your thoughts of the inference side. My understanding is, a lot of custom chips are being built in the industry on the inferencing side. So I would love to get your color on what competition you're foreseeing on that front? Thanks for letting me ask a question. One for either Jen-Hsun or Colette. In your first-quarter guidance, it looks like the down-10% has some seasonality to it. But you also have a lot of businesses that have secular trends behind them. So I was hoping that you could provide a little color on seasonality versus secular? Or which of the drivers would be better or worse than that 10%? Acknowledging also that you lose a week of business guiding into that April quarter. Hi, good afternoon, and congratulations on another solid quarter. Jen-Hsun, you talked about the installed base and the upgrade opportunity in gaming, I think, last call. You quantified it as around three-quarters of the installed base that really needs a more updated GTX processor. Given the 37% growth in the gaming business last year and the success of Maxwell, it actually does appear that you did drive some meaningful upgrade in the installed base. So the question for you is, is there any updates to your views on where the installed base sits at from that upgrade opportunity? And then the second question is, do you get a sense that the cadence of these upgrades will be accelerating, given the advancements you and your gaming engine partners are bringing to the market every year? Yes, thanks a lot, guys. So Quadro grew for the first time, I think, in several quarters. Can you describe what drove that growth, and how we should think about the growth profile for the rest of this year? Is it possible that it could grow in the mid-single-digits for all of 2017? Great, thank you. Can you give us some color on what you are seeing in emerging markets, with a lot of macro concern about some of your end markets, notably China? And you guys keep putting out very good numbers. Can you just sort of talk about that demand by geography, and how you weigh the economic and currency risks over the course of this year? Thank you very much. Jen-Hsun, there's been a lot of speculation about how the emergence of VR headsets, in particular for gaming, would drive your GPU business. I guess I would like to hear your perspective on a couple of things. One, obviously it requires high-end and high GPU use to support some of these applications. So of the early adopters, maybe a sense of which of those folks might already own those type of GPUs, and which ones might have to upgrade in the near term? And second, how VR might penetrate the [East Forts] phenomenon over time and drive more of those upgrades into the higher-volume mainstream parts of the gaming market? Thanks. Thanks very much for taking my question, and congrats on the good quarter. First of all, we've talked a lot about some of the really exciting businesses that have a lot of great growth prospects for you. I thought we would look at, as well, some that have been a little more stagnant. And I was curious to your take on, for example, the OEM business. It looks like it may finally be stabilizing after declining. How do you look at that business in the coming fiscal year? Do you think it can even get back to growth, or do you expect continued declines? Hey, guys, thanks for the question, and great quarter. Speaking about your installed base briefly, can you guys share with us your view on GPU replacement rates? How fast are they or how long are they now, and whether they are shrinking or speeding up here? And also perhaps how they might differ by geography? Thanks, and congrats as well. Just a follow-up question on the virtual reality market and how you are looking at that. Can you talk a little bit about some of the PC requirements that are going to be necessary to use in Facebook, Oculus Rift headset when it comes out this quarter? And the cost that is going to be needed for the individual user? And contrast that from the business model that Sony is employing, which is basically, based on my knowledge, a bundling strategy with their PlayStation 4, which wouldn't require an upgrade to the graphics card or buying a new desktop? Just wondering how you think about those two different business models? Thanks for squeezing me here at the end. I'll be very brief. The opportunity in automotive obviously continues to grow. And your unveiling at CES was impressive by anyone's standards. However, there seems to be two approaches to the automotive market as we move forward. There is obviously the closed system approach that Mobile [Life] seems to be pursuing, and what appears to be more of an open-architecture approach that you seem to have. Can you maybe describe how things shape up for you as you look out over the horizon with sensor fusion and the various OEMs, and getting things right from a safety standard space, et cetera? Do we need the control of a closed system, or can we get it done with collaboration with others? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q1 2017,1689,5425,1755,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of FY17. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that today's call is being webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until May 19, 2016. The webcast will be available for replay up until next quarter's conference call to discuss Q2 financial results. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without our prior written consent. During the course of this call, we may make forward-looking statements based on current expectations. These forward-looking statements are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 12, 2016 based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Arnab. In March, we introduced our newest GPU architecture, Pascal. This extraordinary scalable design built on the16 nanometer finFET process provides massive performance and exceptional power efficiency. It will enable us to extend our leadership across our four specialized platforms, gaming, professional visualization, datacenter and automotive. Year-on-year revenue growth continued to accelerate, increasing 13% to $1.3 billion. Our GPU business grew 15% to $1.08 billion from a year ago. Tegra processor business was up 10% to $160 million. Growth continued to be broad-based across all four platforms. Record performance in datacenter was driven by the adoption of deep learning across multiple industries. In Q1, our four platforms contributed nearly 87% of revenue, up from 81% a year earlier. They collectively increased 21% year-over-year. Let's start out with our gaming platform. Gaming revenue increased 17% year-on-year to $687 million, momentum carried forward from the holiday season, helped by the continued strength of Maxwell-based GTX processors. Last weekend at DreamHack Austin, we unveiled GeForce GTX 1080 and GTX 1070, our first Pascal GPUs for gamers. They represent a quantum leap for gaming and immersive VR experiences, delivering the biggest performance gains from the previous generation architect in a decade. Media reports and gamers have been unanimously enthusiastic. The Verge wrote, what NVIDIA is doing with its new GTX 1000 series is bringing yesteryear's insane high-end into 2016's mainstream. We also extended our VR platform, by adding spatial acoustics to our VRWorks software development kit that helps provided an even greater sense of presence within VR. We introduced simultaneous multi-projection, enabling accurate efficient projection of the real world to surround monitors, VR headsets, as well as future displays. To showcase these technologies, we created our own amazing open-source game called NVIDIA VR Funhouse available on Steam. In addition, we've announced Dazzle, an in-game photography system which enables gamers to capture high-resolution and VR scenes within their favorite games. Moving to professional visualization. Quadro grew year-on-year for the second consecutive quarter. Revenue rose 4% to $189 million. Growth came from higher-end products and mobile workstations. We've launched the M6000 24 gig, and are seeing good success among multiple customers, including Toyota and Pixar. [Roche] is using the M6000 to speed its DNA sequencing pipeline by 8 times, enabling more affordable genetic testing. We see exciting opportunities for our Quadro platform with Virtual Reality, and NVIDIA Iray, a photorealistic rendering tool that enables designers effectively to walk around their creations, and make real-time adjustments. Moving to datacenter, revenue was a record $143 million, up 63% year-on-year, and up [40%] sequentially, reflecting enormous growth in deep learning. In just a few years, deep learning has moved from academia, and is now being adopted across the hyperscale landscape. We expect growing deployment in the coming year, among large enterprises. GPUs have become the accelerator of choice for hyperscale datacenters due to their superior programmability, computational performance and power efficiency. Our Tesla M4 is over 50% more power efficient than other programmable accelerators for applications such as real-time image classification for AlexNet, a deep learning framework. Hyperscale companies are the fastest adopters of deep learning, accelerating their growth in our Tesla business. Starting from infancy three years ago, hyperscale revenue is now similar to that from high-performing computing. NVIDIA GPUs today accelerate every major deep learning framework in the world. We power IBM Watson and Facebook's Big Sur server [through our] AI, and we are in AI platforms at hyperscale giants such as Microsoft, Amazon, AliBaba and Baidu, for both training and real-time imprints. Twitter has recently said, they used NVIDIA GPUs to help users discover the right content among the millions of images and videos shared every day. During the quarter, we hosted our seventh annual GPU Technology Conference. The event drew record attendance, with 5,500 scientists, engineers, designers and others across a wide range of fields, and featured 600 sessions and 200 exhibitors. At GTC, we unveiled the Tesla P100, the world's advanced GPU accelerator based on the Pascal architecture. The P100 utilizes a combination of technologies including NVLink, a high-speed interconnect [allowing] application performance to scale on multiple GPUs, high memory bandwidth, and multiple hardware features designed to natively accelerate AI applications. The Next Platform, an enterprise IT site, called it a beast, in all of the good sense of that word. Among the first customers for our Pascal accelerator with the Swiss National Computer Center, which will use it to double its speed of Europe's fastest supercomputer. And GTC, we also announced the DGX-1, the world's first deep learning supercomputer. Loaded with eight P100s in a single box, interconnected with MVLink, it provides the deep learning performance equivalent to 250 traditional servers. DGX-1 comes loaded with a suite of software designed to aid AI and application developers. Universities, hyperscale vendors and large enterprises developing AI-based applications are showing strong interest in the system. Among the first to get DGX-1, will be the Massachusetts General Hospital. It launched an initiative that applies AI techniques to improve the detection, diagnosis, treatment and management of diseases, drawing on its database of some10 billion medical images. In our GRID graphics virtualization business, we are seeing interest across a variety of industries, ranging from manufacturing, energy, education, government and financial services. Finally in automotive, revenue continued to grow reaching $113 million, up 47% year-over-year, and up 22% sequentially, reflecting the growing popularity of premium infotainment features in mainstream cars. NVIDIA is working closely with partners to develop self-driving cars, using our end-to-end platform with Tesla in the datacenter, and extends with the deployment with DRIVE PX 2. Since we have unveiled DRIVE PX 2 earlier this year, worldwide interest has continued to grow among car makers, tier 1 suppliers, and others. We are now collaborating with more than 80 companies, using the open architecture of DRIVE PX to develop their own software and driving experiences. At GTC, we demonstrated the world's first self-driving car, trained using deep learning, and showed its ability to navigate on roads without lane markings, even in bad weather. Additionally, we announced that DRIVE PX 2 will serve as the brain behind the new Roborace initiative in the Formula E racing circuit. The circuit will include 10 teams, racing identical cars, all using DRIVE PX 2. Beyond our four platforms, our OEM IP business was $173 million, down 21% year-on-year, reflecting weak PC demand. Now turning to the rest of the income statement. We had record GAAP and non-GAAP gross margins for the first quarter, at 57.5% and 58.6%, respectively. Driving these margins was the strength of our Maxwell GPUs, the success of our platform approach, and strong demand for deep learning. GAAP operating expenses for the first quarter were $506 million, and declined from $539 million in Q4 on lower restructuring charges. Non-GAAP operating expenses were $443 million, flat sequentially and up 4% from a year earlier, reflecting increased hiring for our growth initiatives, and development-related expenses associated with Pascal. GAAP operating income for the first quarter was $245 million, up 39% from a year earlier. Non-GAAP operating income was $322 million, also up 39%. Non-GAAP operating margins improved more than 470 basis points from a year ago to 24.7%. For the first quarter, GAAP net income was $196 million. Non-GAAP net income was $263 million, up 41% fueled by the strong revenue growth, and improved gross and operating margins. During the first quarter, we'd entered into a $500 million accelerated share repurchase agreement, and paid $62 million in quarterly cash dividends. Since the restart of our capital return program in the fourth quarter of FY13, we have returned over $3.5 billion to shareholders. This represents over 100% of our cumulative free cash flow for FY13 through this Q1. For FY17, we intend to return approximately $1 billion to shareholders through share repurchases and quarterly cash dividends. Now turning to the outlook for the second quarter of FY17. We expect revenue to be $1.35 billion, plus or minus 2%. Our GAAP and non-GAAP gross margins are expected to be 57.7% and 58.0%, respectively, plus or minus 50 basis points. GAAP operating expenses are expected to be approximately $500 million. Non-GAAP operating expenses are expected to be approximately $445 million. GAAP and non-GAAP tax rates for the second quarter FY17 are both expected to be 20%, plus or minus 1%. Further financial details are included in the CFO commentary, and other information available on our IR website. We will now open the call for questions. Operator, could you please poll for questions? Thank you.","Yes, Vivek, thank you. Our PC gaming platform, GeForce is strong and getting stronger than ever, and I think the reason for that is several-folds. First of all, our GPU architecture is just superior, and we dedicated an enormous amount of effort to advancing our GPU architecture. I think the engineering of NVIDIA is exquisite, and our craftsmanship is really unrivaled anywhere. The scale of our company in building GPUs is the highest and the largest of any company in the world. This is what we do, this is the one job that we do. And so, it is not surprising to me that NVIDIA's GPU technology is further ahead than any time in its history. The second thing, however, it's just so much more than just chips anymore as you know. Over the last 10 years, we've started to evolve our Company to much more of a platform company. And it's about developing all of the algorithms that sit on top of our GPUs. GPU is a general purpose processor. It's a general-purpose processor that is dedicated to a particular field of computing, such that it is computer graphics here, physics simulation, et cetera. But the thing that's really important all of the algorithms that sit on top of it. And we have a really fantastic team computational of mathematicians that captures our algorithms and our know-how into GameWorks, into the physics engine, and recently the really amazing work that we're doing in VR that we have embodied into VRWorks. And then, lastly, lastly it is about making sure that the experience always just works. We have a huge investment, in working with game developers all over the world. From the moment that the game is being conceived of, all the way to the point that it is launched. And we optimize the games on our platform. We make sure that our drivers run perfectly. And even before a gamer downloads or buys into a game, we've already updated their software so that it works perfectly when they install the game. And we call that GFE, GeForce Experience. And so, Vivek, it's really about a top to bottom approach. And I haven't even started talking about all of the marketing work that we do, in engaging the developers, and engaging the gamers all over the world. This is really a network platform, and all of our platform partners that take our platform to market. And so it's a pretty extensive network, and it's a pretty extensive platform. And it's so much more than chips anymore. Yes, thanks. You know that I have been rather enthusiastic about high-performance computing for some time. We've been evolving our GPU platforms, so that it's better at general-purpose computing than ever. And almost every single data center in the world, every single server company in the world are working with us, to build servers that are based on GPUs, based on video GPUs and high-performance computing. One of the most important areas of high-performance computing has been this area called deep learning. And this deep learning -- deep learning as you know, as you are probably starting to hear, is a brand-new computing model that takes advantage of the massively parallel processing capability of the GPU, along with the big data that many companies have, to essentially have software write algorithms by itself. Deep learning is a very important field of machine learning, and machine learning is now the process of revolutionizing artificial intelligence, making machines more and more intelligent, and using it to discover insights that quite frankly, is impossible otherwise. And so, this particular field was first adopted by hyperscale companies, so that they could find insight, and make recommendations, and make predictions from the billions of customer transactions they have every day. Now it is in the process of moving into enterprises, but in the meantime hyperscale companies are now the process of deploying our GPUs, and deep learning applications into production. And so, we've been talking about this area for some time, and now we're starting to see the broad deployment in production. So we're quite excited about that. Well, decelerate, I guess, I'm not sure I recall that. The thing about HPC, about GPU computing is, as you know this is a new computing model, and we've been promoting this computing model for close to seven years. And a new computing model doesn't come along very frequently. In fact, as I know it, I do not know if there's a new computing model that's used anywhere, that has been revolutionary in the last 20 years. And so, GPU computing took some time to develop. We've been evangelizing it for quite some time. We developed robust tools, so that would make it easier for people to take advantage of our GPUs. We have industry expertise in a large number of industries now. We have APIs that have been created for each one of the industries. We've been working with the ecosystem in each one of the industries, and developers in each one of the industries. And as of this time, we have quite a large handful, quite a large number of industries that we accelerate applications for. And so, I think that  -- I guess, my recommendation -- my recollection would be is that, that it has taken a long time, in fact, to have made GPU computing into a major, new computing model. But I think at this point, it is pretty clear that it's going mainstream. It is really one of the best ways to achieve post Moore's law error of computing acceleration, and a lot of (inaudible) competition. And the one that, of course, that is very big deal is deep learning and machine learning. This particular field is a brand-new, new way of doing computing for a large number of companies, and we're seeing traction all over the place. Yes, Stephen, I would expect that all of our businesses grow in Q2. And so, it is across the board. We are seeing great traction in gaming. Gaming as you know, has multiple growth drivers covers. But partly of the gaming is growing, because the production value of game is growing, partly because the number of people who are playing is growing. E-sports is more popular sport than ever, sports spectatorship is more popular than ever. And so, gaming is just a larger and larger market, and it's surprising everybody. And the quality of games is going up, which means that the complexity of GPUs has to go up. High-performance computing has grown, and the killer app is machine learning and deep learning. And that's going to continue to go into production from the hyperscale companies, as we expand our region to enterprises all over the world now. Companies who have a great deal of data that they would like to (inaudible). Automotive is growing, and we're delighted to see that the enterprise is growing as well. Sure. Thanks, Stephen. Yes, our gross margins within the quarter for Q1 did hit record levels, just due to very strong mix across our products, on the Maxwell side both from a gaming perspective, as well as what we have in enterprise for pro visualization and data center. As we look to Q2, a good review of where we also see gross margins, and those are looking at a non-GAAP at about 58%.  So, again, be a strong component of that. As the launch of Pascal will come out with high-end gaming and with datacenter, and the growth essentially across all of our platforms will help our overall gross margins. As we go forward, there is still continued work to do. We're here to guide just one quarter out, but we do have a large TAM in front of us on many of these different markets, and the mix will certainly help us. We're in the initial stages of rolling out what we have in software services, our overall systems. So I don't expect it to be a material part of the overall gross margin, but it will definitely be a great value proposition for us, for what we put forth. Yes, thanks a lot, Deepon. We're expecting a lot out of Pascal. Pascal was just announced with 1080 and 1070, and both of those products are in full production. We're in production with Tesla P100. And so, all of our Pascal products that have we've already announced are in full production, so we're expecting a lot. Yields are good and building these semiconductor devices are always hard, but we're very good at it. And this is now -- a year behind when the first 16 nanometer finFET products went into production at TSMC. They have yields under great control. TSMC is the world's best manufacturer of semiconductors, and we work very closely with them, to make sure that we are ready for production. And we surely wouldn't have announced it, if we didn't have manufacturing under control. So we're in great shape. Yes, thanks. The thing that's most important, is that the value is greater than ever. And one of the things that we know is games are becoming richer than ever, production value has become richer than ever. And gamers want to play these games with all of the settings maxed out. They would like to play at a very high resolution, and they want to play it at very high frame rates. When I announced 1080, I was showing all of the latest, the most demanding games running at twice the resolution of a game console, at twice the frame rate of the game console, and it was barely even breathing hard. And so, I think one of the most important things is for customers of this segment, they want to buy a product that they can count on, and that they can rely on to be ready for the future generation games. And some of the most important future generation games are going to be in VR. And so, the resolution is going to be even higher, the frame rate expectation is 90 hertz, and the latency has to be incredibly low, so that you feel a sense of presence. And so, I think the net of it all, is that the value proposition we deliver with 1080 and 1070 is just through the roof. And if you look at the early response on the web, and from analysts, they're quite excited about the value proposition that we brought. Yes, CJ, thanks a lot. I think a lot -- the answer to a lot of your questions is I don't know. However, there are some things I do know very well. One of the things that we do know is that high-performance computing is an essential approach for one of the most important computing models that we know today, which is machine learning and deep learning. Hyperscale datacenters all over the world is relying on this new model of computing, so that it can harvest, it could study all of the vast amounts of data that we're getting to find insight for individual customers, make the perfect recommendation, predict when somebody would anticipate, would look forward to in terms of news or products or whatever it is. And so, this approach of using computing is really unprecedented, and this is a new computing model, and the GPU is really ideal for it. And we have been working on this for coming up on a decade. And it explains one of the reasons why we have such a great lead in this particular aspect. The GPU is really the ideal processor, if you use massively parallel problems. And we've optimized our entire stack of platforms from the architecture, to the design, to the system, to the middleware, to the system software, all the way to the work that we do with developers all over the world, so that we can optimize the entire experience to deliver the best performance. And so, this is something that has taken a long time to do. I have a great deal of confidence that machine learning is not a fad. I have a great deal of confidence that machine learning is going to be the future computing model for a lot of very large and complicated problems. And I think that all of the stories that you see, whether it's the groundbreaking work that's done at Google, and Google DeepMind on AlphaGo, to self driving cars, to the work that people are talking about, and artificial intelligence recommendation chatbots to -- boy, the list just goes on and on. And I think that it goes without saying, that this new computing model in the last couple of years has really started to deliver very promising results, and I would characterize results as being superhuman results. And now they're going into production. And we're seeing production deployments, not just in one or two customers, but basically in every single hyperscale data center in the world, in every single country. And so, I think this is a very big deal. And I don't think it's a short-term phenomenon, and the amount of data that we process is just going to grow. And so, that those are some of the things I do know. Yes, thanks, Mark. Well, part of transitions are always tricky, and we take it very seriously, and there's several things that we do know. We have a great deal of visibility to the channel. And so, we know how much inventory is where, and of which kind. And secondarily, we have perfect visibility into our supply chain, and both of those matters, we've taken into account, when we launch a new product. And so, anything could happen. The fact of the matter is we are in a high tech business, and high tech is hard. The work that we do is hard. The team is -- doesn't take it for granted and we're not complacent about our work. And so, I think that I can't imagine a better team in the world that is to manage this transition. We manage transitions all the time. And so, we do not take it lightly, however, you're absolutely right. I mean, it requires care, and the only thing I can tell you is that we're very careful. The founders edition is something we did, as a result of demand from the end-user base. The founders edition is basically designed by -- a wholly designed by NVIDIA product. A reference design is really not designed to be an end product. It's really designed to be a reference for manufacturers to use as a starting point. But a founders edition is designed so that it could be manufactured, it could be marketed, and customers can continue to buy it from us, for as long as they desire. Now our strategy -- our strategy is to support our global network of adding card partners, and we're going to continue to do that. And we gave them -- we gave everybody reference designs like we did before. And, in this particular case, we created the founders edition so that people who like to buy directly from us, people who like our industrial design, and people who would like the exquisite design and quality that comes with our products that we can do. And so, it's designed to be extremely [over clockable]. It's designed with all the best possible components. And if somebody would like to buy products directly from us, they have the ability to do that. I expect that the vast majority of the add-in cards will continue to be manufactured by our add-in card partners, and that's our expectation, and that's our hope. And I don't expect any dramatic change in the amount of shifting of that. So that's basically it, Founders edition, the most exquisitely engineered add-in card the world has ever seen, directly from NVIDIA. Well, I think you just said it. Depending on which one of our businesses that you're talking about, gaming is rather macro-insensitive for some reason. People enjoy gaming whether the economy is good or not, whether the oil price is high or not, people seem to enjoy gaming. Don't forget gaming is not something that people do once a month, like going out to a movie theater or something like that. People game every day, and the gamers that use our products are gaming every day. It's their way of engaging with their friends, they hang out with their friends that way. It's a platform for chatting. Don't forget that the number one messaging company in China, is actually a gaming company. And the reason for that is, because while people are gaming, they're hanging out with their friends and they're chatting with their friends. And so, it is really a medium for all kinds of things, whether it's entertaining, or hanging out, or expressing your artistic capabilities or whatnot. And so, gaming for one, appears to be doing quite well, in all aspects of the market. The second thing is, enterprise, however, is largely or hyperscale is largely a US dynamic. And the reason for that is, because a US dynamic as well as a China dynamic, because that's where most of the world's hyperscale companies happen to be and so. And then, automotive, most of our automotive success to date has been from the European car companies, and we're seeing robust demand from the premium segments of the marketplace. However, in the future we're going to see a lot more success with automotive here in the United States, here in Silicon Valley, in China. We're going to see a lot more global penetration because of our self-driving car platform. Well, all of the Pascal chips have been taped out.  And so -- but we still have a lot of engineering work to do. The difference -- differences are minor. We're a large company, and we have a lot of things that we're doing. I wouldn't over-study the small deltas in  OpEx. We don't manage things a dollar at a time, and we're trying to invest in the important things. On the other hand, this Company is really good about not wasting money. And so, we want to make sure that, on the one hand, we invest into opportunities that are very important to our company, but we just have a culture of frugality that permeates our company. And then lastly, from an operational perspective, we unified everything in our company behind one architecture. And whether you're talking about the cloud or workstations, or datacenters, or PCs, or cars, or embedded systems, or autonomous machines, you name it, everything is exactly one architecture. And the benefit of one architecture is that we can leverage one common stack of software, and that -- a base software, it really streamlines our execution. And so, it's an incredibly efficient approach for leveraging our one architecture into multiple markets. And so, those three aspects of how we run the company really helps. I think this question I should -- I think OEM business, will that be up year over year? I think the OEM business is down year over year, isn't it? Right. And so, on Q2 we'll probably follow along in Q2, along with the overall PC demand, which is not expected to grow. So we'll look at that as our side product, and probably would not be a growth business in Q2. Yes. So Blayne, you know that our OEM business is a declining part of our company's overall business.  And not to mention that the margins are also significantly low on the corporate average. And so, I would suggest that it's just increasingly a less important part of the way that we go to market. Now what I don't mean by that, is that we don't partner with the world's large OEMs, HP, Dell, IBM, Cisco, Lenovo, all of the world large enterprise companies are our partners. We partner with them to take our platforms, our differentiated platforms, or specialty platforms to the world's market, and most of them are related to enterprise. We just do less and less volume, high-volume components devices. Generic devices like cell phones that we got out of, generic PCs that we've gotten out of. Largely we tend not to do business like that anymore. We tend to focus on our differentiated platforms. Now you mentioned some, you mentioned learning and training and inferencing. First of all, training is production. You can't train a network just once, you have to train your network all the time. And every single hyperscale company in the world is in the process of scaling out their training. Because the networks are getting bigger, they want their networks to do even better. The difference between a 95% accurate network, and a 98% accurate network, or a 99% accurate network could mean billions of dollars of differences to internet companies. And so, this is a very big deal. And so, they want their networks to be larger, they want to deploy their networks across more applications, and they want to train their network with new data all the time. And so, training is a production matter. It is probably the largest HPC, high-performance computing application on the planet that we know of at the moment. And so, we're scaling, we're ramping up training for production for hyperscale companies. On the other hand, I really appreciate you asking about the inferencing We, recently -- well, this year, several months ago we announced the Tesla M4 that was designed for inferencing. And it's a little tiny graphics card, a little tiny processor, and it's less than 50 watts. It's called the M4. And at GTC, I announced a brand-new compiler called the GPU inference engine, GIE. And GIE recompiles the network that was trained, so that it can be optimumly inferenced at the lowest possible energy. And so, not only are we already 50 watts, which is low power, we can also now inference at a higher energy efficiency, than any processor that we know today, better than any CPU by a very long shot, better than any FPGA. And so, now hyperscale companies could use our GPUs for both training, and use exactly the same architecture for inferencing, and the energy efficiency is really fantastic. Now the benefit of using GPU for inferencing, is that you're not just trying to inference only. You're trying to, often times, decode the image, or you could be decoding the video, you inference on it. And you might even want to use it for transcoding, which is to re-encode that video, and stream it to whoever it is, that is -- that wants to share live video with. And so the [proxy] that you want to do on the images and the video and the data, is more than just inferencing, and the benefit of our GPU is that it's really great for all the other stuff too. And so, we're seeing a lot of success in M4. I expect M4 to be quite a successful product, and hyperscale datacenters, my expectation will start to ramp that into production Q2, Q3, Q4 time frame. Sure, thanks for the question. So in our automotive business, there's definitely a process even before we're shipping platforms into the overall cars, that we're working jointly with the auto manufacturers, start ups and others on what may be a future product. Many of those agreements continue, and will likely continue going forward, and that is what you see incorporated in our automotive business. So, yes, you probably will see this continue, and go forward. It is not necessarily consistent, it's starts in some quarters, are bigger in other quarters. But that's what is incorporated in our automotive. And Colette, let me just add one thing. The thing to remember is that we're not selling chips into a car. We're not selling -- you know that DRIVE PX is the world's first autonomous driving car computer that's powered by AI, it's powered by deep learning, and we're seeing a lot of success with DRIVE PX. And as Colette mentioned earlier, there's some 80 companies that we're working with, whether it's tier 1s, or OEMs, or start-up companies all over the world that we're working with, in this area of autonomous vehicles. And the thing to realize is, you're not selling a chip into that car. You're working with a car company to build an autonomous driving car. And so that process requires a fair amount of engineering. And so, we have a mechanism, we have a development mechanism that allows car companies to work with our engineers to collaborate, to develop these self-driving cars. And that's most of that stuff that Colette was talking about. Well first, let's -- thanks for the question, and we don't comment on unannounced products as you know. I hate to ruin all of the surprises for you, but Pascal is the single most ambitious GPU architecture we have ever undertaken. And this is really the first GPU that was designed from the ground up, for applications that are quite well beyond computer graphics and high-performance computing. It was designed to take into consideration, all of the things that we've learned about deep learning, all of the things that we've learned about VR. For example, it has a brand-new graphics pipeline that allows Pascal to simultaneously project into multiple surfaces at the same time, with no performance penalty, that otherwise, it would degrade your performance in VR by factor by a factor of 2,  just because you have two surfaces you're projecting into. And then, we can do all kinds of amazing things for augmented reality, other types of virtual reality displays, surround displays, curved displays, domed displays, I mean there's all kind of -- holographic displays, all kind of display that are being invented at the moment. And we have the ability to now support those type of displays, with a much more elegant architecture, without degrading performance. And so, Pascal is, whether it's AI, whether it's gaming, whether it's VR, is really the most ambitious project we have ever undertaken, and it's going to go through all of our markets. The application for self-driving cars is going to be pretty exciting. And so, it's going to go through all of our market. And so, we are in -- of course, we have plenty to announce in the future, but we've announced what we announced. First of all, to -- working on full autonomy is a great endeavor. And whether we get there 100%, 90%, 92%, 93%, is in my mind completely irrelevant. The endeavor of getting there, and making your car more and more autonomous -- initially, of course, we would like to have a virtual copilot. Having a virtual copilot is the way I get to work every day. I mean, every single day I drive my Model S. And every single day I put it into autonomous mode, and every single day it brings me joy. And I'm not confessing necessarily, texting a little bit is okay. And so, I think that the path to full autonomy is going to be paved by amazing capabilities along the way. And so, we're not waiting around for 2019, we'll ship autonomous vehicles by the end of this year. And so I understand that we're three years ahead of other people's schedules. However, we also know that DRIVE PX 2 is the most advanced autonomous computing car computer in the world today. And it's powered by AI fully. And DRIVE PX 2 will be a DRIVE PX 3, there will be a DRIVE PX 4. And then by 2019, I guess, we'll be shipping DRIVE PX 5. And so, those -- our road map just like that. That's how we work, as you guys know very well. And so, I think there's a point, there 's a lot of work to be done, which is the exciting part. The thing about a technology company, a thing about any company, unless there's great problems and great challenges that we can help solve, what value do we bring? And what NVIDIA does for a living, is to do what--  to build computers that no other company in the world can build, whether it's high-performance computers that are used to power a nation's supercomputers, or a deep learning supercomputer so that we can gain insight from data, or self-driving car computers, so that autonomous cars can save people's lives, and make people's lives more convenient. That's what we do. This is the work that we do. And I am delighted here, that we're three years ahead of the competition. Yes, the truth is that nobody really knows how big, this deep learning market is going to be. Until a couple of two or three years ago, it was really even hard to imagine how good the results were going to be. And if it wasn't because the groundbreaking work that was done at Google and Facebook, and in other researchers around the world, how would we have discovered, that it was going to be superhuman? The work that recently was done by Microsoft Research. They've achieved superhuman levels of inferencing that -- of image recognition and voice recognition that's really kind of hard to imagine. And these networks are now huge The Microsoft Research network, super deep network is 1,000 layers deep. And so, training such a network is quite a chore, it's quite endeavor, and this is a problem that high-performance computing will have to deployed, and this is why our GPUs so sought after. In terms of how big that's going to be, my sense is that almost no transaction -- my sense is that almost no transaction with the internet will be without deep learning, or some machine learning inference in the future. I just can't imagine that. There's no recommendation of a movie, no recommendation of a purchase, no search, no image search, no text that won't somehow had passed through some smart chatbot, or smartbot, or some machine learning algorithms, so that they could make the transaction more -- make the inference, or more request, more useful to you. And so, I think this is going to be a very big thing. And then, on the other hand, the enterprises, we use deep learning all over our Company today. And we're not -- we had the benefit of being early, because we saw the power of this technology early on. But we are seeing deep learning being used now in medical imaging all over the world. We're seeing it being used in manufacturing, it's going to be used for scientific computing. More data is generated by high-performance computers and supercomputers than just about anything. They generate it through simulation. They generate so much data, that they to throw the vast majority of it away. For example, Hadron Collider, whenever the protons collide, they throw away 99% of the data, and they're barely able to keep up with just that 1%. And so, by using machine learning, and our GPUs, they could find insight in the rest of the 99%. And so, there's just -- applications go on and on and on. And people are now starting to understand this deep learning. It really puts machine learning and puts artificial intelligence in the hands of engineers, is understandable. And that's one of the reasons why it's growing so fast. And so, I don't know exactly how big it's going to be. But here's my proposition, and this is going to be the next big computing model, the way that people compute. That in the past, software programmers wrote programs, compiled it. And in the future, we're going to have algorithms write the software for us. And so, that's a [very good way of computing] and I think it's a very good deal. The primary parts of our automotive business today comes from infotainment, in the premier infotainment systems. For example, the virtual cockpit that Audi ships. And the vast majority of our development projects today come from DRIVE [PX] on those projects. We probably have 10 times as many autonomous driving projects, as we have infotainment projects today. And we have a fair number of infotainment projects. And so, that gives you a sense of where we were in the past, and where we're going in the future.","Thank you for taking my question, and good job on the results and the guidance. Maybe as my first one, Jen-Hsun, how do you assess the competitive landscape in PC gaming? AMD recently claimed to be taking a lot of share, and they are launching a Polaris soon. If you could just walk us through, what does NVIDIA to better than AMD? So that helps you maintain your competitive edge in this market, and what impact does Pascal have in that? Got it. Thank you, Jen-Hsun. And as my follow up, so things like data center products were the big upside surprise in Q1, grew over 60% from last year. Could you give us some more  color on what drove that upside? Was it the initial Pascal launch? Is that impact still to come?  And just broadly, what trends are you seeing there in HPC, versus some of these new AI projects that you're involved with? Thanks for taking my questions. A first question, the growth in the Tesla business is impressive, and in looking back, it seemed like that business actually decelerated in 2015, which was a head-scratcher for me. And I wonder do you think that your customers in that business paused in anticipation of Pascal? Or do you think it is the AI apps and deep learning applications that are just hitting their stride right now? Hey, thanks for taking my questions. Jen-Hsun or Colette, first of all I want to see if you can help provide some color on some of the drivers of growth for fiscal 2Q, whether most of it is coming from Pascal possibly in the gaming market, or in the Tesla products, or if there was also some outgrowth and Tegra automotive as called for fiscal 2Q? Great. As my follow-up, maybe for Colette. On the gross margin side of things, you guys are guiding margins up nicely for the quarter. And just kind wondering looking further across the year, whether you have --  whether or not that the levers that you have available to you currently, if there's further room for expansion, whether it is from product mix, higher ASPs, and or maybe some of the platform-related elements  such as software services? And I was kind wondering, especially on the software side, how much that can continue to help the margins from a platform perspective? Yes, thanks, guys, and congratulations on the great quarter. For Q2, can you kind of talk about how much of a contribution you expect from Pascal? And also maybe give us an update on where you think that yields are progressing right now? This is Gabriel calling in for Ambrish. Thanks for taking my question. I think when you recently launched a new GPU products, looks like your pricing, your MSRP appears to be higher than your prior generation. And how should we think about your ASP, and even gross margin trend as you are ramping this product for the rest of the year? Yes, good afternoon. Thank you for taking my question. I guess, two questions around the data center. I guess, first part, how is the visibility here today? And I guess how do you see perhaps the transition from hyperscale to ramp in HPC? And then, I know you guys are not like to forecast over the next couple of quarters. But looking out over the next 12 to 24 months, this part of your business has grown from 8% to 11% year over year. And curious as you look at one to two years, what do you think this could be as a percentage of your overall company? Thank you. Hi, thanks for cycling me back in for a follow up. Sometimes  in -- when you introduce a new product, and this is broadly for technology, there's kind of a hiccup as the transition happens, where the supply chain blows out the older inventory, and before the new products can ramp in, so people call that an air pocket. So I was wondering is that something that you can manage, how do you try to manage that? Did you account for it, when you think about the outlook for this quarter? Thank you. Great, thank you. I guess, along the same lines, can you talk a little bit about the founders edition of the new gaming products? And how does that differ from sort of previous reference designs that you've done, and is there any kind of difference in economics to NVIDIA if you sell founders edition? Good afternoon, and solid job on the execution. At the recent Analyst Day, I think articulated its exposure to developed and emerging markets and the unit and ASP growth opportunities around EM.  Just wondering, what are the current demand dynamics that you're seeing in the emerging markets? Clearly, I think macro-wise, they're still pretty weak, but on the flip side, gaming has shown to be fairly macro-insensitive. Would be great to get your views here. Yes, thank you. So for July, it looks like you've got some operating expense discipline. Given some hiring activity in April, you're down sequentially. Is that related to the timing of some tape out activity? And as Pascal rolls out, what should the shape of tape outs be, do you think of the upcoming quarters? Hey, guys, thanks for taking my question,  and nice results. I was just curious, two questions. Jen-Hsun, you talked about the ramp of deep learning, and you talked about that you are going to use GPUs for both learning, as well as applying the inferences. Just curious, what stages, you mentioned all these consumers, what stages are all these customers? Are they actually deploying it in volume, or are they still more of sales for learning? And then, you said all segments up. Just curious, OEM is finally hitting some easy compares. Is that also going to be up year over year? Hi, thanks for letting me ask a question. On the automotive side, I just wondered, Colette, in your CFO commentary, you mentioned by product development contracts as part of the reason it was increasing. Can you give us a little bit of an indication what those are? And is the percentage of revenue coming from those increasing? And then maybe finally, is that activity indicative of future growth in any way, that can be meaningful for us to track? Thanks for taking the question, and congratulations on the revenue and the margin performance. Jen-Hsun, I wanted to follow up on one of the comments that you made regarding Pascal. I think you indicated that all Pascal parts had taped out. So the question is, if that is the case, will we see refresh activity across all of the platform groups in FY17, or in fact will some of the refresh activity taking place FY18? So what's the duration of the refresh that we are looking at? Yes, thanks very much. Jen-Hsun, I was hoping you could just share your view today on fully autonomous driving, because your mobilized chairman has said very recently, that the technology basically isn't ready. And that fully autonomous cars won't be available until -- I think he was saying 2019. And I guess, my question is well, one, I'd love your view on that? And two, whether the cars are fully autonomous, or autonomous in certain environments say, one or two years out, does it impact the trajectory of your automotive business? Hi, Jen-Hsun, hi, Colette. Congratulations on the impressive results here. On the data center business, is there an inflection going on, with deep learning, with the software maturity that's driving some at this point? And can you give us any metrics. Jen-Hsun, for how to think about the size of this opportunity before you? And I know it's hard, but things like server attach rates, what percent of servers you could attach? Will it be an M4 in an high end in every box? Or was it -- or maybe the number of GPUs a single deep learning implementation has?  Something like that.  That would help. Thanks. Thanks very much. In automotive, what product are your revenues coming from currently? Is DRIVE PX at all significant, or are your sales primarily DRIVE CX, or something else? We had a great start to the year, with strong revenue growth and profitability. Pascal is a quantum leap in performance for AI, gaming, and VR and is in full production. Deep learning is spreading across every industry, making datacenter our fastest growing business. With growing worldwide adoption of AI, the arrival of VR, and the rise of self-driving cars, we're really excited about the future. Thanks for tuning in. PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q3 2017,1701,5667,1294,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of FY2017. With me on the call today from NVIDIA our Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until the 17th of November 2016. The webcast will be available for replay up until next quarter's conference call to discuss Q4 financial results. The content of today's call is NVIDIA's property. It cannot be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These forward-looking statements are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All of our statements are made as of today, the 10th of November 2016, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find your reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary which is posted on our website. With that, let me turn the call over to Colette. Thanks, Arnab. Revenue reached a record in the third quarter exceeding $2 billion for the first time. Driving this was success in our Pascal-based gaming platform and growth in our data center platform reflecting the role of NVIDIA's GPU as the engine of AI  computing. Q3 revenue increased 54% from a year earlier to $2 billion and was up 40% from the previous quarter. Strong year-on-year gains were achieved across all four of our platforms. Gaming, professional visualization, data center, and automotive. The GPU business was up 53% to $1.7 billion, and the Tegra processor business increased 87% to $241 million. Let's start with our gaming platform. Gaming revenue crossed the $1 billion mark and increased 63% year on year to a record $1.24 billion fueled by our Pascal-based GPUs. Demand was strong in every geographic region across desktop and notebook and across the full gaming audience from GTX 1050 to the Titan X. GeForce gaming PC notebooks recorded significant gains. Our continued growth in the GTX gaming GPUs reflects the unprecedented performance and efficiency gains in the Pascal architecture. It delivers seamless play on games and richly immersive VR experiences. In Q3 for desktops, we launched the GTX 1050 and the 1050 Ti bringing eSports and VR capabilities at great value. For notebooks, we introduced GTX 1080, 1070, and 1060 giving gamers a major leap forward in performance and efficiency in a mobile experience. The fundamentals of the gaming market remain strong. The production value of blockbuster games continues to increase. Gamers are upgrading to higher-end GPUs to enjoy highly anticipated fall titles like Battlefield 1, Gears of War 3, Call of Duty: Infinite Warfare, and eSports is attracting a new generation of gamers to the PC. League of Legends is played by over 100 million gamers each month. And, there is now a Twitch audience of more than 300 million who follow eSports. VR and AR will redefine entertainment and gaming. A great experience requires a high performance GPU, and we believe we are still in the early innings of these evolving markets. Pascal represents not only the biggest innovation gains we've made in a single GPU generation in a decade, it's also our best executed product rollout. Moving to professional visualization, Quadro revenue grew 9% from a year ago to $207 million driven by growth in the high end of the markets for Realtime rendering and mobile workstations. We are seeing strong customer interest in the Pascal-based P6000 among digital entertainment leaders like: Pixar, Disney, and ILM, architectural, engineering, and construction companies like Japan's SHIMIZU, and automotive companies like Hyundai. Next, data center. Revenue nearly tripled from a year ago and was up 59% sequentially to $240 million. Growth was strong across all fronts in AI and supercomputing for hyperscale as well as for GRID virtualization and supercomputing. GPU deep learning is revolutionizing AI and is poised to impact every industry worldwide. Hyperscale companies like Facebook, Microsoft, and Baidu are using it to solve problems for their billions of consumers. Cloud GPU computing has shown explosive growth. Amazon Web Services, Microsoft Azure, and Ali cloud are deploying NVIDIA GPUs for AI data analytics and HPC.  AWS most recently announced its new EC2 P2 Instance which scales up to 16 GPUs to accelerate a wide range of AI applications including image and video recognition, unstructured data analytics, and video transcoding.  We saw strong growth in AI training.  For AI inference, we announced the Tesla P4 and P40 to serve power-efficient and high performance workloads, respectively. Shipments began in Q3 for the DGX-1 AI super computer. Early users include major universities like Stanford, UC Berkeley, and NYU, leading research groups such as OpenAI, the German Institute of Artificial Intelligence, and the Swiss Artificial Intelligence Lab as well as multinationals like  SAP. So far this year, our GPU technology conference program has reached 18,000 developers and ecosystem partners underscoring the broad enthusiasm for AI. Complementing our major spring event in Silicon Valley, we have organized GPCs in seven cities on four continents. They drew sellout audiences in Beijing, Taipei, Tokyo, and Seoul, as well as Amsterdam, Melbourne, and Washington DC, with Mumbai still to come. Along with 400 sessions and labs, we provided training in AI skills to nearly 2,000 individuals through our Deep Learning Institute construction program. We also have begun partnering with key global companies to enable the adoption of AI. To implement AI in manufacturing, we announced a collaborative with Japan's FANUC focused on robots and automated factories, and in the transportation sector, more than 80 OEMs, Tier 1s, and startups are using our GPUs for their work on self-driving cars. Our GRID graphics virtualization business continues to achieve extremely strong growth. Adoption is accelerating across a variety of industries particularly manufacturing, automotive, engineering, and education. Among customers added this quarter were John Hopkins University and GE global India. And, finally, in automotive, revenue increased to a record $127 million, up 61% year over year and up 7% sequentially from premium infotainment products. NVIDIA is developing an end-to-end AI computing platform for autonomous driving. This allows car makers to collect and label data, train their own deep neural networks on the video GPUs in the data center, and then process them in the car with DRIVE PX 2. We have also been developing a cloud-to-car HD mapping system with mapping companies all over the world. Two such partnerships were announced this quarter. We are working with Baidu to create a cloud-to-car development platform with HD maps, Level 3 autonomous vehicles, and automated parking. We are also partnering with TomTom to develop an AI-based, cloud-to-car mapping system that enables real-time, in-car localization to mapping. We've developed an integrated, scalable AI platform with capabilities ranging from automated highway driving to fully autonomous driving operation. We are extending the DRIVE PX2 architecture to scale in performance and power consumption. It will range from DRIVE PX2 auto cruise with a single SSE for self-driving on highways up to multiple DRIVE PX2 computers capable of enabling fully autonomous driving. We also announced a single-chip AI supercomputer called Xavier with over 7 billion transistors. Xavier incorporates our next GPU architecture, a custom CPU design, and a new computer vision accelerator. Xavier will deliver performance equivalent to today's full DRIVE PX2 board, and its two Parker SoCs and two Pascal GPUs while only consuming a fraction of the energy. Finally, Tesla motors announced last month that all its factory-produced vehicles: the Model S, the Model X, and upcoming Model 3 feature a new autopilot system powered by the NVIDIA DRIVE PX2 platform and will be capable of fully autonomous operation via future software updates. This system delivers over 40 times the processing power of the previous technology and runs a new, neural network for vision, sonar, and data processing. Beyond our four platforms, our OEM and IP business was $186 million, down 4% year on year. Now, turning to the rest of the income statement.  GAAP gross margin for Q3 was a record 59%, and non-GAAP gross margin was a record 59.2%. These reflect the strength of our GeForce gaming GPUs, the success of our platform approach, and strong demand for Deep Learning. GAAP operating expenses were $544 million including $66 million in stock-based compensation and other charges. Non-GAAP operating expenses were $478 million, up 11% from one year earlier. This reflects headcount-related costs for our growth initiatives as well as investments in sales and marketing. We intend to continue to invest in Deep Learning to capture this once-in-a-lifetime opportunity. Thus, we would expect the operating expense growth rate to be sustained over the next several quarters. GAAP operating income was $639 million. Non-GAAP operating income more than doubled to $708 million. Non-GAAP operating margins were over 35% this quarter. For FY18, we intend to return $1.25 billion to shareholders through ongoing quarterly cash dividends and share repurchases. We also announced a 22% increase in our quarterly cash dividend to $0.14 per share. Now turning to the outlook for the fourth-quarter of FY17, we expect revenue to be $2.1 billion, plus or minus 2%. Our GAAP and non-GAAP gross margin are expected to be 59% and 59.2%, respectively, plus or minus 50 basis points. GAAP operating expenses are expected to be $572 million. Non-GAAP operating expenses are expected to be approximately $500 million. And, GAAP and non-GAAP tax rates for the fourth quarter of FY17 are both expected to be 20%, plus or minus 1%. With that, Operator, I'm going to turn it back to you and see if we can take some questions.","A couple things. First of all, GPU computing is more important than ever. There's so many different types of applications that require GPU computing today, and it's permeating all over enterprise. There are several applications that we're really driving. One of them is graphics virtualization, application virtualization.  Partnering with VMware and Citrix, we have essentially taken very compute-intensive, very graphics-intensive applications, virtualizing it and putting it into the data center. The second is computational sciences. Using our GPU for general-purpose scientific computing, and scientific computing, as you know, is not just for scientists. It's running equations and using numerics is a tool that is important to a large number of industries. And then, third, one of the most exciting things that we're doing because of deep learning we've really ignited a wave of AI innovation all over the world. These several applications -- graphics application, virtualization, computational science, and data science has really driven our opportunity in the data center. The thing that made it possible though -- the thing that really made it possible was really the transformation from our Company from a graphics processor to a general-purpose processor, and then on top of that -- probably the more important part of that is transforming from a chip Company to a platform Company. What makes application and graphics virtualization possible is a complicated stack of software we call GRID, and you have heard me talk about it for several years now.  And, second, in the area of numerics and computational sciences, CUDA, our rich library of applications and libraries on top of numerics -- numerical libraries on top of CUDA and all the tools that  we have invested in the ecosystem we have worked with all the developers all around the world that now know how to use CUDA to develop applications makes that part of our business possible. And then, third, our deep learning toolkit, the NVIDIA deep learning toolkit has made it possible for all frameworks in the world to get GPU acceleration. And, with GPU acceleration the benefit is incredible. It's not 20% it's not 50%. It's 20 times, 50 times. That translates to most importantly for researchers the ability to gain access to insight much, much faster. Instead of months, it could be days. It's essentially like having a time machine. And, secondarily, for IT managers it translates to lower energy consumption, and most importantly, it translates to a substantial reduction in data center cost. Whereas you have a rack of servers with GPUs, it replaces an entire basketball court of cluster of off-the-shelf servers.  And so, a pretty big deal. A great value proposition. I think embedded in your question, in fact, are many of the variables that influence our business. Especially in the beginning, several years ago when we started working on GPU computing and bringing this capability into data centers.  We relied on supercomputing centers in the beginning, and then, we relied on remote workstations, data center workstations, if you will, virtualized workstations.  And, then, increasingly we started relying on -- we started seeing demand from hyperscale data centers as they used our GPUs for deep learning and to develop their networks. And now, we're starting to see data centers take advantage of our new GPUs, P40 and P4 to apply to operate, to use the networks for inferencing in a large-scale way. So, I think we are moving, if you will, our data center business in multiple trajectories. The first trajectory is the number of applications we can run. Our GPU now has the ability with one architecture to run all of those applications that I mentioned from graphics virtualization to scientific computing to AI. Second, we used to be in data centers, but now we're in data centers, supercomputing centers, as well as hyperscale data centers. And then, third, the number of applications -- industries that we effect is growing. It used to start with supercomputing.  Now, we have supercomputing.  We have automotive.  We have oil and gas.  We have energy discovery.  We have financial services industry.  We have, of course, one of the largest industries in the world, consumer Internet Cloud services.  And so, we're starting to see applications in all of those different dimensions. I think the combination of those three things: the number of applications, the number of platforms and locations by which we have success.  And then, of course, the number of industries that we affect.  The combination of that should give us more upward directory in a consistent way. But, I think really, the mega point though is really the size of the industries we are now able to engage. In no time in the history of our Company have we ever been able to engage industries of this magnitude. And so, that's the exciting part, I think, in the final analysis. Yes.  Thanks a lot, Toshi. First of all, the reason why I've been on the road for almost two months solid is because at the request and the demand, if you will, from developers all over the world for a better understanding of GPU computing and getting access to our platform and learning about all of the various applications that GPUs can now accelerate. The demand is just really great. We no longer could do GTC, which is our developer conference, essentially our developer conference.  We can no longer do GTC just here in Silicon Valley, and so we this year decided to take it on the road.  And, we went to China, went to Taiwan, went to Japan, went to Korea.  We had one in Australia, and also one in India and Washington DC and Amsterdam for Europe. So, we pretty much covered the world with our first global developer conference. I would say probably the two themes that came out of it, is that GPU acceleration, the GPU has really reached a tipping point. That it is so available everywhere.  It's available on PCs.  It's available from every computer company in the world.  It's in the cloud.  It's in the data center.  It's in laptops.  GPU is no longer a niche component. As they say, it's a large-scale, massively available general-purpose computing platform. So I think people realize now the benefits of GPU and that the incredible speedup or cost reduction -- basically, the opposite sides of a coin that you can get with GPUs. So, GPU computing. Number two, is AI. Just the incredible enthusiasm around AI, and the reason for that, of course, for everybody who knows already about AI what I'm going to say is pretty clear.  But, there's a large number of applications, problems, challenges where a numerical approach is not available. A laws-of-physics-based, equation-based approach is not available. These problems are very complex.  Oftentimes, the information is incomplete, and there's no laws of physics around it. For example, what's the laws of physics of what I look like? What's the laws of physics for recommending tonight's movie? So, there's no laws of physics involved. The question is how do you solve those kind of incomplete problems? There's no laws-of-physics equation that you can program into a car that causes the car to drive and drive properly. These are artificial intelligence problems. Search is an artificial intelligence problem.  Recommendations is an artificial intelligence problem.  So, now that GPU deep learning has ignited this capability, and it has made it possible for machines to learn from a large amount of data and to determine the features by itself -- to compute the features to recognize.  GPU deep learning has really ignited this wave of AI revolution. So, I would say the second thing that is just incredible enthusiasm around the world is learning how to use GPU deep learning.  How to use it to solve AI-type problems, and to do so in all of the industries that we know from healthcare to transportation to entertainment to enterprise to you name it. Atif, first of all, there were several places where you cut out, and this is one of those artificial intelligence problems. Because what I heard incomplete information, but I'm going to infer from some of the important words that I did hear, and I'm going to apply in this case human intelligence to see if I can predict what it is that you were trying to ask. The baseline, the basis of your question was that Maxwell -- in the past, Maxwell GPU during that generation, we saw an upgrade cycle about every two or three years. And, we had an install base of some 60 million, 80 million gamers during that time and several years have now gone by. The question is what would be the upgrade cycle for Pascal, and what would it look like? There are several things that have changed that I think it's important to know that could affect the Pascal upgrade. First of all, the increase in adoption, the number of units has grown, and the number of the ASP has grown.  And, I think the reason for that is several-fold. I think, one, the number of gamers in the world is growing. Everybody that is effectively born in the last 10, 15 years are likely to be a gamer and so long as they have access to electricity and the Internet, they are very likely a gamer. The quality of games has grown significantly. One of the factors of production value of games that has been possible is because the PC and the two game consoles, Xbox and PlayStation, and in the future, in the near future, the Nintendo Switch.  All of these architectures are common in the sense that they all use modern GPUs.  They all use programmable shading, and they all have basically similar features. They have very different design points. They have different capabilities, but they have very similar architectural features. As a result of that, game developers can target a much larger install base with one common code base. As a result, they can increase the production quality, the production value of the games. The second -- and one of the things that you might have noticed that recently, PlayStation and Xbox both announced 4K versions.  Basically, the Pro versions of their game console. That's really exciting for the gaming industry.  It's really exciting for us because what's going to happen is the production value of games will amp up, and as a result, it would increase the adoption of higher-end GPUs. I think that that's a very important positive.  That's probably the second one.  The first one being the number of gamers is growing.  Second is game production value continues to grow. And then the third is gaming is no longer just about gaming. Gaming is part sports, part gaming, and part social. There's a lot of people who play games just so they can hang out with their other friends who are playing games. It's a social phenomenon.  And then, of course, because games are -- the quality of games, the complexity of games, and some such as League of Legends, such as StarCraft; the real-time simulation, the real-time strategy component of it, the agility, the hand-eye coordination part of it, the incredible teamwork part of it is so great that it has become a sport. Because there are so many people in gaming, because it's a fun thing to do, and  it's hard to do so it's hard to master.  And the size of the industry is large.  It has become a real sporting event. And, one of the things that I'll predict is that one of these days, I believe that gaming would likely be the world's largest sport industry. And, the reason for that is because it's the largest industry. There are more people who play games and now enjoy games and watch other people play games than there are people who play football, for example. So, I think it stands to reason that eSports will be the largest sporting industry in the world, and that's just a matter of time before it happens. So, I think all of these factors have been driving both the increase in the size of the market for us as well as the ASP of the GPUs for us. Sure. I would say that we're probably in the first at-bat of the first inning of GRID. The reason for that is this. We prepared ourselves. We went to spring training camp.  We came up through the farm league or something like that.  I'm not really a baseball player, but I heard some people talk about it. So, I think we're probably at the first at-bat of the first inning. The reason why I'm excited about it is because I believe in the future applications are virtualized in the data center or in the cloud. On first principles, I believe that data applications will be virtualized, and that you will be able to enjoy these applications irrespective of whether you're using a PC, a chrome notebook, a Mac, or a Linux workstation. It simply won't matter. And yet, on the other hand, I believe that in the future applications will become increasingly GPU-accelerated. How do you put something in the cloud that have no GPUs, and how do you GPU-accelerate these applications that are increasingly GPU-accelerated? The answer is, of course, is putting GPUs in the cloud and putting GPUs in data center.  That's what GRID is all about. It's about virtualization.  It's about putting GPUs in large-scale data centers and be able to virtualize the applications so that we can enjoy it on any computer, on any device, and putting computing closer to the data. I think we're just in the beginning of that, and that could explain why GRID is finally after a long period of time of building the ecosystem, building the infrastructure, developing all the software, getting the quality of service to be really exquisite, working with the ecosystem partners, it has really taken off. And I could surely expect to see it continue to grow at the rate that we're seeing for some time. In terms of Pascal, we are still ramping. Production is fully ramped in the sense that all of our products are fully qualified. They are on the market. They have been certified and qualified with OEMs. However, demand is still fairly high so we're going to continue to work hard. Our manufacturing partner, TSMC, is doing a great job for us.  The yields are fantastic for 2016 FinFET, and they're just doing a fantastic job supporting us.  We're just going to keep running at it. Sure, I'll start backwards. I'll start backwards and answer the FPGA question first. FPGA is good at a lot of things, and anything that you could do on an FPGA if the market opportunity is large, you could always -- it's always better to develop an ASIC.   And, FPGA is what you use when the volume is not large. FPGA is what you use when you are not certain about the functionality you want to put into something. FPGA is largely useful when the volume is not large. Because you could build an ASIC -- you could build a full-custom chip that obviously could deliver more performance.  Not 20% more performance but 10 times better performance and better energy efficiency than you could using FPGAs. I think that's a well-known fact. Our strategy is very different than any of that. Our strategy is really about building a computing platform. Our GPU is not a specific function thing anymore. It's a general-purpose parallel processor. CUDA can do molecular dynamics.  It could do fluid dynamics.  It could do partial differential equations.  It could do linear algebra.  It could do artificial intelligence.  It could be used for seismic analysis.  It could be used for computer graphics, even computer graphics.  And so, our GPU is incredibly flexible, and it's really designed for, it's designed specifically for parallel throughput computing. And, by combining it with the CPU, we have created a computing platform that is both good at sequential information, sequential instruction processing as well as very high throughput data processing. And so, we have created a computing architecture that's good at both of those things. The reason why we believe that's important is because several things. We want to build a computing platform that is useful to a large industry. You could use it for AI.  You could use it for search.  You could use it for video transcoding.  You could use it for energy discovery.  You could use it for health.  You could use it for finance.  You could use it for robotics.  You could use it for all these different things. On the first principles, we're trying to build a computing platform. It's a computing architecture. And, not a dedicated application thingy. Most of the customers that we're calling on, most of the markets that we are addressing, and the areas that we have highlighted are all computer users. They need to use and deploy a computing platform. It has the benefit of being able to rapidly improve their AI networks. AI is still in the early days. It's the early days of early days, and GPU deep learning is going through innovations at a very fast clip. Our GPU allows people to learn to develop new networks and deploy new networks as quickly as possible. So, I think the way to think about it is think of our GPU as a computing platform. In terms of the market opportunity, the way I would look at it is this. The way I would look at is there are something along the lines of 5 million to 10 million hyperscale data center nodes. I think, as you have heard me say this before, I think that training is a new set of HPC clusters that have been added into these data centers.  And then, the next thing that's going to happen is you're going to see GPUs being added to a lot of these 5 million to 10 million nodes so that you could accelerate every single query that will likely come into the data center will be an AI query in the future. I think GPUs have an opportunity to see a fairly large hyperscale installed base. But, beyond that there is the enterprise market. Still although, a lot of computing is done in the cloud, a great deal of computing especially the type of computing that we're talking about here that requires a lot of data -- and we're a data throughput machine -- the type of computers that we're talking about tends to be one of being in enterprise.  And, I believe a lot of the enterprise market is going to go towards AI; and the type of things that we are looking for in the future is to simplify our business processors using AI, to find business intelligence or insight using AI, to optimize our supply chain using AI, to optimize our forecasting using AI, to optimize the way that we find and surprise and delight customers, digital customers or customers in digital using AI. So, all of these parts of the business operations of large companies, I think AI can really enhance. And then, the third -- so hyperscale, enterprise computing, and then the third is something very, very new.  It's called IoT. IoT -- we're going to have 1 trillion things connected to the Internet over time, and they are going to be measuring things from vibration, to sound, to images, to temperature, to air pressure, to -- you name it. These things are going to be all over the world, and we are going to measure and we are going to be constantly measuring and monitoring their activity. And, using the only thing that we can imagine that can help to add value to that and find insight from that is really AI using deep learning. We could have these new types of computers, and they will likely be on-premise or near the location of the cluster of things that you have. And, monitor all of these devices and keep -- prevent them from failing or adding intelligence to it so that they add more value to what it is that people have them do. So, I think the size of the marketplace that we are addressing is really larger than any time in our history.  And, probably the easiest way to think about it is we're now a computing platform Company. We are simply a computing platform Company, and our focus is GPU computing and one of the major applications is AI. I don't know that I have really granular breakdowns for you, Craig, partly because I'm just not sure.  But, I think the dynamics are that self-driving cars is probably the single-most disruptive event -- the most disruptive dynamic that's happening in the automotive industry. It's almost impossible for me to imagine that in five years time, a reasonably capable car will not have autonomous capability at some level. And, a very significant level at that. I think what Tesla has done by launching and having on the road in the very near future here full autonomous driving capability using AI, that has sent a shockwave through the automotive industry. It's basically five years ahead. Anybody who's talking about 2021, that's just a non-starter anymore. I think that, that's probably the most significant bit in the automotive industry. Anybody who was talking about autonomous capabilities and 2020 and 2021 is at the moment reevaluating in a very significant way. So, I think that, of course, will change how our business profile will ultimately look. It depends on those factors. Our autonomous vehicle strategy is relatively clear, but let me explain it anyway. Number one, we believe that autonomous vehicles is not a detection problem, it's an AI computing problem. That it's not just about detecting objects. It's about perception of the environment around you.  It's about reasoning about what to do -- what is happening and what to do -- and to take action based on that reasoning. And, to be continuously learning. So, I think that AI computing requires a fair amount of computation, and anybody who thought that it would take only one or two watt -- basically, the amount of energy -- one-third the energy of a cell phone.  I think it's unfortunate, and it is not going to happen any time soon. So, I think people now recognize that AI computing is a very software-rich problem, and it is a supremely exciting AI problem. And, that deep learning and GPUs could add a lot of value, and it is going to happen in 2017, it's not going to happen in 2021. I think number one.  Number two, our strategy is to apply, to deploy a one-architecture platform that is open that car companies could work on to leverage our software stack and create their network, their artificial intelligence network. And, that we would address everything from highway cruising, excellent highway cruising, all the way to full autonomous to trucks to shuttles. And, using one computing architecture, we could apply it for radar-based systems, radar plus cameras, radar plus cameras plus Lidars.  We could use it for all kinds of sensor fusion environments. So, I think our strategy is really resonating well with the industry as people now realize that we need the computation capability five years earlier. That's not a detection problem, but it's an AI computing problem and that software is really intensive. But, these three observations, I think, has put us in a really good position. Our architecture for Drive PX is scalable. You could start from one Parker SoC, and that allows you to have surround camera.  It allows you to use AI for highway cruising. And, if you would like to have even more cameras so that your functionality could be used more frequently in more conditions, you could always add more processors. So, we go from one to four processors.  And, if it's a fully autonomous, driverless car -- a driverless taxi, for example, you might need more than even four of our processors. You might need eight processors. You might need 12 processors. And, the reason for that is because you need to reduce the circumstance by which auto-pilot doesn't work, doesn't turn on, excuse me, doesn't engage. And, because you don't have a driver in the car at all. I think that depending on the application that you have, we will have a different configuration, and it's scalable. It ranges from a few hundred dollars to a few thousand dollars so I think it just depends on what configuration people are trying to deploy. Now for a few thousand dollars, the productivity of that vehicle is incredible as you can simply do the math. It's much more available. The cost of operations is reduced. And, a few thousand dollars is surely almost nothing in the context of that use case. Harlan, good question.  And, it's exactly the reason why having started almost five years ago in working with all of these large-scale data centers is what it takes. The reason for that is because several things have to happen. Applications have to be developed. They're hyperscale.  They are enterprise -- their data center-level software has to accommodate this new computing platform. The neural networks have to be developed and trained and ready for deployment. The GPUs have to be tested against every single data center and every single server configuration that they have, and it takes that type of time to deploy at the scales that we are talking about. So, I think that, that's number one. The good news is that between Kepler and Maxwell and Pascal, the architecture is identical. Even though the underlying architecture has been improved dramatically and the performance increases dramatically, the software layer is the same. So, the adoption rate of our future generations is going to be much, much faster, and you will see that. It takes that long to integrate our software and our architecture and our GPUs into all of the data centers around the world. It takes a lot of work. It takes a long time. I think there are three things that we offer today. The first thing is that it's not a detection problem, it's an AI computing problem. And, a computer has processors and the architecture is coherent and you can program it.  You could write software.  You can compile to it.  It's an AI computing problem, and our GPU computing architecture has the benefit of 10 years of refinement. In fact, this year is the 10-year anniversary of our first GPGPU, our first CUDA GPU called G8, and we been working on this for 10 years.  And so, the number one is autonomous driving, autonomous vehicles is an AI computing problem.  It's not a detection problem. Second, car companies realize that they need to deliver ultimately a service. That the service is a network of cars by which they continuously improve. It's like phones. It's like set-top boxes. You have to maintain and serve that customer because they are interested in the service of autonomous driving. It's not a functionality. Autonomous driving is always being improved with better maps and better driving behavior and better perception capability and better AI, so the software component of it and the ability for car companies to own their own software once they develop it on our platform is a real positive. Real positive to the point where it's enabling, or it's essential for the future of the driving fleets. And then, the third -- to be able to continue to do OTA on them.  And, third, is simply the performance and energy level. I don't believe it's actually possible at this moment in time to deliver an AI computing platform of the performance level that is required to do autonomous driving at an energy efficiency level that is possible in a car and to put all the functionality together in a reasonable way. I believe DRIVE PX2 is the only viable solution on the planet today. So, because Tesla had a great intention to deliver this level of capability to the world five years ahead of anybody else, we were a great partner for them. So, those are probably the three reasons. Yes, Matthew, I really appreciate that. We see a large number of AI startups around the world. There's a very large number here in the United States, of course. There's quite a significant number in China. There's a very large number in Europe. There's a large number in Canada. It's pretty much a global event. The number of software companies that have now jumped on to using GPU deep learning and taking advantage of the computing platform that we have taken almost seven years to build, and it's really quite amazing. We are tracking about 1,500. We have a program called Inception, and Inception is our startup support program, if you will. They can get access to our early technology.  They can get access to our expertise, our computing platform, and all that we've learned about deep learning we can share with many of these startups. They are trying to use deep learning in industries from cybersecurity to genomics to consumer applications, computational finance, to IoT, robotics, and self-driving cars. The number of startups out there is really quite amazing. So, our deep learning platform is a really unique advantage for them because it's available in a PC so you can -- almost anybody with even a couple hundred dollars of spending money can get a startup going with a [video] GPU that can do deep learning. It's available from system builders and server OEMs all over the world: HP, Dell, Cisco, IBM, system builders, small system builders, local system builders all over the world. And very importantly, it's available in cloud data centers all over the world so Amazon AWS, Microsoft's Azure cloud has a really fantastic implementation ready to scale out.  You have got the IBM cloud.  You have got Alibaba cloud.  So, if you have a few dollars an hour for computing, you pretty much can get a company started and use the NVIDIA platform in all of these different places. So, it's an incredibly productive platform because of its performance.  It works with every framework in the world.  It's available basically everywhere, and so as a result of that, we've given artificial intelligence startups anywhere on the planet the ability to jump on and create something. The availability, if you will, the democratization of deep learning -- NVIDIA's GPU deep learning is really quite enabling for startups. It's largely the same channels. Our channel has been pretty stable for some time. We have a large network.  I appreciate your question. It's one of our great strengths, if you will. We cultivated over two decades a network of partners who take the GeForce platform out to the world. You could access our GPUs. You can access GeForce and be part of the GeForce PC gaming platform from literally anywhere on the planet. So, that's a real advantage, and we're really proud of them. I guess you could also say that Nintendo contributed a fair amount to that growth, and over the next -- as you know, the Nintendo architecture and the Company tends to stick with an architecture for a very long time so we've worked with them now for almost two years.  Several hundred engineering years have gone into the development of this incredible game console.  I really believe when everybody sees it and enjoy it, they are going to be amazed by it. It's like nothing they've ever played with before, and of course, the brand -- their franchise and their game content is incredible. I think this is a relationship that will likely last two decades, and I'm super-excited about it. Thank you very much for joining us today. I would leave you with several thoughts that, first, we're seeing growth across all of our platforms from gaming to Pro graphics, to cars to data centers. The transformation of our Company from a chip Company to a computing platform Company is really gaining traction, and you can see that you can see the results of our work as a result of things like GameWorks and GFE and Driveworks.  All of the AI that goes on top of that.  Our graphics virtualization remoting platform called GRID to the NVIDIA GPU deep learning toolkit are just really examples of how we have transformed a Company from a chip to a computing platform Company. In no time in the history of our Company have we enjoyed and addressed as exciting large market as we have today.  Whether it's artificial intelligence, self-driving cars, the gaming market as it continues to grow and evolve, and virtual reality. And, of course, we all know now very well that GPU deep learning has ignited a wave of AI innovation all over the world, and our strategy and the thing that we've been working on for the last seven years is building an end-to-end AI computing platform. An end-to-end AI computing platform.  Starting from GPUs that we have optimized and evolved and enhanced for deep learning to system architectures to algorithms for deep learning, to tools necessary for developers to frameworks, and the work that we do with all of the framework developers and AI researchers around the world, to servers to the cloud to data centers to ecosystems and working with ISVs and startups and all the way to evangelizing and teaching people how to use deep learning to revolutionize the software that they build.  And, we call that the Deep Learning Institute, the NVIDIA DLI. These are some of the high-level points that I hope that you got, and I look forward to talking to you again next quarter.","Thanks for taking my questions and congratulations on a great quarter. I think to start out, Jen-Hsun, maybe if you could help us understand -- the data center business tripled year over year. What's going on in that business that's enabling that to happen?  if you could maybe talk about if it's on the technology side or the end market side?  And, maybe as part of that, you can help us deconstruct the revenues and what's really driving that growth?  And, I had a follow-up, too. Thanks. Thanks for taking my question and congratulations on the consistent growth and execution. Jen-Hsun, one more on the data center business.  It has obviously grown very strongly this year, but in the past, it has been lumpy.  For example, when I go back to your FY15, it grew 60% to 70% year on year. Last year, it grew about 7%.  This year, it is growing over 100%. How should we think about the diversity of customers and the diversity of applications to help us forecast how the business can grow over the next one or two years? Great.  Thanks for taking my question and congratulations on a very strong quarter. Jen-Hsun, you've been on the road quite a bit over the past few months, and I'm sure you've had the opportunity to connect with many of your important customers and partners. Can you maybe share with us what you learned from the multiple trips?  And, how your view on the Company's long-term growth trajectory changed, if at all? Hi. Thanks for taking my question and congratulations. You mentioned that Maxwell upgrade was about 30% of your (technical difficulty) exactly two years. Should we be thinking about a two-year time (inaudible)? Hi, thanks for taking my questions. Jen-Hsun, first question if I could on your comments regarding the GRID systems. You mentioned some accelerating demands in the manufacturing and automotive verticals?  Just wondering if you had any thoughts on what inning you are currently in, in terms of seeing a strong ramp-up towards a full run rate for those areas?  And, especially for the broader corporate enterprise and market vertical, also? And, as a quick follow-up on the gaming side, was wondering if you had any thoughts on whether or not there is still a big gap between the ramp-up of Pascal supply and the pent-up demand for those new products? Thank you. Thank you very much.  Great quarter by the way and still amazed how good this is. Can you talk a little bit about the size of the inference opportunity?  Obviously, you have done really well in training.  I assume penetrating inference is reasonably early on, but can you talk about how you see GPUs competitively versus FPGAs on that side of it, and how big you think that opportunity could become? Thank you. Thanks for taking the question and congratulations on the stellar execution. Jen-Hsun, I wanted to go back to the automotive business.  In the past, the Company has mentioned that the revenues consist of display and then on the auto-pilot side both consulting and product revenues.  But, I think much more intensively on the consulting side for now. But, as we look ahead to Xavier and the announcement that you had made intra-quarter that, that's coming late next year, how should we expect that the revenue mix would evolve?  Not just from consulting to product, but from Parker towards Xavier? Hi.  Thanks for taking my question.  Great quarter across the board.  I did want to return to the automotive segment because the data center segment has been talked about at length.  With the new Drive PX platform increasing potentially the ASPs, how do we think about the ASPs for automotive going forward?  And, if I recall, you had about $30 million in backlog in terms of cars?  I'm not sure if it's possible to get an update there as well? Good afternoon.  Congratulations on the solid execution and growth. Looking at some of your cloud customers' new services offerings, you mentioned AWS EC2 P2 platform.  You have Microsoft Azure's Cloud Services platforms.  It's interesting because they are ramping new instances primarily using your K80 accelerator platform which means that the Maxwell base and the recently introduced Pascal-based adoption curves are still way ahead of the team which obviously is a great setup as it relates to the continued strong growth going forward.  Can you just help us understand why the long design and cycle times for these accelerators?  And, when do you expect the adoption curve for the Maxwell-based accelerators to start to kick in with some of your Cloud customers? Yes, thank you.  Jen-Hsun, I just wanted to ask regarding the auto-pilot win.  We know that you displaced Mobileye,  and I was just curious if you could talk about why Tesla chose your GPU?  And, what you can give us in terms of the ramp and timing, and how would a ramp like this affect automotive gross margin? Thank you very much. Good afternoon. Jen-Hsun, I make an interesting observation about your commentary that your Company has gone from a graphic accelerator Company to a computing platform Company, and I think that's fantastic. One of the things that I wonder as maybe AI and deep learning acceleration standardize on your platform, what you are seeing and hearing in the Valley about startup activity?  And, folks that are trying to innovate around the platform that you are bringing up both complementary to what you are doing, and potentially really long-term competitive to what you are doing?  Would love to hear your perspectives on that. Thanks. Thanks very much. It was really impressive that 60% growth in your gaming revenues.  So, does this imply that there was a 60% jump in [cards] that are being  been sold by [online] retailers and retail stores?  Or, does the growth reflect new channels through which NVIDIA gaming products are getting to customers? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q2 2018,1717,5890,1184,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the second quarter of fiscal 2018. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until August 17, 2017. The webcast will be available for replay up until next quarter's conference call to discuss Q3 financial results. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, August 10, 2017, based on information available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Shawn. NVIDIA continues to fire on all cylinders. We achieved strong revenue in each of our businesses. Data center revenue grew more than 2.5x, reflecting momentum behind artificial intelligence as we expanded our product portfolio and began shipping our new Volta platform. Overall, quarterly revenue reached a record $2.23 billion, up 56% from a year earlier, up 15% sequentially and well above our outlook of $1.95 billion. From a reporting segment perspective, Q2 GPU revenue increased 59% to $1.9 billion from a year earlier. Tegra Processor revenue doubled to $333 million. OEM revenue reached $251 million, reflecting sales of our cryptocurrency-specific GPUs, partially offset by the lapse in our licensing agreement with Intel. Let's start with our Gaming platform. Gaming revenue was $1.19 billion, up 52% year-on-year and up 15% from Q1. This reflects the vibrant Gaming ecosystem underpinned by continued excitement over our recent launched GPUs and other technologies, great games and growing interest in eSports. Gamers continue to love our Pascal-based GPUs, with demand remaining strong for GeForce GTX 10 Series products. Our new Max-Q design approach enabling Gaming notebooks that are thinner, lighter and faster is finding a strong market. Max-Q is being utilized in more than 20 new notebook models from a wide range of OEMs. Quality games continue to drive GPU sales. At the E3 Gaming expo in L.A., we showed the eagerly anticipated Destiny 2 running on 4K on PCs, which drew rave reviews. Major fall titles in addition to Destiny 2 include the new Call of Duty, World War II, Star Wars: Battlefront 2 and Middle-earth: Shadow of War. We're also seeing the rise of independent titles that will become runaway hits, most notably, a new last-man-standing shooter called PlayerUnknown's Battlegrounds. Gaming also continues to be driven by the surging popularity of eSports. This week's biggest sporting event isn't playing out at Dodger Stadium or Wrigley Field; it's in Seattle at the International Dota 2 Championships. The $24 million in prize money is more than twice the purse of golf's richest event, the US Open. For the third straight year, GeForce GTX is Dota 2's official graphics platform. GPU sales were lifted by demand from increasingly mining activity, or Ethereum. We served a large portion of this specialized market with a dedicated board, as seen in our OEM sales, and some with GeForce GTX boards. Our strategy is to stay alert to this fast-changing market, knowing that GPUs are highly efficient at running the algorithms used to mine cryptocurrencies. Moving to professional visualization. Quadro revenue grew $235 million, up 10% from a year ago, up 15% sequentially and a demand for high-end, real-time rendering and for more powerful mobile workstations. Demand was especially strong in education, both among universities and large public school districts, as well as in the financial sector and defense industry. And last week, at SIGGRAPH's computer graphics show, we highlighted how AI will augment the process of content creation. This new Optix 5 SDK uses AI to accelerate ray tracing. When running on our DGX rendering appliance, it provides the rendering capability of 150 standard dual-CPU servers. We also introduced NVIDIA eGPU system solutions. This creates a new category for our platforms using an external chassis to expand access to TITAN Xp and Quadro GPUs for the 25 million content creators using standard notebook PCs. Next, Datacenter. Revenue of $416 million was up more than 2.5x from a year ago. This growth, shared across AI, deep learning, high-performance computing and GRID, is particularly notable given that we announced and shipped production units of our Volta-based V100 accelerator as we transition from Pascal-generation GPUs. Looking ahead, we see inferencing and video transcoding as emerging applications that are well suited for our GPUs. V100 was among the most important launches at this quarter's GPU Technology Conference. It provides 10x the deep learning power of its year-old predecessor, widely outpacing Moore's Law. Some of the early V100 production units were given out in recent weeks to leading AI researchers attending the [CBPR] and ICML conferences. We made available our TensorRT 3 inference optimizer and run time for deep learning applications. It delivers 100x faster inferencing on V100 than the best CPU implementation. And it supports the industry's 2 most common AI frameworks: Google TensorFlow and Facebook's Caffe. We also entered into a wide range of important partnerships based on AI. Among them, Baidu has aligned with us on Volta. It's bringing this new architecture to its cloud and optimizing the PaddlePaddle open-source deep learning framework for Volta. Volkswagen is collaborating with us to bring the power of AI across their organization, and we announced a new partner program with Taiwan's top ODMs including Foxconn, Inventec, Quanta, Wistron to provide them with the early access to the HGX reference architecture, GPU computing technology and design guidelines. Demand remains strong for our DGX AI supercomputer as organizations take on multiple systems to build out AI-enabled applications. Facebook disclosed a system incorporating 128 DGXs. We have shipped systems to more than 300 unique customers, with 1,000 plus in the pipeline. Our HTC business remained strong. The new Green500 list showed that the world's 13 most efficient supercomputers run on NVIDIA Tesla P100 accelerators. The top position is held by the Tokyo Institute of Technology's TSUBAME 3.0 system. It achieved an extraordinary 14.1 gigaflops per watt, 50% higher efficiency than the previous leader, helping to point the way to [exascale] supercomputing. Momentum continued in our GRID graphics virtualization business. Among key wins was Amazon Web Services whose G3 instances now run on NVIDIA Tesla GPUs. In automotive. Revenue grew to $142 million, up 19% year-over-year. We announced important new partnerships based on our DRIVE PX AI platform, which is being used by more than 225 car and truck makers, Tier 1 suppliers, HD-mapping companies, start-ups and research institutions. Additionally, production cars using our technology continue to make their way into the market. Audi announced last month that the 2019 A8 will be the first car to feature NVIDIA's powered Level 3 autonomous technology. This means that, under certain specific use cases, it will be able to be driven by its own software without monitoring from the driver. Our DRIVE PX platform is already on the road in Tesla Motors' full line of cars, including the new Model 3, which uses the second-generation Tesla autopilot. This quarter, we also announced that Toyota selected NVIDIA DRIVE PX for their next-generation autonomous cars. Volvo, long a byword for safety and value, and Autoliv selected DRIVE PX for self-driving cars targeted to hit the market by 2021. ZF and Hella have selected the NVIDIA DRIVE PX platform for their line of autonomous vehicle products. And Baidu announced that its Project Apollo open-source self-diving platform for the China market will use NVIDIA DRIVE PX. You'll be hearing more from us in the months ahead at regional GTCs that we will be conducting in Beijing, Munich, Tel Aviv, Taipei, Washington, D.C. and Tokyo. Now turning to the rest of the Q2 income statement. GAAP and non-GAAP gross margins for the second quarter were 58.4% and 58.6%, respectively, reflecting the decline in in-sell licensing revenue. Q2 GAAP operating expenses were $614 million; non-GAAP operating expenses were $533 million, up 19% from a year ago, reflecting hiring for our growth initiatives. GAAP operating income was $688 million; non-GAAP operating income was $773 million, doubling from a year-ago. GAAP net income was $583 million; non-GAAP net income, $638 million, also doubling from a year ago, reflecting revenue strength as well as gross margin and operating margin expansion. For fiscal 2018, we intend to return approximately $1.25 billion to shareholders through stock repurchases and quarterly cash dividends. In the first half of fiscal 2018, we have paid $758 million in share repurchases and $166 million in quarterly cash dividends. Now turning to the outlook for the third quarter of fiscal 2018. We expect revenue to be $2.35 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 58.6% and 58.8%, respectively, plus or minus 50 basis points. GAAP operating expenses are expected to be approximately $672 million, non-GAAP operating expenses are expected to be approximately $570 million. GAAP OI&E is expected to be an expense of approximately $2 million, inclusive of additional charges from the early conversions of convertible notes. Non-GAAP OI&E is expected to be nominal. GAAP and non-GAAP tax rates are supposed -- are expected to be 17%, plus or minus 1%, excluding discrete items. Further financial details are included in the CFO Commentary and other information available on our IR website. We will now open the call for questions. (Operator Instructions) Operator, will you please poll for the first question? Thank you.","Maybe a clarification or a question, and a question after that on the data center. On the clarification side, we have seen several quarters where your Datacenter business grew very strongly on a sequential basis. This time, the growth was somewhat more modest, and I was wondering if there is a little more color around that. And then the bigger question is, Jensen, it seems the data center market is sort of bifurcating between your GPU approach on one side and ASICs on the other side. What are you doing to make sure that the balance sort of stays in your favor as the market matures from here? Yes, so first of all, Q2 was a transition quarter for our data center, right? I thought we did great. I thought we did great. We almost tripled year-over-year. And we ramped Volta into volume production. And because Volta was so much better than our last-generation processor, Volta is 100x faster than Kepler. 100x faster than Kepler just 4 years ago, and Kepler was already 10x faster than CPUs. And so Volta was such a giant leap when we announced it in GTC right at the beginning of the quarter. I thought the team did fantastically, transitioning the customer base to Volta, and now Volta is in high-volume production. The application of data center -- you asked a larger question about data center. The data center, data center is a very large market, as you know, and the reason for that is because the vast majority of the world's future computing will be largely done in data centers. And there's a very well-accepted notion now that GPU acceleration for servers delivers extraordinary value proposition. If you have a data-intensive application and the vast majority of the future applications in data centers will be data-intensive, a GPU could reduce the number of servers you require or increase the number of the amount of throughput pretty substantially. Just adding one GPU to a server could reduce several hundred thousand dollars of reduction in the number of servers. And so the value proposition in the cost savings of using GPUs is quite extraordinary. There are several applications in data centers. First of all, there's training and there's high-performance computing; there's cloud virtual PC, as what Amazon AWS G3 announcement was about this last -- this quarter; and then there's also new applications such as inferencing as these models are now going into production and the new applications that are coming online, which is likely to overwhelm the Internet in the near future, which is live video. Consumers taking live video on their phones and sharing with their friends and there's -- there are going to be hundreds of millions of these happening all the time. And each one of these videos will have to be transcoded to a variety of formats to be shared with their friends and also has to be -- you have to perform AI on it instantaneously so that you could avoid inappropriate video from being streamed to large audiences. And so the number of applications where GPUs are valuable, from training to high-performance computing to virtual PCs to new applications like inferencing and transcoding and AI, are starting to emerge. The one area where you're talking about ASICs and TPUs, TPU is basically an ASIC. The way to think about that is this is after 4 generations of evolution of our GPU, NVIDIA GPU is basically a TPU that does a lot more. We could perform deep learning applications, whether it's in training or in inferencing now, starting with the Pascal P4 and the Volta generation. We can inference better than any known ASIC on the market that I've ever seen. And so the new generation of our GPU is essentially a TPU that does a lot more, and we can do all the things that I just mentioned and the vast number of applications that are emerging in the cloud. And so our belief is this. Our belief is that, number one, a GPU has to be versatile to handle the vast array of big data and data-intensive applications that are happening in the cloud, because the cloud is a computer, it's not an appliance. It's not a toaster, it's not a lightbulb, it's not a microphone. The cloud has a large number of applications that are data-intensive. And second, we have to be world-class at deep learning. And so our GPUs have to evolve into something that can be absolutely world-class TPU, but it has to do all of the things that a data center needs to do. Yes, thanks. Cryptocurrency and blockchain is here to stay. The market need for it is going to grow, and over time, it will become quite large. I -- it is very clear that new currencies will come to market. And it's very clear that the GPU is just fantastic at cryptography, and as these new algorithms are being developed, the GPU is really quite ideal for it. And so this is a market that is not likely to go away anytime soon, and the only thing that we can probably expect is that there will be more currencies to come. It will come in a whole lot of different nations. It will be -- it will emerge from time to time, and the GPU's really quite great for it. What we've done, our strategy is to stay very, very close to the market. We understand its dynamics really well. And we offer the coin miners a special coin-mining SKU, and this product is -- this product -- this GPU configuration is optimized for mining. We stay very close to the market. We know its every single move and we know its dynamics. And then the last thing that I can say is that, the larger of a GPU company you are, the greater ability you could absorb the volatility. And so between the combination of the fact that we have GPUs at just about every single price point, we have such incredibly efficient designs, that we're so close to the marketplace and because we have such large volumes, we have the ability to rock and roll with this market as it goes. Okay? But this is an important market that likely will continue to grow over time. Sure. Let's see. First of all, we actually gave a really great guidance last quarter, and we beat it by $250 million. And the $250 million, you could see in our -- what we categorized under the OEM SKUs, basically the cryptocurrency SKUs. And that, if you reverse-engineered it out, I think, is approximately $150 million. And I -- and we serve the vast -- I would say, the large majority of the cryptocurrency demand out of that specialized products. There're still small miners that buy GeForces here and there, and that probably also increased the demand of GeForces. There were a lot of shortages all over the world, and as we go into this quarter, it's -- there's still cryptocurrency mining demand that we know is out there. And based on our analytics and understanding of the marketplace, there will be some amount of demand for the foreseeable future. But I -- it's also the case that there were gamers who -- whose needs and demands were not filled last quarter. And the second quarter, the second quarter is an important part of the year for us. I mean, we -- GeForce is in an incredibly great strategic position. After all of the numerous product launches that we've seen from other players, it's very, very clear that the GeForce product lineup is absolutely the best in the world. And the second half is going to see some very exciting titles. You've got Destiny 2, you have Call of Duty from Activision, you have Star Wars Battlefront from EA. I mean, these are going to be blockbusters, and we're expecting them to do incredibly well. We also know that a game that came out of nowhere -- and this is one of the things that's really great about the video game market, you never know where the next amazing new title's going to come from. PlayersUnknown Battleground (sic) [PlayerUnknown's Battlegrounds], it's really essentially survival -- Survivor meets Hunger Games. How could that not be a fun game? And so they've done incredibly well. And so I think the market dynamic is really, really vibrant for the second half of the year, and we have a really great position. With respect to our guidance. The way to think about our guidance, we gave a good guidance and we're comfortable with our guidance. And we know that the dynamics in our business, our data center position, is quite exciting. We know that our Gaming business is vibrant and our position is excellent. We've -- we saw growth across all of our product segments. And we'll just see how it turns out at the end of the next quarter. Well, first of all, it's very difficult to reverse-engineer from the first ramp of Volta any impact on gross margins. And the reason for that is because the first ramps tend to be more costly, and you're still trying to stabilize yield, there's a lot of complexities involved. But what I can tell you is that we shipped a lot of Voltas. We shipped a lot of Voltas. And Volta is fully ramped. Customers are clamoring for it. The leap generationally for deep learning is quite extraordinary. And so we're expecting Volta to be very, very successful. Yes. So the first way to think about our ASP is to think about the value proposition that our GPUs provide. Whenever you include a Volta in your data center, in your server that is doing data-intensive processing, the number of commodity servers that it replaces and the number of just NICs and cables that it replaces is pretty extraordinary. Every single Volta allows you to save several hundred thousand dollars. And so the price of Volta is driven by the fact that, of course, the manufacturing process is quite extraordinary. I mean, these are expensive things to go and design. The manufacturing cost itself, you guys can estimate, is probably in the several hundred dollars, close to $1,000. However, the software intensity of developing Volta, the architectural intensity of developing Volta, the -- all of the software intensity associated with all the algorithms and optimizing all the algorithms of Volta is really where the value added ultimately ends up. And so I guess, the pricing -- your question really is pricing, we're going to -- we expect pricing to be quite favorable for Volta. And then your second question, I think, is related to acceleration. The growth -- the data center growth opportunity for us is quite significant, as you know. I mean, there are several applications that demand GPUs today. Almost every single data center in the world today recognizes that GPU is the path forward for data-intensive processing. Every single OEM and every single cloud service provider now supports NVIDIA GPUs and offer NVIDIA GPUs, and Volta is going to be the engine that serves them. And so I'm expecting a lot of good things from Volta. Sure. Thanks a lot, Atif. The road map for auto kind of looks like this. For this year and next, what you should see is development partnerships that we have with a growing number of car companies, and they're reflected in our e-projects, development systems and purchasing of our AI supercomputers like DGX. And so for the next, I would say, this year and the vast majority of next year, that's what you should expect from the autonomous driving perspective. Starting next year, you're going to start to see robot taxis start to come to the road. We're working with a handful, maybe, I guess, about 6 or 7 really exciting robot taxi projects around the world. And you could see them start to go into prototype or beta testing starting now. And then, next year, you'll see a lot more of them. And starting 2019, you'll see them going to real commercial services. And so those are robot taxis, what some in the industry call Level 5s, basically driverless cars. And then the fully autonomous driver cars -- driven cars, branded cars, will start hitting the road around 2020 and 2021, okay? And so the way to think about it is this year and next is really about development. Starting next year and the following year is robot taxis. And then 2021 to -- forward, you're going to see a lot of Level 4s. Great. I wanted to actually ask about the pro vis business. That business continues to grow faster than I had expected, and you had a really good for quarter there. Can you talk about what's driving that and what the trajectory of that business looks like? Sure. Our Pro business, call it, roughly nearly $1 billion, it was -- it grew, what, 8% last year. It grew 8% the year before that, maybe a little bit less. And this year, it grow -- it grew about 10%, maybe a little faster. The way to think about that business is it's really a platform for design, and the digital design of all kinds. And it's designing movies, designing cars, designing products, people designing websites. Anybody who's doing digital design could really benefit from our Quadro platform. It's very software-intensive, it's certified with every major computer-aided design package. It's certified by large industrial companies all over the world. You could use Quadro and bring up a database 10 years from now and know that, because of the nature of how we manage our software, the certification process we go through with each one of the major industry partners, we could pull up an entire design that's designed 5 years ago 10 years from now. And so if anything were to happen to a product or a plane or a ship or a building, the level of certainty in your data integrity is going to be complete. And so the software intensity is high, and the -- and our platform is recognized all over the world as the industry standard. The growth opportunity for Quadro are several, and it's starting to kick in. And I have -- I'm rather optimistic about its future growth as well. One of them is photorealistic rendering. We now have the ability to use our artificial intelligence and ray tracing technology in combination, called Optix 5.0 that we just announced at SIGGRAPH, that allows you to visualize photorealistic rendering practically interactively. And it's just an amazing thing to watch. Second, we now have a new system, called the external GPU system, that's a partnership between the work that we did with Intel. And all of our partners in the ecosystem taking advantage of Thunderbolt 3 and the new external GPU-capable Windows system, you can now have an external system connect to Thunderbolt. And basically, our GPU is outside the laptop. And so for some 25 million, 20 million users of thin and light notebooks, you can now have the ability to have the GPU as well and get a boost in your productivity like you've never seen before. And so you can now have thin notebooks and still have the benefit of our GPUs. And so that's a new market for us. We're going to see virtual reality do quite well, and especially in design. And we've partnered with HP recently to do an industrial version of a backpack that allows designers to be able to freely roam within their design space and completely in virtual reality. And so there's a variety of growth drivers in that business that I'm quite excited about. Congratulations on the very good execution and capitalizing on the crypto opportunity. Jensen, I wanted to start, just connecting a few dots, you had mentioned that there was significant upside from that opportunity, and we've seen through checks that we can do via public means that demand was very strong in the quarter. And as I look ahead at the guidance for the fiscal third quarter's up 5%, when I think, normally, it would be up in the low double digits. So can you talk about how comfortable you are with your supply availability here? And if demand was there for double-digit growth, if you'd be able to achieve that? And then, Colette, one for you. The OpEx guidance setup, I think it's $37 million quarter-on-quarter, more than we've seen the last few quarters. Can you just bin out what some of the bigger drivers for that increase are? Sure. Thanks a lot. So first of all, there's -- to answer that question, I would say there are 3 factors. The first factor is our strategic position. Our competitive lineup is probably the best it's ever been, and better than last year, even, which was incredibly strong, better than a year before that because it was incredibly strong. I think our strategic position and the value of our architecture is more powerful today than ever. And so our -- I think #1 is our strategic position. The second, if the demand were there in the second half with respect to -- from a perspective of Gaming demand, and if there's any residual with crypto demand, we will surely be able to serve it. And then, lastly, the factor is related to our guidance. Our guidance is -- we're comfortable with our guidance. We're happy with our guidance, and we want to have an opportunity to come back and give you an update in Q3. And Craig, on your second question regarding the OpEx guidance in Q3. Generally, our guidance and actuals as we move into Q3 is usually a little stronger, and it's consistent with our normal annual compensation increase that happens in Q3. And also keep in mind, we are expected to move into our new headquarter building within Q3. And underlying our overall growth and investments is our hiring and focus in terms of on AI, autonomous driving as well as Gaming. So all of these factors contribute to that with about a 19% year-over-year growth rate in terms of what we're targeting. The first -- the answer to your first question is yes. Volta is -- it was a giant leap. It's kind of 120 teraflops. 120 teraflops. Another way to think about that is 8 of them in 1 node is essentially 1 petaflops, which puts it in the -- among the top 20 fastest supercomputer on the planet. And I -- the entire world's top 500 supercomputers is only 700 petaflops. And with 8 Voltas in 1 box for doing artificial intelligence, it represents one of them. And so that's -- Volta is just a gigantic lead for deep learning, and it's such a gigantic leap for processing that -- and we announced that at GTC, if you recall, which is practically right at the beginning of the quarter. And so the transition was not insignificant, and it was that the team just executed flawlessly. I'm so proud of the team that they executed the most complex processor that's ever been built. And working with our teams, working with our partners at TSMC and Samsung and all of our package partners, Bill, and they just did a great job for us. And so the team did great. Now looking forward, there's a whole bunch of growth drivers for our data center business. Deep learning is -- training is a growth driver. Cloud computing, high-performance computing is a growth driver, and we have new growth drivers with inferencing. And so I'm pretty excited about our prospects going into the age of -- the generation of Volta. In terms of the guidance and what we expect, I think our dynamics are really positive. And so we just got to -- we're happy with the guidance, and let's give you an update at the end of the quarter. Yes. I think, at the highest level, the way to think about that is data-intensive computing. Data-intensive computing, whether it's deep learning or high-performance computing, the GPU is just phenomenal at it. NVIDIA's CUDA GPU was after 12 years of driving this architecture and pioneering this computing approach, it's just a home run. And the value proposition and the money that it saves people, the amount of energy that it saves, is quite extraordinary. One way to think about that is if you speed up an application by a factor of 10, you're basically saying that it takes 10x fewer servers to do the same job or you could do 10x as much work in the same amount of servers. And so the value proposition is really quite great. And the applications that we serve is really diverse now. It used to be just high-performance computing and supercomputing, but the number of applications we serve span Internet service providers, manufacturing, health care, financial services, transportation. The number of data-intensive applications and industries that need them is really growing very fast. And so how fast does that -- what does that imply in terms of long-term growth? It's kind of hard to say. It's kind of hard to say. But first principles would suggest, first principles would suggest that every single data center in the world will be GPU-accelerated someday, and I've always believed that. And I believe that even more today because I believe that, in the future, this new computing model that we all finally call AI is going to be a highly data-intensive business model, and the GPU is the ideal computing model for that. And so I'm not exactly sure if that completely answers your question, and partly because I'm not exactly sure. I just know that the -- on first principles, the computing architecture is ideal. There's every evidence that every single data center and every single OEM and every single Internet service provider is jumping on this architecture and jumping on Volta. And I believe that AI is going to be the future of computing. And so somewhere between those beliefs and executing the business is the truth. Well, that's a good question, Hans, and it's a good observation. Because Purley, I didn't know if everybody understood that codename, but Purley is a new motherboard, new platform for Intel servers and the CPU, Skylake. And it's an excellent server platform. And obviously, every OEM and every service provider was waiting for the launch of that, and it officially launched in the middle of this quarter. And so did it affect the rollout of new servers based on GPUs? Probably, it did, and surely, it did. But now that it's ramped, it's a successful ramp. Every single cloud provider and every single OEM is now fully geared up to take that server to market, and they all have GPU options. Every single OEM in the world now and every cloud provider in every ODM now has NVIDIA GPU chassis and platforms, whether it's in one GPU in one [U] to 8 GPUs in a supercomputing configuration. And so the number of options of ways to enjoy NVIDIA GPUs is really quite countless now. Volta for Gaming, we haven't announced anything. And all I can say is that our pipeline is filled with some exciting new toys for the gamers. And we have some really exciting new technology to offer them in the pipeline. But for the holiday season, for the foreseeable future, I think Pascal is just unbeatable. It's just the best thing out there. And everybody who's looking forward to playing Call of Duty or Destiny 2, if they don't already have one, should run out and get themselves a Pascal. Well, first of all, it's not really possible because our GPUs are all architecturally compatible, which, at some level, is one of our strengths. There are hundreds of millions of NVIDIA GPUs in the world, and they're all CUDA-compatible and they're all 100% CUDA-compatible. And we're so rigorous and so disciplined about ensuring their compatibility that, for developers, it's really a wonderful platform. However, we are thoughtful about how we configure the GPUs so that they're best for the applications. Some applications would like to have the maximum amount of performance in a few nodes. Some would like to have the maximum amount of performance within 30 watts. Some would like to have the maximum amount of flexibility with all of the I/O pin connectors and all display connectors. Some people like to have multi-GPUs and that they have the ability to configure them together. And so every market has a slightly different need. And we have to understand the market needs and understand what it is that the customers are looking for and configure something that is best for them. Let's see. A neural net, in terms of complexity, is approximately -- not quite, but approximately doubling every year, and this is one of the exciting things about artificial intelligence. In no time in our -- in my history of looking at computers in the last 35 years have we ever seen a double exponential, where the GPU computing model, our GPUs, are essentially increasing in performance by approximately 3x each year in order to be 100x in just 4 years. We have to increase in performance, overall system performance, by a factor of 3 -- by over a factor of 3 every year. And yet on the other hand, on top of it, the neural network architecture and the algorithms that are being developed are improving in accuracy by about twice each year. And so object recognition and accuracy is improving by twice each year, or the error rate is decreasing by half each year. And speech recognition is improving by a factor of 2 each year. And so you've got these 2 exponentials that are happening, and it's pretty exciting. And it's one of the reasons why AI is moving so fast. On the data center, it was slightly below The Street number. I know it's not your number, but I think we've gotten used to you surpassing The Street number by a wide margin. Wondering if you could just talk about the July quarter, the 3 subsegments, and if they came in as expected. And as you look to October, if you could talk about what kind of growth or -- you're looking for, for that segment. I'm not sure I understand the question. Colette, if you understand it, go ahead and answer it. So in discussing how we did versus our guidance, again, we overshot our overall guidance for Q2. Part of that was the cryptocurrency. And your question was more around the Datacenter and the Datacenter number. We set out with a good amount of work ahead of us to transition to Volta within the quarter. We're very pleased with that result and the overall year-over-year growth that we accomplished in terms of the Datacenter. We always have different ranges across the organization and across the different businesses. But we don't have specifics in terms of our guidance nor did we provide specific guidance in terms of on the Datacenter. Well, our GPUs are useless without software, [surfing the backboard forward]. Our GPUs are useless because -- without software. And the reason for that is because, otherwise, they -- each one of the markets, whether it's playing games or professional visualization or high-performance computing, doing molecular dynamics, computation or doing seismic processing or perceiving the 3-dimensional world around the car and reasoning about where it is and trying to figure out how to drive, all of that software is very, very different. What we do is we -- there's a core in our company, the core architecture is a GPU core. However, the configuration of the products and the chips and the systems are very different from market to market. And so somebody asked me earlier, the gentleman asked me about cryptocurrency. That configuration is very, very different than a Gaming configuration, which is different than a high-performance computing configuration. And it's different from our inferencing configuration, and it's different from our self-driving-car configuration. And so the chips are designed to be different even though they're architecturally identical. And then the systems are designed to be market-specific and application-specific. And the software on top of it is super, super application-specific. And that's one of the reasons why our company is increasingly differentiated from a components company and what we call a platform company. Each one of these platforms that we bring to market are very, very different even though, at its core, this data-intensive parallel computing architecture called CUDA is essential among all of them. We don't break out the automotive from the rest of the Tegra business. The Tegra business consists of basically 3 parts at the moment. One part is -- one major component of it is the Nintendo Switch gaming console, and they're just doing incredibly well. I'm so happy for Nintendo because they're risk-takers, they're innovators, they -- they're -- they don't -- they're not influenced by what other people do, and they're original thinkers and I just love the way they invented the Switch and the way they've taken it to market. I'm so happy for them. And it's doing really well. The second major component is our self-driving car platforms and a lot of it still is the infotainment systems. Our infotainment system is going to evolve into an AI cockpit product line. We're going to -- we initially started with autonomous driving, but you've probably heard me say at GTC that our future infotainment systems will basically turn your cockpit or turn your car into an AI. So your whole car will become an AI. It'll talk to you, it'll know where you are, and knows who's in the cabin. And if there's potential things to be concerned about around the car, it might even just tell you in natural language. And so the entire AI will be -- the entire car will become an AI. We announced at CES a partnership with Daimler, and they talked about the work that we're doing together in the next-generation car, how we're going to bring the AI into the car. And that's our first visible -- highly visible project, and there are others that we're working on. And then the future projects, starting from end of next year with robot taxis, and starting with 2020, the autonomous cars, fully autonomous cars. You're going to see the rest of that come online. And that's a major component of Tegra. And then the last component, the third component, the major component, I would say, is AI at the edge. That's the next major revolution. And I -- we have a new product line. It's -- that we announced about a year ago, and it's called Jetson. Jetson is just -- it's just an amazing little AI computer. And if you want to do deep learning at the edge, whether it's really, really clever cameras for smart and safe cities, to traffic lights that can now monitor traffic, Jetson and AI at the edge is going to be the next growth opportunity for us. And those 3 major segments make up essentially the Tegra business. We haven't split each one of them out separately. But one of these days, we'll consider doing it. Yes, Ambrish (sic) [Gabriel], that's a good question. Partly, I'm not in total control of the answer. But on first principles, let me maybe explain it this way. I believe that there are a few hundred million office workers and information workers whose PCs will be virtualized and just become an application, like Netflix, and it will be virtualized, it'll be streamed from our cloud GPUs called GRID. I believe that every single company in the world, manufacturing, health care, finance, will use computational approaches to analyzing their business. And some of them will use it -- will use AI and some of them will use traditional first-principle, physics-based algorithms. And I -- and it's hard to say exactly what the split's going to be. I -- my guess, however, is that AI will be the larger part of that. But you're going to see hybrid versions of it. Most computations, the reason why we're so bullish about CUDA and our GPU, which is able to do both general purpose computation as well as deep learning, is because most algorithms has the combination of both. Inside the car, we don't just use deep learning, we use CUDA and deep learning. We use CUDA for all kinds of algorithms, computer vision algorithms, including deep learning. And so -- and we're seeing in quantum chemistry, in physics simulations like fluid dynamics, more and more of the algorithms are hybrids of deep learning and numerics. And so I -- that segment of the marketplace is kind of hard to predict. And then there's the -- there's just the consumer Internet service providers and the billions and billions of queries that are going into the cloud. Some of them are text, some of them are speech, and increasingly, some of them are video. But the amount of traffic that's going to be inferenced using deep learning is going to be quite explosive. It's kind of hard to know exactly the pace of each one of these. But I think, on first principles, we would all agree that these are large computation challenges and that the previous model of using just microprocessors to do that computation is not efficient. And that the GPU, with its parallel data processing capability and now our fourth-generation deep learning architecture, you essentially have a TPU that does a whole bunch more. And so I think the approach that we take has great promise, and we're just super enthusiastic about it. But exactly how much it's going to contribute in the near term in percentages is going to be hard to guess. Okay, that was great. I appreciate all the questions. We had a great quarter. We're seeing exciting growth dynamics driving in each of our businesses. This is the era of artificial intelligence, and NVIDIA has dedicated ourselves to be its brain. Cloud and Internet service providers are going in -- going all in on AI and jumping on to our new Volta GPU. Enterprises and giant industries from transportation to health care to manufacturing to financial services have awakened to the power of AI, and the growing pipeline of the NVIDIA DGX AI supercomputer is a great indicator. The next revolution of AI will be AI at the edge, and the most visible and impactful evidence will be the autonomous vehicle. Our strategy is to build a ground-up, deep learning platform for self-driving cars, and that has put us in pole position to lead the charge. And in Gaming, which is actually the first consumer AI application, we have a great strategic position in this growing market. We have a once-in-a-lifetime opportunity ahead. We can make an amazing impact on the future of the world. Thanks for joining us today, and we look forward to giving you another update next quarter.","It sounds like things went very well on the cryptocurrency side. And that market has not had a lot of history, but a little history it has had, had some volatility. And I was wondering if you could help us understand how you think about managing that volatility. And a broader question on this topic is, do you consider cryptocurrency or other blockchain applications on par with your other 4 big markets? Yes, great. I have a question on some of the numbers. So Q2 revenue came in roughly about $250 million above your guide. Can you kind of confirm what some of the drivers were to the upside relative to your guidance? Was it all cryptocurrency or was it a combination of multiple things? And related to that, for your Q3 guide, I think you are guiding revenue up about $120 million sequentially. What are the puts and takes here on a sequential basis? Thank you. First, I was wondering if you could tell us how much Volta contributed to the data center revenue in the quarter. And what are your expectations for that ramp trajectory into the second half? And the reason I ask is, when I look at gross margins, they're fine but it doesn't look like the Volta ramp is driving upside to that number. So I'm trying to get some feeling for the trajectory of that ramp. Yes, I guess, a follow-up question to that on the Volta transition and now that, that is ramping in high-volume manufacturing and considering the pretty large uplift in die size there. Curious how you're thinking about ASP uplift over time, and whether you would expect that to drive a reacceleration in growth in data center looking into the second half of the calendar year. Congratulations on the strong results. Even if you exclude the [QEM] contribution, you would have beaten The Street expectations. My question is in auto. You've announced a very strong pipeline of auto partnerships this year. Can you just talk about when do you expect acceleration in auto sales? And are there any other ways you can monetize your auto partnerships maybe through licensing of software stacks? Yes, I just wanted to clarify some earlier comments with regard to Volta and data center. Is it correct to interpret your comments to mean that some customers may have tended to delay purchases as you went through the quarter as they're waiting for Volta, given the stronger performance gains for that? And if that's the case, if I've got that right, now that Volta is fully ramped, do you expect that to drive stronger growth rates as you go through the second half? Great. Jensen, you've had a couple questions already about sort of the pace of growth in data center in the second half from Volta, but I'm thinking a little bit further out. At GTC, you highlighted this $30 billion TAM opportunity by 2020. And when we look at the charts that you've published about your expectation for exaflops -- the number of exaflops required to train an increasing number of deep learning networks through 2020, it looks like your expectation is for that to accelerate over time. But naturally, the Street's contemplating a decelerating growth for data center. People don't expect things to grow 150-plus percent forever. So can you comment as to the growth trajectory beyond maybe the very near term in that business? Jensen, can you give us an update in terms of how the new platforms and servers may have impacted the business in the data center, with Purley launching here recently and the upcoming Epyc? And as a follow-on, when can we expect Volta in the consumer gaming markets? I just had 2 kind of high-level ones. So first, it's your comments on cryptocurrency and blockchain. So when decentralized applications begin to take off and we see people essentially renting out parallel processing, how are you guys going to essentially be able to tell what products are being used in kind of a [lease] model versus what's being used in Gaming, et cetera? Got it. And then just one small one and a follow-up, do you guys have an estimate on how fast the neural network is growing right now relative to a year ago? This is Robert Mertens on behalf of Raji. I guess I wanted to get a little more clarity towards your automotive division. I didn't know if you broke out rough percentages about the near-term growth between infotainment and these ADAS or autonomous vehicle systems. And then as a follow-up in the autonomous systems, how you're sort of pricing between the different levels, if that's mainly just the GPUs or if the software is baked in there as well. This is Gabriel Ho calling in for Ambrish. I think, last year this time, I think you -- on the Datacenter business, you disclosed -- I think, highlighted the growth for your cloud business, your deep learning business was about half, and then for HPC. I was wondering, as you're ramping Volta, how would you -- how should we think about maybe the mix as you're entering the second half of the year between HPC, cloud and maybe the rest of the business for the Datacenter? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q4 2017,1754,5225,1355,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the fourth quarter and FY17. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until the 16th of February, 2017. The webcast will be available for replay up until next quarter's conference call to discuss Q1 financial results. The content of today's call is NVIDIA's property. It cannot be replayed, reproduced, or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K, and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, the 9th of February, 2017, based on information currently available to us. Except as required by law, we assume no outlook obligation to update any such statements. During this call, we will discuss non GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Arnab. We had a stellar Q4 and FY17, with records in all of our financial metrics -- revenue, gross margin, operating margins, and EPS. Growth was driven primarily by data center tripling, with a rapid adoption of AI worldwide. Quarterly revenue reached $2.17 billion, up 55% from a year earlier, and up 8% sequentially, and above our outlook of $2.1 billion. FY17 revenue was just over $6.9 billion, up 38%, and nearly $2 billion more than FY16. Growth for the quarter and fiscal year was broad-based, with record revenue in each of our four platforms -- gaming, professional visualization, data center, and automotive. Our full-year performance demonstrates the success of our GPU platform-based business model. From a reporting segment perspective, Q4 GPU revenue grew 57% to $1.85 billion from a year earlier. Tegra processor revenue was up 64% to $257 million. Let's start with our gaming platform. Q4 gaming record revenue was a record $1.35 billion, rising 66% year on year, and up 8% from Q3. Gamers continue to upgrade to our new Pascal-based GPUs. Adding to our gaming lineup, we launched GTX 1050-class GPUs for notebooks, bringing e-sports and VR capabilities to mobile at great value. The GTX 1050 and 1050-TI were featured in more than 30 new models launched at last month's consumer electronics show. To enhance the gaming experience, we announced G-Sync HDR, a technology that enables displays which are brighter and more vibrant than any other gaming monitor. Our partners have launched more than 60 G-Sync-capable monitors and laptops, enabling smooth play without screen tear artifacts. E-sports too continues to attract new gamers. Major tournaments with multi-million dollar purses are drawing enormous audiences. This last quarter, Dota 2 held its first major tournament of the season in Boston. Tickets sold out in minutes. The prize pool reached 3 million, and millions of gamers watched online. Moving to professional visualization, Quadro revenue grew 11% from a year ago to a record $225 million, driven by demand for high-end real time renderings and mobile workstations. We recently launched a family of Pascal-based GPUs designed for mobile workstations, which leading OEMs are embracing. Earlier this week, we introduced Quadro GP100, which creates a new super-computing workstation. This new type of workstation enables engineers, designers, and artists to take advantage of new technologies of photo-realism, fluid simulation, and deep learning. Next, data center. Revenue more than tripled from a year ago, and was up 23% sequentially to $296 million. Growth was driven by AI, cloud service providers deploying GPU instances, high-performance computing, grid graphics virtualization, and our DGX AI super-computing appliance. AI is transforming industries worldwide. The first adopters were hyper-scale companies like Microsoft, Facebook, and Google, which use deep learnings to provide billions to customers with AI services that utilizes image recognition and voice processing. The next area of growth will occur as enterprise in such fields as health care, retail, transportation, and finance embrace deep learning on GPUs. At November's SC16 super-computing conference, Microsoft announced that its GPU-accelerated Microsoft Cognitive Toolkit is available both in Azure Cloud and on-premesis with our DGX-1 AI super-computer. In a series of related announcements at SC16, we described our plans to join the Cancer Moonshop project, in conjunction with the National Cancer Institute, the US Department of Energy, and several national labs. To help build predictive models and guide treatment under those projects, we are collaborating on a new AI framework called CANDLE, the Cancer Distributed Learning Environment. To support this work, we unveiled our new own super-computer, the NVIDIA DGX Saturn 5, which joins together 124 DGX-1 systems. It's currently the world's 28th fastest super-computer, and the number one system in energy efficiency. Our grid graphics virtualization business doubled year on year, driven by strong growth in the education, automotive, and energy sectors. We are excited to be hosting our eighth annual GPU Technology Conference here in Silicon Valley from May 8 to May 11. This will be the year's most important event for AI and accelerating computing, and we expect it to be our largest GTC yet, attended by thousands of application developers, scientists and academics, as well as entrepreneurs and corporate executives. Finally, in automotive, revenue grew to a record $128 million, up 38% year over year. At Jen-Hsun's CES opening keynote, we demonstrated our leadership position in self-driving vehicles. With a growing list of industry players adopting our AI car platform, we also showcased AI Copilot, a technology that will recognize a driver and their preferences, monitor their alertness, understand natural spoken language, and provide alerts in dangerous situations. One of the highlights of CES was the demonstration of our own autonomous car, dubbed BB8. More than 500 passengers took rides in the back seat without a driver behind the wheel. We announced a number of new partnerships at the show, among them were collaborations with Bosch, the world's largest automotive supplier, and ZF, Europe's leading supplier for the truck industry. Both center on developing AI car computers with Drive PX2 technology. We also announced that we're working on cloud-to-car mapping collaborations with HERE, focused on the US and Europe, and Zen Ren, focused on Japan. These complement partnerships announced in Q3 with Europe's TomTom, and China's Baidu. Our mapping partnerships now span to all geographies. Jen-Hsun was joined on the CES stage by Audi of America's President Scott Keogh. They announced the extension of our decade-long partnership to deliver cars with level four autonomy starting in 2020, powered by Drive PX technology. Audi will deliver level three autonomy in its A-8 luxury sedan later this year through its V-fast system, powered by NVIDIA. We also shared news at CES of our partnership with Mercedes-Benz to collaborate on a car that will be available by year's end. During the quarter, Tesla began delivering a new autopilot system powered by the NVIDIA Drive PX2 platform in every new Model S and Model X, to be followed by the Model 3. Tesla's cars will be capable of fully autonomous operation via future software updates. In addition, Volvo started turning over the keys to initial customers of its Drive Me program. Its XC90 SUVs equipped with Drive PX2 are capable of fully autonomous operation on designated roads in Volvo's hometown of Gothenburg, Sweden. With NVIDIA's powering the market's only self-driving cars and partnerships with leading automakers, tier-one suppliers, and mapping companies, we feel very confident in our position as the transportation industry moves to autonomous vehicles. Next, our OEM and IP business was $176 million, down 11% year on year. Now turning to the rest of the income statement for Q4. Gross margins were at record levels, with GAAP gross margins at 60% and non-GAAP at 60.2%. These reflect the success of our platform approach, as well as strong demand for G-Force gaming GPUs and deep learning. GAAP operating expenses were $570 million. Non-GAAP operating expenses were $498 million, up 12% from a year earlier, reflecting head-count-related costs for our AI growth initiatives, as well as investments in sales and marketing. We are investing into huge market opportunities: AI, self-driving cars, cloud computing, and gaming. Thus we expect our operating expense growth rate to be in the high teens over the next several quarters. GAAP operating income was $733 million, and non GAAP operating income was $809 million. Both more than doubled from a year earlier. Our GAAP tax rate was 10%, and our non-GAAP was 13%. These rates were lower than expected, primarily due to a decrease in the amount of earnings subject to US tax. GAAP EPS was $0.99; non-GAAP EPS was $1.13. In FY17, we returned $1 billion to shareholders through dividends and share repurchases, in line with our intentions. For FY18, we intend to return $1.25 billion to shareholders through dividends and share repurchases. Now turning to the outlook for the first quarter of FY18, we expect revenue to be $1.9 billion, plus or minus 2%. At the mid-point, this represents 46% growth over the prior year. We expect data center to grow sequentially. Our GAAP and non-GAAP gross margins are expected to be 59.5% and 59.7%, respectively, plus or minus 50 basis points. This guidance assumes that our licensing agreement with Intel ends at March, and does not renew. GAAP operating expenses are expected to be approximately $603 million. Non-GAAP operating expenses are expected to be approximately $520 million. GAAP OI&E is expected to be an expense of approximately $20 million, including additional charges from the early conversions of convertible notes. Non-GAAP OI&E is expected to be an expense of approximately $4 million. GAAP and non-GAAP tax rates for the first quarter of FY18 are both expected to be 17%, plus or minus 1%, excluding any discrete items. With that I'm going to turn it back for the Operator so we can open up for questions. Please limit your questions to just one. Operator, let's start with the questions.","Yes, C.J. First of all, thanks a lot. Well, the single biggest mover would have to be data center. When you look back on last year and when you look forward, there's a lot of reasons why data center business overall grew 3X, grew by a factor of three. I would expect that to happen, to continue. There's several elements of our data center business. There's the high-performance computing part, there's the AI part; there's grid, which is graphics virtualization; there's the cloud computing, which is providing our GPU platform up in the cloud for start-ups and enterprises and all kinds of external customers to be able to access in the cloud, as well as a brand new AI super-computing appliance that we created last year, for anybody who would like to engage deep learning and AI, but don't have the skills or don't have the resources or don't have the desire to build their own high-performance computing cluster. We integrated all of that with all of the complicated software stacks into an appliance that we maintain over the cloud. We call that DGX-1. These pieces -- AI, high-performance computing, cloud computing, grid, and DGX -- all in contribution contributed to our growth in data center quite substantially. My sense is that as we look forward to next year, we're going to continue to see that major trend. Of course, gaming was a very large and important factor, and my expectation is that gaming is going to continue to do that. Then longer term, our position in self-driving cars I think is becoming more and more clear to people over time. I expect that self-driving cars will be available on the road starting this year with early movers, and no later than 2020 for level four by the majors, and you might even see some of them pull into 2019. Those are some of the things we're looking forward to. Well, let's see. We typically assume that we have an installed base of $200 million G-Force gamers, and we've upgraded about two quarters of them -- as in two operating quarters out of four years. It takes about three to four years to upgrade the entire installed base. We started ramping Pascal, as you know, a few quarters ago. Our data would suggest that the upgrade cycle is going well, and we have plenty to go. Thanks, Vivek. On your question on inventory, as you know, in many of our businesses we are still carrying significant architectures, and a broad list of different products for those architectures across. We feel comfortable with our level of inventory as we look forward into FY18 and our sales going forward. Your second question was regarding OpEx, and comparing it to where we finished in 2017 and moving into FY18. We do have some great opportunities, large businesses for us to go capture the overall TAMs for, and we are going to be continuing to invest in the data center, specifically in AI, self-driving cars, as well as gaming. Rather than a focus on what the specific operating margin is, we're going to focus primarily just on growing the overall TAM, and capturing that TAM on the top line. On hyper-scale, you're absolutely right that there's what we call internal use for deep learning, and then there's the hosting GPU in the cloud for external high-performance computing use, which includes deep learning. Inside the hyper-scalers, the early adopters are moving obviously very fast, and -- but everybody has to follow. Deep learning has proven to be too effective. You guys -- everybody knows now that every hyper-scaler in the world is investing very heavily in deep learning. My expectation is that over the next coming years, deep learning and AI would become the essential tool by which they do their computing. Now when they host it in the cloud, people on the cloud use it for a variety of applications. One of the reasons why the NVIDIA GPU is such a great platform is because of its broad utility. We've been working on GPU computing now for coming up on 12 years, and industry after industry, our GPU computing architecture has been embraced for high-performance computing, for data processing, for deep learning and such. When somebody hosted up in the cloud -- for example, Amazon putting our GPUs up in the cloud -- that instance has the ability to do molecular dynamics to deep learning training to deep learning inferencing. Companies could use it for off-loading their computation, to start-ups being able to build their Company and build their application, and then host it for hundreds of millions of people to use. I think the hyper-scalers are going to continue to adopt GPU, both for internal consumption and cloud hosting for some time to come. We're just in the beginning of that cycle, and that's one of the reasons why we have a fair amount of enthusiasm around the growth here. You mentioned enterprise. Enterprise has all woken to the power of AI, and everybody understands that they have a treasure trove of data that they would like to find a way to discover insight from. In the case of real applications that we're engaging now, you could just imagine that in the transportation industry, in car companies creating self-driving cars, one car company after another needs to take all of their road data and start to train their neuro-networks for their future self-driving cars. They use our DGX or Tesla GPUs to train the networks, which is then used to run their cars running on Drive PX. That's one application example. Another application example, which is quite significant, it's going to be the future of processing all of the HD maps in the world. You guys might have seen that we announced at GTC this API SDK called MapWorks. MapWorks takes video information, video information that is recorded from a car, and reconstructs the three-dimensional terrain information from that live video. It has to do computer vision, 3-D reconstruction, has to determine and detect where the lanes are, the signs are, the lights are, and even some interesting 3-D features, maybe buildings and curbs and such. It would do that automatically, and we need to process that for the world, for the planet. You could just imagine how much video is being recorded today, and how much data is being generated, and how much inferencing, computer vision, and 3-D reconstruction that has to be done, and our GPUs are really quite perfect for it. MapWorks runs on top of our GPUs, and we're working with just about every mapping company in the world today to adopt, MapWorks and to do HD processing for their maps. That's another example. Medical imaging companies all over the world have recognized the importance of deep learning in their ability to detect cancer and retinopathy, and the list of examples goes on and on. All the different modalities have now recognized the importance of deep learning, and you're going to start to see one medical imaging company after another. The list of examples just keep on going. The fact of the matter is at this point, deep learning and AI has really become how future software development's going to be done for a large number of industries, and that's the enthusiasm we're seeing around the world. The first year of VR has sold several hundred thousand units, and many hundreds of thousands of units. Our VR Works SDK, which allows us to process graphics in very low latency, dealing with all of the computer vision, processing, and whether it's lens warping and such, has been -- has delivered really excellent results. The early VR is really targeted at early adopters. I think the focus of ensuring an excellent experience that surprises people, that delight people by Oculus and by Valve and by Epic and by Vive, by ourselves, by the industry, has really been a good focus. I think that we've delivered on the promise of a great experience. The thing that we have to do now is we have to make the headsets easier to use with fewer cables. We have to make it lighter, we have to make it cheaper. Those are all things that the industry's working on. As the applications continue to come on line, you're going to see that they're going to meet themselves and find success. I think the experience is very clear that VR is exciting. However, remember that we are also in the VR, we also brought VR to computer-aided design, and to professional applications. In this particular area, the cost is just simply not an issue. In fact, many of the applications previously were power walls or caves, VR caves, that cost hundreds of thousands of dollars, and now you could put that same experience if not even better on the desk of designers and creators. I think that you're going to find that creative use and professional use of VR is going to grow quite rapidly. Just recently we announced a brand new Quadro P5000 with VR, the world's first VR notebook that went to market with HP and Dell, and they're doing terrifically. I would think about VR in the context of both professional applications as well as consumer applications; but I think the first year was absolutely a great success. Well, the global PC gaming market is still vibrant and growing. The number of e-sports gamers around the world is growing. You guys know that Overwatch is a home run. Activision Blizzard's Overwatch is raging all over Asia, and e-sports fans all over the world are picking it up. It's graphically very intensive, without a 1050 class and above, it's simply a non-starter. To really enjoy it, you need at least a 1060. This last quarter we launched a 1050 and a 1050 TI all over the world, and we're seeing terrific success out of that. My expectation going into next year is that Overwatch is going to continue to spread all over the world. It really basically just started. It started in the west, and it's now moving into the east, where the largest e-sports markets are. Overwatch is going to be a huge success. League of Legends is going to continue to be a huge success. My expectation is that the e-sports, along with AAA titles that are coming out this year, is going to keep PC gaming continuing to grow. I quite frankly thought Q4 was pretty terrific. We had a record quarter, we had a record year. I don't remember the last time that a large business the size of ours, and surely the size of a data center business, grew by factor of three. I think we're in a great position going into next year. Yes, first of all, thanks for your question. The way to think about that is deep learning is a breakthrough technique in the category of machine learning, and machine learning is an essential tool to enable AI, to achieve AI. If a computer can't learn, and if you can't learn continuously and adapt with the environment, there's no way to ever achieve artificial intelligence. Learning, as you know, is a foundational part of intelligence. Deep learning is a breakthrough technique where the software can write software by itself, by learning from a large quantity of data. Prior to deep learning, other techniques like expert systems and role-based systems and hand-engineered features, where engineers would write algorithms to figure out how to detect at cat, and then the - they would figure out how to write another algorithm to detect a car. You could imagine how difficult that is, and how imperfect that is. It basically kind of works, but it doesn't work good enough, well enough, to be useful. Then deep learning came along. The reason why deep learning took a long time to come along is because it's singular handicap is that it requires an enormous amount of data to train the network, and it requires an enormous amount of computation. That's why a lot of people credit the work that we've done with our programmable GPUs and our GPU computing platform, and the early collaboration with deep learning AI researchers as the Big Bang, if you will -- that catalyst that made modern AI possible. We made it possible to crunch through an enormous amount of data to train these very deep neuro networks. The reason why deep learning has just swept the world, it started with convolusion neuro networks, but reinforcement networks, and time sequence networks, and all kinds of interesting adversarial networks. The list of the types of networks -- there are 100 networks being created a week. Papers are coming out of everywhere. The reason why is because deep learning has proven to be quite robust. It is incredibly useful, and this tool has at the moment found no boundaries of problems that it's figured out how to solve. I think that the traditional methods of machine learning are still going to be useful if the absolute precision of the prediction or classification is not necessarily super-important. For example, if you wanted to understand the sentiment of consumers on a particular new product that you sent, whether the sentiment is exactly right, so long as you understand the basic trend and you largely understand the sentiment, I think people would consider that information to be useful. However, if you're using machine learning for cancer detection, obviously we need to have a level of precision that is quite high. Whether it's in health care or financial services or high-performance computing, and in some areas where for example, ad-supported internet search, small differences in accuracy could make a very large difference in financial results for the advertiser, and for the people hosting the service. In all these cases, deep learning has found a great utility, and that's one of the reasons why we're seeing so much growth. Obviously for self-driving cars, being kind of right is not a good idea, and we like to be exactly right. First of all, you know that we are a full-stack platform. The way we think about all of our platforms is from the application all the way back to the fundamental architecture and the semiconductor device. In the case of Drive PX, we created the architecture, optimized for neural net, for sensor fusion, for high-speed processing; the semiconductor design, in the case of Drive PX2 called Parker, Tegra Parke; the system software for high-speed sensor fusion and moving data all the way around the car -- the better you do that, the lower cost the system will be; the neural networks on top of that, that sits on top of our deep learning SDK, called KU DNN and Tensor RT, basically frameworks of AI; then on top of that, the actual algorithms for figuring out how to use that information from perception to localization to action planning. Then on top of that, we have an API and an SDK that is integrated into map makers. We integrate into every single map, HD map service in the world, from HERE to TomTom to Zen Ren in Japan, to Baidu in China. This entire stack is a ton of software. But your question specifically has to do with the perception layer, and that perception layer quite frankly is just a small part of the self-driving car problem. The reason for that is because in the final analysis you want to detect lanes. You've got video coming in, you want to detect lanes. You have video coming in, you want to detect a car in front of you. All we have to do -- it's not trivial, but it's also not monumental -- we have to detect and sense the lanes and the cars, and we train our networks to do so. As you know very well now, the deep neural net has the ability to detect objects far better than any human-engineered computer vision algorithms prior to deep learning. That's one of the reasons why Tesla and others have jumped on top of the deep learning approach, and abandoned traditional hand-featured computer vision approaches. Anyway, the answer to your question is that by working on self-driving cars end to end, we realized that this is much more than computer vision, that the self-driving car platform is a stack of software and algorithms that's quite complex, and now we've had a lot of experience doing so. Then recently at CES, we announced partnerships with Audi, which we announced that we will have level four self-driving cars on the road by 2020. We announced a partnership with Daimler, we announced a partnership with ZF and Bosch, two of the world's top tier-one suppliers. We also announced partnerships with all of the mapping companies. If you put all that stuff together, we have the processor, we have the tier-one partnerships for the integration of the systems, we have all the software on top of it, the deep learning networks, the car partnerships, of course, and integrated into maps around the world. All that entire stack, when you put them all together, should allow us to have self-driving cars on the road. Yes, the inference market is going to be very large. It's -- as you know very well, in the future almost every computing device will have inferencing on it. A thermostat will have inferencing on it, a bicycle lock will have inferencing on it, cameras will have inferencing on it, and self-driving cars would have a large amount of inferencing on it. Robots, vacuum cleaners, you name it, smart microphones, smart speakers, all the way into the data center. I believe that long term there will be a trillion devises that has inferencing connected to edge computing devices near them, connected to cloud computing devices, cloud computing servers. That's basically architecture. The largest inferencing platform will likely be ARM devises. I think that goes without saying. ARM will likely be running inferencing networks, one bit [ex-nor], eight bit, and even some floating point. It just depends on what level of accuracy do you want to achieve, what level of perception do you want to achieve, and how fast you want to perceive it. The inferencing market is going to be quite large. We're going to focus in markets where the inferencing precision, the inferencing -- the perception scenario, and the performance by which you have to do it, is mission critical. Of course self-driving cars is a perfect example of that. Robots, manufacturing robots will be another example of that. In the future, you're going to see GTC -- if you had a chance to see that. We're working with AI City partners all over the world for end-to-end video analytics. That requires very high throughput, a lot of computation. The examples go on and on, all the way back into the data center. In the data center, there's several areas where inferencing is quite vital. I mentioned one earlier, just mapping the earth. Mapping the earth at the street level, mapping the earth in HD in three-dimensional level for self-driving cars, that process is going to require -- just a pile of GPUs running continuously as we continuously update the information that needs to be mapped. There's an inferencing which is called off-line inferencing where you have to re-train a network after you've deployed it. You would likely re-train and re-categorize, re-classify the data using the same servers that you used for training. Even the training servers will be used for inferencing. Then lastly, all of the nodes in cloud will be inferencing nodes in the future. I've said before that I believe that every single node in the cloud data center will have inferencing capability and accelerated inferencing capability in the future, and I continue to believe that. These are all opportunities for us. Sure. Yes, this is Colette. Let me see if I can help answer that. You're correct in terms of how to look at that in Q1. The delta from Q4 to Q1 is we only have a partial part of recognition from the Intel, and that stops in the middle of March. As we move forward, as well, going into Q2, we will also have the absence of what we had in Q1 moving to Q2. I'm not here to give guidance on Q2 because we just give guidance out one quarter, but keep that in mind. There's a partial amount of Intel still left in Q1, and then it depletes in Q2. If you think about our overall model, our overall business model, it has moved to higher-end, value-added platforms, and that's what we're selling. Our goal is absolutely to continue to concentrate on providing those higher-value platforms. That gives us the opportunity for gross margin as we make those investments in terms of an OpEx. We'll see what that kind of mix looks like as we go into Q2, but just to leave you with a understanding of Intel is probably what we can do here, okay? It would have to be Tesla processors using the cloud. There are several SKUs of Tesla processors. There's the Tesla processors used for high-performance computing, and it has FP64, FP32, ECC. It's designed. As Kuda of course and has been optimized for molecular dynamics, astrophysics, quantum chemistry, flow dynamics, the list goes on and on. The vast majority of the world's high-performance super-computing applications, imaging applications, 3-D reconstruction applications, has been ported on to our GPUs over the course of the last decade in sum. That's a very large part of our Tesla business. Then of course we introduced on top of the architecture our deep learning stack. Our deep learning stack starts with KU DNN, the numerics kernels -- a lot of algorithms inside them to be optimized for numerical processing of all kinds of different precisions. It's integrated into frameworks of different kinds. There's so many different frameworks from Tensor RT to Cafe, to Torch to Tiano to MXNet, to CNTK; the work that we did with Microsoft, which is really excellent -- scaling it up from one GPU to many GPUs across multiple racks. That's our deep learning stack, and that's also very important. Then the third is grid. Grid is a completely different stack. It's the world's first graphics virtualization stack, fully integrated into Citrix, integrated to VMware. Every single workstation and PC application has been verified, tested, and has the ability to be streamed from a data center. Then last year, starting -- I think we announced it and we started shipping it in August -- our DGX-1, the world's first AI super-computer appliance, which integrates a whole bunch more software of all different types, and has the ability to -- we introduced our first NVIDIA docker. It containerizes applications. It makes it possible for you to have a whole bunch of users use one DGX. They could all be running different frameworks because most environments are heterogeneous. That's DGX-1, and it's got an exciting pipeline ahead of it. It's really designed for companies and work groups who don't want to build their own super-computer like the hyper-scalers, and aren't quite ready to move into the cloud because they have too much data to move to the cloud. Everybody basically can easily buy a DGX-1 that's fully integrated, fully supported, and get to work on deep learning right away. Each one of these are all part of our data center business, but the largest, because it's been around the longest, is our Tesla business; but they're all growing, every single one of them. We currently have -- Drive PX today is a one-chip solution for level three. It can have -- and with two chips, two processors, you could achieve level four. With many processors, you could achieve level five today. Some people are using many processors to develop their level five, and some people are using a couple of processors to develop their level four. Our next generation -- so that's all based on a Pascal generation. Our next generation, the processor's called Xavier. We announced that recently. Xavier basically takes four processors and shrink it into one. We'll be able to achieve level four with one processor. That's the easiest way to think about it. While achieve level three with one processor today, next year we'll achieve level four with one processor, and with several processors, you could achieve level five. I think that the number of processors is really interesting, because we need to do the processing of sensor fusion, and we've got to do perception, we have to do localization, we have to do driving. There's a lot of functional safety aspects to it, fail over functionality, all kinds of black box recorders, all kinds of different functionality that goes into the processor. I think it's really quite interesting. In the final analysis, what's really hard -- and this is one of the reasons why our positioning in the autonomous driving market is becoming more and more clear -- is that in the final analysis, it's really a software problem, and it's an end-to-end software problem. It goes all the way from processing in the perception processing in the car, to AI processing to helping you drive, connected to HD clouds for HD map processing all over the world. This end-to-end stack of software is really quite a large undertaking. I just don't know where anybody else is currently doing that, with the exception of one or two companies. I think that's really where the great complexity is. We have the ability to see and to optimize across the entire range. Now the other thing that we announced at CS that's worth mentioning is that we believe in the future, level four means that you will have autopilot capability, hands-free autopilot capability in many scenarios; however, it's unlikely to ensure and to guarantee that in every scenario that you can achieve level four. It's just not practical for some time. However, during those circumstances, we believe that the car should still have an AI, that the car should be monitoring what's happening outside, and it should be monitoring the driver. When it's not driving for you, it's looking out for you. We call that the AI co-pilot. Whereas AI autopilot achieves level four driving, AI co-pilot looks out for you in the event that it doesn't have the confidence to drive on your behalf. I believe that's a really big breakthrough, and we're just seeing incredible excitement about it around the industry, because I think it just makes a lot of sense. The combination of the two systems allows us to achieve or build a better car. Yes, appreciate that. I think first of all, the PC gaming market is growing because of a dynamic that nobody ever expected -- a dynamic that nobody ever expected 20 years ago. That's basically how video games went from being a game to becoming a sport. Not only is it a sport, it's a social sport. In order to play some of these modern e-sports games, it's a five on five, so you kind of need four other friends. As a result, in order to enjoy, to be part of this phenomenon that's sweeping the world, that it's rather sticky. That's one of the reasons why Activision Blizzard's doing so well. That's one of the reasons why Tensen's doing so well. These two companies have benefited from tremendously from the e-sport dynamic, and we're seeing it all over the world. Although it's free to play for some people, of course you need to have a reasonably good computer to run it, and that's one of the reasons why you need G-Force in your PC, so that you can enjoy these sports. When it's also a sport, nobody likes to lose, and surely nobody likes to blame their equipment when they do lose. Having G-Force allows -- gives you confidence and gives you an edge. For a lot of gamers, it's just a world standard. I think that number one, e-sport is one of the reasons why gaming continues to grow. I think at this point it's fair to say that even though it's now the second most watched spectator sport on the planet behind Super Bowl, it is also the second highest paid winning sport behind football. It is -- it will soon be the largest sport in the world. I can't imagine too many young people long term not coming into the sport somehow, as this sport continues to expand in genres. That's one of the core reasons. Now aside for -- you asked the question about G-Force Now, which I really appreciate. The simple way to think about that is this. There are many computers in the world that simply don't have the ability to enjoy video games, whether it's extremely thin and light notebooks, Apple Macs, ChromeBooks, the integrated graphics that don't have very good capabilities. I think that it's the reasonable thing to do is to put the technology in the cloud. It took us some five years to make this possible, to put the technology in the cloud and stream the video game experience with very low latency to the computer, like Netflix does. We're basically turning the PC into a virtualized gaming experience, and putting that in the cloud. I don't know exactly how big it's going to be yet, but our aspiration is that we would reach the parts of the market where they're casual, or they just want to have another way, another device where they can game, or somebody would like to come into the gaming world and isn't quite ready to invest the time in building a computer or buying into a G-Force PC yet. I'm anxious to learn from it. When I learn more about G-Force Now, I'll be more than happy to share it. I want to thank all of you guys for following us. We had a record year, a record quarter, and most importantly, we're at the beginning of the AI computing revolution. This is a new form of computing, a new way of computing, where parallel data processing is vital to success and GPU computing that we've been nurturing for the last decade in sum is really the perfect computing approach. We're seeing tremendous growth and exciting growth in the data center market. Data center now represents -- had grew at 3X over year over year, and it's on its way to become a very significant business for us. Gaming is a significant business for us, and longer term, self-driving cars is going to be a really exciting growth opportunity. The thing that has really changed about our Company, what really defines how our Company goes to market today is really the platform approach, that instead of just building a chip that is industry standard, we created software stacks on top of it to serve vertical markets that we believe will be exciting long term that we can serve. We find ourselves incredibly well-positioned now in gaming, in AI, and in self-driving cars. I want to thank all of you guys for following NVIDIA, and have a great year.","Can you here me? Yes, my apologies, stuck on a plane here. Great results. I guess I was hoping to get a little more color on the data center side. Now that we've completed a full FY17, would love to get some clarity on the different moving parts and contributions there, then looking into FY18, how you see the growth unfolding thereafter? Thank you. Thanks. I actually had one question for Jen-Hsun and one clarification for Colette. Jen-Hsun, where are we in the gaming cycle? It's been very strong the last few years. What proportion of your base do you think has a credit to Pascal? Where does that usually peak before you launch your next-generation products? Then for Colette, just inventory dollars and base ticked up. If you could give us some comment on that? Then on OpEx activity, you did a very good job last year, but this time you're saying OpEx will go up mid-teens. Do you still think there is operating leverage in the model? Thank you. Thanks for taking my question. Question back on the data center. The growth was impressive, and I'm wondering you mentioned that the hyper-scale players really have embraced the products first. I'm wondering if you could share with us to the extent that you think that they're embracing it for their own use, or to the extent that they're deploying it for services such as machine learning as a service and enterprises are really tapping into this, also, through the hyper-scale guys. I'm wondering if you could help -- you mentioned that the enterprise is where you expect to see embracing the technology next in health care, retail, transport, finance. I'm wondering if you could share with us how you feel about that visibility, where you're getting that visibility from? Thank you. Hi, thanks for taking my question, and congratulations to the team on great results and guide. My first question is for Jen-Hsun. Jen-Hsun, on the adoption of VR for gaming, if I look at the price points of the headset and the PC, a little bit high for wider adoption. Could the use of GPU in the cloud like you guys are introducing with G-Force now be a way for the price points on VR to come down? Then I have a follow-up for Colette. Yes, thank you. First of all, congratulations on a strong FY17. If I may, Jen-Hsun, the revenue beat this quarter wasn't as big as we've seen in the last several periods, and most of it came from data center. I totally understand that when your gaming business expands as much as it has it becomes harder to beat expectations by the same margin, but I was wondering if you could just spend some time talking about gaming demand, and how you think it was during the holiday season? Yes, thanks. Jen-Hsun, can you talk a little bit about the evolution of artificial intelligence, and make a distinction between artificial intelligence versus machine learning versus deep learning? They're different kind of categorizations and implementations of those different sub-segments, so I wanted to get a sense from you how NVIDIA's end-to-end computing platform dominates machine learning relative to say the competition? Then I have a question on the gross margins, if I could, as well? Thank you very much. Jen-Hsun, you guys obviously have won some business with your automotive super-computer at Tesla in recent periods. I was curious if you could comment on some of the application porting and moving of features from the previous architecture on to your architecture, and how that's gone, and what you guys have learned through that process, and how it might be applied to some of your future partnerships? Thank you. Great, thank you for taking the question. I wonder if you could talk a little bit about the entrance market? Where are you in terms of hyper-scale adoption for specialized inference-type solutions, and how big do you think that market counts going to be? Thank you. Hello, can you hear me? Hi, this is Toshiya from Goldman. Thanks for taking the question, and congrats on the results. I had a question on gross margins. I think you're guiding Q1 gross margins only mildly below levels you saw in fiscal Q4, despite the royalty stream from Intel rolling over. I'm guessing improvement in mix and data center and parts of gaming are driving this, but A, is that the right way to think about the puts and takes going into Q1? B, if that is indeed the case, should we expect gross margins to edge higher in future quarters and future years as data center becomes a bigger percentage of your business? Hi, thanks for taking my questions. First one is on the data center segment. Just given the expected sequential growth in that business here in the April quarter, can you talk about what products are helping to drove that? Is it possibly the DGX-1 computer box, or is it more GPUs for training purposes at the hyper-scale cloud data center? Great, thanks a lot for the time. Just a quick question in the auto market. At CES, you had some solutions you were demonstrating that showed pretty significant declines in the size of what was being offered. You really shrunk it down a lot, yet still having great performance. If you think out to the level-four solution that you talk about for 2020, how small can you ultimately make that? It seems like you could be relative to the size of the car pretty small. Just curious if you would comment on that, and what impact having the system in the car makes on it? Thanks for sneaking me in, and congratulations on the very good execution. Jen-Hsun, I wanted to come back to the gaming platform. You've now got the business running at a $5 billion annualized run rate, so congratulations on the growth there. I think investors look at that as a business that's been built on the strength of a vibrant enthusiast market, but at CES, you announced the G-Force Now offering, which really allows you to tap into the more casual potential gamer. The question is, what will G-Force now do incrementally for the opportunity that you have with your gaming platform? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q1 2018,1824,5627,929,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2018. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until May 16, 2017. The webcast will be available for replay up until next quarter's conference call to discuss Q2 financial results. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risk factors and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 9, 2017, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO Commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Shawn. We had a strong start to the year, highlighting our record first quarter with a near tripling of Datacenter revenue, reflecting surging interest in artificial intelligence. Overall, quarterly revenue reached $1.94 billion, up 48% from a year earlier, down 11% sequentially and above our outlook of $1.9 billion. Growth remained broad-based with year-on-year gains in each of our 4 platforms: Gaming, Professional Visualization, Datacenter and Automotive. From a reporting segment perspective, Q1 GPU revenue grew 45% to $1.56 billion from the year earlier and Tegra processor revenue more than doubled to $332 million. And we recognized the remaining $43 million in revenue from our Intel agreement. Let's start with our Gaming platform. Gaming revenue in the first quarter was $1.03 billion, up 49% year-on-year. Gamers continue to show great interest in the Pascal-based GPUs, including gaming notebooks. Our Tegra gaming platforms also did extremely well. Demand remained healthy for our enthusiast class, GeForce GTX 1080 GPU, introduced nearly a year ago. It was complemented this past quarter with GTX 1080 Ti, which runs 35% faster and was launched at the annual Game Developers Conference in San Francisco. GTX 1080 Ti is designed to handle the demand of 4K gaming and high-end VR experiences. Typical of many supportive reviews, Ars Technica, stated, ""It is undoubtedly a fantastic piece of engineering, cool, quiet and without rival. Those that demand the absolute very best in cutting-edge graphics need look no further."" We also released the next generation of our Titan class product, the TITAN Xp, designed for enthusiasts and researchers who demand extreme performance. Gaming continues to be driven by the headlong growth in eSports. The newest title, Overwatch, added 30 million gamers in its first year. GeForce was the graphics platform of choice at all of the top eSports tournaments, including the finals of the big 4 international competitions. With apologies to the start of the baseball season, eSports is now as popular among U.S. male millennials as America's favorite pastime. More people watch gaming than HBO, Netflix, ESPN and Hulu combined. GeForce sales remained underpinned by the steady stream of AAA titles coming onto market, which continue to push for more GPU performance. In the months ahead, we'll see a series of highly anticipated blockbuster titles. Among them are Destiny 2, coming to the PC for the first time, Star Wars: Battlefront 2, Shadow of War and the next installment of the Call of Duty franchise, World War II. We are excited to be working with Nintendo on its acclaimed Switch gaming system. Great reviews and reports of the system selling out in many geographies are a strong part of this platform. Moving to Professional Visualization. Quadro revenue grew to $205 million, up 8% from a year ago, amid continued demand for high-end real-time rendering and more powerful mobile workstation. We're seeing significant increase in professional VR solutions driven by Quadro P6000 GPUs. Lockheed Martin is deploying Quadro to create realistic VR walk-throughs of the U.S. Navy's most advanced ships. The Marines utilize VR to train aircrew personnel. And IKEA is rolling out VR to many of its stores, helping consumers configure their kitchens from a huge array of options, which they can visualize in sharp detail. Next, Datacenter. Record revenue of $409 million was nearly triple that of a year ago. The 38% rise from Q4 marked its seventh consecutive quarter of sequential improvement. Driving growth was demand from cloud service providers and enterprises building training clusters for web services plus strong gains in high-performance computing, GRID graphics visualization and our DGX-1 AI supercomputer. AI has quickly emerged as the single most powerful force in technology. And at the center of AI are NVIDIA GPUs. All of the world's major Internet and cloud service providers now use NVIDIA Tesla-based GPU accelerators: AWS, Facebook, Google, IBM and Microsoft as well as Alibaba, Baidu and Tencent. We also announced that Microsoft is bringing NVIDIA Tesla P100 and P40 GPUs to its Azure cloud. Organizations are increasingly building out AI-enabled applications using training clusters, evident in part by growing demand for DGX-1. We are seeing a number of significant deals. Among them are Fujitsu's installment of 24 systems integrated into an AI supercomputer for RIKEN, Japan's largest research center, as well as new supercomputers at Oxford University, GE and Audi. Working with Facebook, we announced the launch of the Caffe2 deep learning framework as well as Big Basin servers with Tesla P100 GPUs. To help meet huge demand for expertise in the field of AI, we announced earlier today plans to train 100 people this year through the NVIDIA Deep Learning Institute, representing a 10x increase from last year. Through on-site training, public events and online courses, DLI provides practical training on the tools of AI to developers, data scientists and researchers. Our HPC business doubled year-on-year, driven by the adoption of Pascal GPUs into supercomputing centers worldwide. The use of AI and accelerated computing in HPC is driving additional demand in governance intelligence, higher education research and finance. Our GRID graphic virtualization business more than tripled, driven by growth in business services, education and automotive. Intuit's latest TurboTax release deploys GRID to connect tax filers seeking real-time advice with CPAs. And Honda is using GRID to bring together engineering and design teams based in different countries. Finally, Automotive. Revenue grew to a record $140 million, up 24% year-over-year and 9% sequentially, primarily from infotainment modules. We are continuing to expand our partnerships with companies using AI to address the complex problem of autonomous driving. Since our DRIVE PX 2 AI car platform began shipping just 1 year ago, more than 225 car and truck makers, suppliers, research organizations and start-ups have begun developing with it. That number has grown by more than 50% in the past quarter alone, the result of the platform's enhanced processing power and the introduction of TensorRT for the in-vehicle AI inferencing. This quarter, we announced 2 important partnerships: Bosch, world's largest auto supplier, which does business all over the world, carmakers is working to create a new AI self-driving car computer based on our Xavier platform; and Paccar, one of the world's largest truck makers, is developing self-driving solutions for Peterbilt, Kenworth and DAF. We continue to view AI as the only solution for autonomous driving. The nearly infinite range of road conditions, traffic patterns and unexpected events are impossible to anticipate with hand-coded software or computer vision alone. We expect our DRIVE PX 2 AI platform to be capable of delivering Level 3 autonomy for cars, trucks and shuttles by the end of the year, with Level 4 autonomy moving into production by the end of 2018. Now turning to the rest of Q1 income statement. GAAP and non-GAAP gross margins for the first quarter were 59.4% and 59.6%, respectively, reflecting the decline in Intel licensing revenue. Q1 GAAP operating expenses were $596 million. Non-GAAP operating expenses were $517 million, up 17% from a year ago, reflecting hiring for our growth initiatives. GAAP operating income was $554 million and non-GAAP operating income was $637 million, nearly doubling from a year ago. For the first quarter, GAAP net income was $507 million, non-GAAP net income was $533 million, more than doubling from a year ago, reflecting revenue strengths as well as gross margin and operating margin expansion. For fiscal 2018, we intend to return approximately $1.25 billion to shareholders through share repurchases and quarterly cash dividends. In Q1, we issued $82 million in quarterly cash dividends. Now turning to the outlook for the second quarter of fiscal 2018. We expect revenue to be $1.95 billion plus or minus 2%. Excluding the expiry of the Intel licensing agreement, total revenue is expected to grow 3% sequentially. GAAP and non-GAAP gross margins are expected to be 58.4%, 58.6%, respectively, plus or minus 50 basis points. These reflect approximately a 100 basis points impact from the expiry of the Intel licensing agreement. GAAP operating expenses are expected to be approximately $605 million. Non-GAAP operating expenses are expected to be approximately $530 million. GAAP OI&E is expected to be an expense of approximately $8 million, inclusive of additional charges from early conversions of convertible notes. Non-GAAP OI&E is expected to be an expense of approximately $3 million. GAAP and non-GAAP tax rates for the second quarter of fiscal 2018 are both expected to be 17%, plus or minus 1%, excluding discrete items. Further financial details are included in the CFO Commentary and other information available on our IR website. Finally, this week, we are sponsoring our annual GPU Technology Conference here in Silicon Valley. Reflecting the surging importance of accelerating computing, GTC has grown to more than 7,000 attendees from 60 countries, up from 1,000 when we started 8 years ago. Among its highlights, Jen-Hsun will deliver a news-filled keynote tomorrow morning. We have 550-plus talks, more than half on AI. Developers will have access to 70 labs and workshops to learn about deep learning and GPU computing. And we will award a total of $1.5 million to the 6 most promising companies, among the 1,300 in our Inception Program for AI start-ups. We will be hosting our Annual Investor Day tomorrow and hope to see many of you there. We will now open up the call for questions. (Operator Instructions) Operator, will you please poll for questions?","Yes, Mark, thanks for your question. So our GPU computing business for Datacenter is growing very fast and it's growing on multiple dimensions. On the one hand, there's high-performance computing using traditional numerical methods. We call that HPC. That's growing. There is, in enterprise, the virtualization of graphics. There's a whole lot of desktop PCs running around. However, more and more people would like to have thinner laptops or they would like to have a different type of computer and still be able to run Windows. And they would like to virtualize basically their entire PC and put it in the data center. It's easier to manage. The total cost of ownership is lower, and mobile employees could enjoy their work wherever they happen to be. And so the second pillar of that is called GRID and it's basically virtualizing the PC. And as you can tell, virtualization, mobility, better security, those are all driving forces there. And then there's the Internet companies. And the Internet companies, as you mentioned, really has 2 pillars. There's the Internet service provision part where they're using deep learning for their own applications, whether it's photo tagging or product recommendation or recommending a restaurant or something you should buy or personalizing your webpage, helping you with search, provisioning up the right apps, the right advertisement, language translation, speech recognition, so on and so forth. I mean, there's a whole bunch of amazing applications that are made possible by deep learning. And so Internet service providers are using it for internal application development. And then lastly, what you mentioned is cloud service providers. And basically, because of the adoption of GPUs and because of the success of CUDA and so many applications are now able to be accelerated on GPUs, so that we can extend the capabilities in Moore's Law so that we can continue to have the benefits of computing acceleration, which in the cloud means reducing cost. And that's on the serve -- cloud service provider side of the Internet companies. So that would be Amazon Web Services, the Google Compute cloud, Microsoft Azure, the IBM Cloud, Alibaba's Aliyun... (technical difficulty) by Microsoft Azure. We're starting to see almost every single cloud service around the world standardizing on the NVIDIA architecture. So we're seeing a lot of growth there as well. And so I think the nut of it all is that we're seeing data center growth in GPU computing across the board. Well, PC gaming is growing, I mean, there's no question about that. The eSports is growing. The number of players in eSports, the number of people who are enjoying eSports is growing. Mobile is growing. I think it's amazing. It's amazing, the growth of mobile and the latest games. And of course, the first-party titles, the AAA titles are doing great work. Battlefield is doing great and I'm looking forward to the new Battlefield. I'm looking forward to the new Star Wars and I'm looking forward to the first time that Destiny is coming to the PC. As you know, it was a super hit on console. But the first-generation Destiny wasn't available on PCs and Destiny 2 is coming to the PC. So I think the anticipation is pretty great. So I would say that PC gaming continues to grow and it's hard to imagine people... (technical difficulty) around in another amazing world. And so I think people are going to be amazed at how long the alternate reality of the video gaming market is going to continue to expand. Jen-Hsun, for my first one, it's on the competitive landscape in your Datacenter business. There's been more noise around FPGA or CPU or ASIC solutions also chasing the same market. What do you think is NVIDIA's sustainable competitive advantage? And what role is CUDA playing in helping you maintain this lead in this business? Yes, Vivek, thanks for the question. First of all, it's really important to understand that the data centers, the cloud service providers, the Internet companies, they all get kind of lumped together in one conversation. But obviously, the way they use computers are very different. There are 3 major pillars of computing up in the cloud -- or in large data centers. Hyperscale. The one pillar is just internal use of computing systems for developing, for training, for advancing artificial intelligence. That's a high-performance computing problem. It's a very complicated software problem. The algorithms are changing all the time. They're incredibly complicated. The work that the AI researchers are doing are not trivial, and that's why they're in such great demand. And it's also the reason why computing resources have to be provisioned to them, so that they can be productive. Having a scarce AI researcher waiting around for a computer to finish simulation or training is really quite unacceptable. And so that one -- that first pillar is the market that we -- is a segment of the... (technical difficulty) or gets strained, once the network is trained, it is put into production. Like for example, your Alexa speakers has a little tiny network inside. And so obviously, you can do inferencing on Alexa. It does voice recognition on the hot keyword. In the long term, your car will be able to do voice recognition and speech recognition. Are we okay? Are we still on? Yes, I think the call got dropped. No, no, Vivek, I'm just -- I was wondering whether the phone line was cut or not. So anyways, I -- the second pillar is inferencing. And inferencing, inferencing as it turns out, is far, far less complicated than training. It's 1 trillion times less complicated, 1 billion times -- 1 trillion times less complicated. And so once the network is trained, it can be deployed. And there are thousands of networks that are going to be running inside these hyperscale data centers, thousands of different networks, not one, thousands of different types. And they're detecting all kinds of different kinds of things. They're inferring all kinds of different things, classifying, predicting, all kinds of different things, whether it's photo or voice or videos or searchers or whatnot. And in that particular case, our advantage -- in that particular case, the current incumbent is CPUs. The CPU is really the only processor at the moment that has the ability to basically run every single network. And I think that's a real opportunity for us. And it's a growth opportunity for us. And one would suggest that FPGA is as well. One would suggest that ASICs like TPUs -- TPUs and ASIC is as well. And I would urge you to come to the keynote tomorrow and maybe I'll say a few words about that tomorrow as well. And then the last pillar is cloud service providers, and that's basically the outward public cloud provisioning a computing approach. It's not about provisioning inferencing. It's not about provisioning GPUs. It's really provisioning a computing platform. And that's one of the reasons why the NVIDIA CUDA platform and all of our software stacks that we've created over time, whether it's for deep learning or molecular dynamics or all kinds of high-performance computing codes or linear algebra or computer graphics, all of our different software stacks make our cloud computing platform valuable. And that's why it's become the industry standard for GPU computing. And so those are 3 different pillars of hyperscalers, and it's just important to segment them so that we don't get confused. Got it. Very helpful. And as my quick follow-up, Jen-Hsun, there is a perception that your gaming business has been driven a lot more by pricing and adoption of more premium products, and hence, there could be some kind of ceiling to how much gamers are willing to pay for these products. Could you address that? Are you seeing the number of gamers and the number of cards grow? And how long can they continue to reach for more premium product? The average selling price of the NVIDIA GeForce is about 1/3 of a game console. That's the way to think about it. That's the simple math. People are willing to spend $200, $300, $400, $500 for a new game console. And the NVIDIA GeForce GPU, the PC gaming card is, on average, far less. There are people who just absolutely demand the best. And the reason for that is because they're driving a monitor or they're driving multiple monitors at a refresh rate well beyond the TV. And so if you have a 4K or you want to run 120 hertz, or some people are even driving at the 2 and 4 -- 200 hertz, those kind of displays demand a lot more horsepower to drive than an average television, whether it's 1080p or 4K, at 60 frames per second or 30 frames per second. And so the amount of horsepower they need is great. But that's just because they just really love their rig and they're surrounded in it and they just want the best. But the way to think about that is ultimately, that's the opportunity for us. I think GeForce is a game console, and the right way to think about that is at an equivalent ASP of some $200, $300, that's probably potentially the opportunity ahead for GeForce. Well, first of all, GeForce is sold a unit at a time. And it's sold all over the world and it's a consumer product. It's a product that I -- that is sold both into our installed base as well as growing our installed base. When we think about GeForce, they're -- these are the parameters involved. How much of our installed base has upgraded to Pascal? How much of our installed base is growing? How is Gaming growing overall? How is -- what are the driving dynamics of Gaming, whether it's eSports or mobile or using games for artistic expression? It's related to the AAA titles that are coming out. Some years, the games are just incredible. Some years, the games are less incredible. These days, the production quality of the games have just become systematically so good that we've had years now of blockbuster hits. So these are really good dimensions of it. And then there is -- it's overlaid on top of it with some seasonality because people do buy graphics cards and game consoles for Christmas and the holidays and there are international holidays where people are given money as gifts and they save up the money for a new game console or a new game platform. And so in a lot of ways, our business is driven by games. So it's not unlike the characteristics of the rest of the gaming industry. The driving reasons for inventory growth is new products. And I -- that's probably all I had to say for now. I would come to GTC. Come to the keynote tomorrow. I think it will be fun. Thanks, Toshiya. Let me think here. I think 1 year ago, 1 year ago was -- maybe it was 2 years ago, maybe it's somewhere between 18 months ago or so when I think Jeff Dean gave a talk where he said that Google was using a lot of GPUs for deep learning. I think it wasn't much longer ago than that. And really, really, that was the only public customer that we had in the hyperscale data center. Fast forward a couple of years, we now have basically everybody. Every hyperscaler in the world is using NVIDIA for either deep learning, for some announcements that you'll read about in data center deployment, tomorrow hopefully, and then a lot of them have now standardized on provisioning the NVIDIA architecture in the cloud. And so I guess in the course of 1 or 2 years, we went from basically hyperscale being an insignificant part of our overall business to quite a large part of our business, and as you could see also, the fastest-growing part of the business. Yes. Thanks for the question. We feel good about the guidance that we're providing for Q2. We wanted to make sure those understood the impact of Intel that's incorporated in there. It's still too early given that it's just about the same size as what we just finished in Q1 to make comments specifically exactly where we think each one of those businesses will end up. But again, we do believe Datacenter is a super great opportunity for us. I think you'll hear more about that tomorrow. But we don't have any more additional details on our guidance. But we feel good about the guidance we gave. Yes, Atif, thanks for the question. GeForce NOW is a really -- it's really an exciting platform. It virtualizes GeForce. It puts it in the cloud, turns it into a gaming PC as a service -- that can be streamed as a service. And we are -- I said at GTC that around this time, that we'll likely open it up for external beta. We've been running internal beta for some time. And we'll shortly go to external beta. And last time I checked, there's many, many, tens of thousands of people who are signed up for external beta trials. And so I'm looking forward to letting people try it. But the important thing to realize about that is I -- that's still years away from really becoming a major gaming service. And that's -- it's still years away from being able to find the right balance between cost and quality of service and the pervasiveness of virtualizing a gaming PC. So we've been working on it for several years and these things take a while. My personal experience is almost every great thing takes about a decade. And if that's so, then we've got a few more years to go. Well, consoles is not really a business to us. It's a business to them. And we're selected to work on these consoles. And if it makes sense and the strategic alignment is great and we're in a position to be able to do it -- because the opportunity cost of building a game console is quite high. The number of engineers who knows how to build computing platforms like this -- and in the case of the Nintendo Switch, I mean, it's just an incredible console that fits in such a small, small form factor. And it could both be a mobile computing -- mobile gaming device as well as a console gaming device. It's just really quite amazing. And they just did an amazing job. Somebody asked me a few months ago before it was launched how I thought it was going to do. And of course, without saying anything about it, I said that it delighted me in a way that no game console has done in the last 10, 15 years. And it's true. I mean, this is a really, really innovative product and really quite a genius. And if you ever have a chance to get it in your hands, it's just really, really delightful. And so in that case -- in that case, the opportunity to work on it was just really, really too enticing. We really wanted to do it. And -- but it always requires deep strategic thought because it took several hundred engineers to work on. And they could be working on something else like all of the major initiatives we have. And so we have to be mindful about the strategic opportunity cost that goes along with these. But in the case of the Nintendo Switch, it's just a home run. I'm so glad I did it. And it was a perfect collaboration for us. I wanted to follow up on some of the prepared comments on Automotive with my first question. And it's this. I think Colette mentioned that there were 225 car and truck development engagements that were underway, up 50% in the last quarter. The question is, as you engage with those partners, what's NVIDIA finding in terms of the time from engagement to revenue generation? And what are you finding with your hit rate in terms of converting those individual engagements into revenues? I know the second one: easier. Not -- the second one is the revenue contribution is not significant at the time, at this moment. But I expect it to be high. And that's why we're working on it. The 200 developers who are working on the DRIVE PX platform are doing it in a lot of different ways. And at the core, it's because in the future, every aspect of transportation will be autonomous. And if you think through what's going on in the world and one of the most important -- one of the most powerful effects that's happening right now is the Amazon effect. We're grabbing our phone. We're buying something and then we expect it to be delivered to us tomorrow. Well, in -- when you sent up that -- those set of electronic instruction, the next thing that has to happen is a whole bunch of trucks has to move around. And they have to go from trucks to maybe smaller trucks and from smaller trucks to maybe a small van that ultimate delivers it to your house. And so if you will, transportation is the physical Internet, is the atomic Internet, the molecular Internet of society. And without it, everything that we're experiencing today would be able to continue to scale. And so you could imagine everything from autonomous trucks to autonomous cars, surely, and autonomous shuttles and vans and motorcycles and small piece of delivery robots and drones and things like that. And for a long time, it's going to augment truck drivers and delivery professionals who, quite frankly, we just don't have enough of. The world is just growing too fast in an instant delivery, delivered to your home, delivered to you right now phenomenon. And we just don't have enough delivery professionals. And so I think autonomous capability is going to make it possible for us to take pressure off of that system and reduce the amount of accidents and make it possible for that entire infrastructure to be a lot more productive. And so that's one of the reasons why you're seeing so much enthusiasm. It's not just the branded cars. I think the branded cars get a lot of attention and we are excited about our partnerships there. And gosh, I love driving autonomous cars. But in the final analysis, I think the way to think about the autonomous future is every aspect of mobility and transportation and delivery will be -- will have autonomous -- will be augmented by AI. That's very helpful color, Jen-Hsun. The follow-up is related to the Datacenter business. And you provided a lot of very useful customer and other information. My question is higher level. Given your very unique position in helping to nurture AI for the last many years and your deep insights into the way the customers are adopting it, as investors try and understand the sustainability of recent growth, can you help us understand where you believe AI adoption is overall? And since Colette threw out a baseball comment earlier, if we thought about AI adoption in reference to a 9-inning game, where are we in that 9-inning game? Well, let's see here. There's a -- it's a great question and there's a couple of ways to come at it. First of all, AI is going to infuse all of software. AI is going to eat software. Whereas Mark said that software is going to eat the world, AI is going to eat software. And it's going to be in every aspect of software. Every single software developer has to learn deep learning. Every single software developer has to apply machine learning. Every software developer will have to learn AI. Every single company will use AI. AI is the automation of automation. And it's -- will likely be the transmission. We're going to, for the first time, see the transmission of automation, the way we're seeing the transmission and the wireless broadcast of information for the very first time. And I'm going to be able to send you automation, send you a little automation by e-mail. And so the ability for AI to transform industry is well understood now. It's really about automation of everything. And the implications of it is quite large. We've been using now deep learning. We've been in the area of deep learning for about 6 years. And the rest of the world has been focused on deep learning for about somewhere between 1 to 2. And some of them are just learning about it. And almost no companies today use AI in a large way. So on the one hand, we know now that the technology is of extreme value. And we under -- we're getting a better understanding of how to apply it. On the other hand, no industry uses it at the moment. The automotive industry is in the process of being revolutionized because of it. The manufacturing industry will be. Everything in transport will be. Retail, Etail everything will be. And so I think the impact is going to be large and we just -- we're just getting started. We're just getting started. Now that's kind of a first inning thing. The only trouble with the baseball analogy is that in the world of tech, things don't -- every inning is not the same. In the beginning, the first inning feels like -- it feels pretty casual and people are enjoying peanuts. The second inning, for some reason, is shorter. And the third inning is shorter than that and the fourth inning is shorter than that. And the reason for is because of exponential growth. Speed is accelerating. And so from the bystander who are on the outside looking in, by the time third inning comes along, it's going to feel like people are traveling at the speed of light next to you. Now if you happen to be on one of the photons, you're going to be okay. But if you're not on the deep learning train in a couple of 2, 3 innings, it's gone. And so that's kind of the challenge of that analogy because things aren't moving in linear time. Things are moving in exponential time. Yes, Hans. I think there's a couple of ways to think about it. First of all, we know that this is the -- we know that some -- the world causes the end of Moore's Law, but it's really the end of 2 dynamics that has happened. And one dynamic, of course, is the end of processor architecture productive innovation, okay, end of instruction-level parallelism advances. The second is the end of Dennard scaling. And the combination of those 2 things makes it look like it's the end of Moore's Law. The easy way to think about that is -- the easy way to think about that is that we can no longer rely -- if we want to advance computing performance, we can no longer rely on transistor advances alone. That's one of the reasons why NVIDIA has never been obsessed about having the latest transistors. We want the best transistors. There's no question about it. But we don't need it to advance. And the reason for that is because we advance computing on such a multitude of levels all the way from architecture, this architecture we call GPU accelerate computing, to the software stacks on top, to the algorithms on top, to the applications that we work with. We tune it across the top, from the top to bottom, all the way from bottom to top. And so as a result, transistors is just one of the 10 things that we use. And like I said, it's really, really important to us and I want the best. And TSMC provides us the absolute best that we can get. And we push along with them as hard as we can. But in the final analysis, it's one of the tools in the box. I have attended GTC the last couple of days. I'm really quite impressed by the breadth of presentations and sort of the number of industries you guys are affecting. And I guess just on that note, how do you think about segmenting the sales effort? Do you have a health care vertical, an avionics vertical, financial vertical? Or is it sort of having the best building blocks and you're letting your customers discover stuff? Yes, thanks a lot. Thanks a lot, Joe. You answered it right there. It's both of those. It's both of those. On the first -- the first thing is that we have to develop platforms that are useful per industry. And so we have a team working with the health care industry. We have a team that's working with the Internet service providers. We have a team that's working with the manufacturing industry. We have a team that's working with the financial services industry. We have a team that's working with media and entertainment and with enterprise, so -- with the automotive industry. And so we have different verticals. We call them verticals and we have teams of business development people, developer relations, computational mathematicians that works with each one of the industries to optimize their software for our GPU computing platform. And so it starts with developing a platform stack. Of course, one of the most famous examples of that is our Gaming business. It's just another vertical for us. And it starts with GameWorks that runs on top of GeForce and then it has its own ecosystem of partners. And so that's for each one of the verticals and each one of the ecosystems. And then the second thing that we do is we have, horizontally, partner management teams that work with our partners, the OEM partners and the go-to-market partners so that we could help them succeed. And then, of course, we rely a great deal on the extended sales force of our partners so they could help to evangelize our computing platform all over the world. And so there's this mixed approach between dedicated vertical market, business development teams as well as a partnership approach to partnering with our OEM partners that has really made our business scale so fast. Great. That's helpful. And then the other question I had was regarding Colette's comment that HPC had doubled year-on-year. Just wondering if you have any comments on what drove that. And is that an indication of the supercomputer types of businesses? Or are there sort of other dynamics in terms of addressing new workloads with HPC products? Well, HPC is different than supercomputing. Supercomputing to us is a collection of -- not a collection but is 20 different supercomputing sites around the world. And some of the famous ones, whether it's Oak Ridge or Blue Water at UCSC, you've got TiTech in Japan, there are supercomputing centers that are either national supercomputing centers or they could be public and open supercomputer centers or open science. And so we consider those supercomputing centers. High-performance computing is used by companies who are using simulation approaches to develop products or to, well, to simulate something. It could be scenarios for predicting equity, or, for example, as you guys know, Wall Street is the home of some of the largest supercomputing center -- or high-performance computing centers. The energy industry, Schlumberger, for example, is a great partner of ours and they have a massive, massive high-performance computing infrastructure. And Procter & Gamble uses high-performance computers to simulate their products. And so I think last year, McDonald's was at GTC and I hope they come this year as well. And so I think high-performance computing, another way of thinking about it is that more and more people really have to use simulation approaches for product discovery and product design and product simulation and stuff to stress the products beyond what is possible in a physical way so that they understand the tolerance of the products to make sure they are as reliable as possible. Just curious, Jen-Hsun, we've seen half dozen, dozen private companies going after the (inaudible) silicon Google TPU. I know you felt the comparison to a TPU maybe wasn't fair. But I was just kind of curious to your response to these claims of a 10, 100, 500x performance better than a GPU. Well, it's not that it's not fair. It's just not right. It's not correct. And so in business, who cares about being fair? And so I wasn't looking for fair. I was just looking for right. And so the data has to be correct. It turns out -- and I said earlier that our hyperscale businesses have 3 different pillars. There's training, which our GPUs are quite good at. There's cloud service provision, which is a GPU computing architecture opportunity where CUDA is really the reason why people are adopting it and all the applications that have been -- that has adopted CUDA over the years. And then there's inferencing. And inferencing is a 0 opportunity for us, a 0 business for us at the moment. I mean, we do 0% of our business in inferencing and it's 100% on CPUs. And in the case of Google, they did a great thing and built a TPU as an ASIC. And they compared the TPU against one of our older GPUs. And so I published a blog. I wrote a blog to clarify some of the comparisons and you could look that up. But the way to think about that is our Pascal is probably approximately twice the performance of the TPU, the first generation TPU. And it's incumbent upon us to continue to drive the performance of inferencing. This is something that's still kind of new for us. And tomorrow, I'm probably going to say a few words about inferencing and maybe introduce a few ideas. But inferencing is new to us. It's -- there are 10 million CPUs in the world in a cloud, and today, many of them are running Hadoop and doing queries and looking up files and things like that. But in the future, the belief is that the vast majority of the world's cloud queries will be inference queries, will be AI queries. Every single query that goes into the cloud will likely have some artificial intelligence network that it processes. And I think that's our opportunity. We have an opportunity to do inferencing better than anybody in the world and it's up to us to prove it. At the moment, I think it's safe to say that the P40, the Tesla P40, is the fastest on the planet, period. And then from here on forward, it's incumbent upon us to continue to lean into that and do a better, better job. And then just moving to the gaming GPU side. I was just wondering if you could just talk about the competitive landscape, looking back at the last refresh and then looking forward into the back half of this year. I think your competitor's going to have a new platform. Just kind of curious to your thought as to how the share worked out on the previous refresh and then the competitiveness into the second half of this year. My assessment is that the competitive position is not going to change. Mitch, yes, I was just talking about that earlier in one of the questions. It's called GeForce NOW and I announced it at CES. And I said that right around this time of the year, we're going to open it up for external beta. We've been running internal beta and closed beta for some time. And so we're looking forward to opening up the external beta. My expectation is that it's going to be some time as we scale that out. It's going to take several years. I don't think it's something that's going to be an overnight success. And as you know, overnight successes don't happen overnight. However, I'm optimistic about the opportunity to extend the GeForce platform beyond the gamers that we currently have in our installed base. There are several billion gamers on the planet and I believe that every human will be a gamer someday. And every human will have some way to enjoy an alternative universe some way, someday. And we would love to be the company that brings it to everybody. And the only way to really do that on a very, very large-scale basis and reach all those people is over the cloud. And so I think our PC gaming business is going to continue to be quite vibrant. It's going to continue to advance. And then hopefully, we can overlay our cloud reach on top of that over time. Well, thanks for all the questions today. I really appreciate it. We had another record quarter. We saw growth across our 4 market platforms. AI is expanding. Datacenter nearly tripled. Large ISP, CSP deployments everywhere. PC gaming is still growing; eSports, AAA gaming titles, fueling our growth there. And we have great games on the horizon. Autonomous vehicles, becoming imperative on all sectors of transportation, as we talked about earlier. We have a great position with our DRIVE AI computing platform. And as Moore's Law continues to slow, GPU accelerated computing is becoming more important than ever and NVIDIA is at the center of that. Don't miss tomorrow's GTC keynote. We'll have exciting news to share, next-generation AI, self-driving cars, exciting partnerships and more. Thanks, everybody.","On the HPC and Datacenter business, clearly impressive growth. And I'm hoping that you can maybe drill down on the drivers here. I guess on the cloud side, we think of 2 different areas, GPU as a service versus the cloud company's own AI effort. And I'm hoping you could help us understand to the extent where the demand is falling into either one of those buckets. And then on the enterprise side, I think there's a view out there that the enterprise is just -- is going to the cloud. So to hear you talk about training clusters for web services is very interesting. And I was hoping you could provide some more color on that demand driver. And a follow-up, if I may. On the Gaming side, what we have observed over time is that when you launch a new platform, it definitely creates demand and you see 12 months of very good visibility and into growth. And I wonder -- I was wondering, as you see the Datacenter numbers come in quarter after quarter here, to what extent do you think the Datacenter demand that you're seeing is -- I know it's probably only -- you're only able to answer qualitatively. But to what extent do you think the Datacenter is secondary versus you have a new platform and there's just kind of a platform-driven demand? I guess first question is around Gaming. And I was hoping you could kind of walk through how you're thinking about seasonality here in calendar '17, particularly as Pascal launch calendarizes and you get both the launch coming, I presume, early '18. I would love to hear your thoughts on how we should think about the trajectory of that business. Very helpful. I guess as my follow-up, on the inventory side, that grew, I think, 3% sequentially. Can you walk through the moving parts there? What's driving that? And is foundry diversification part of that? Jen-Hsun, can you maybe talk a little bit about the breadth of your customer base in Datacenter relative to maybe 12 months ago? Are you seeing kind of the same customer group buying more GPUs? Or is it the growth in your business more a function of the broadening of your customer base? Okay. And then as my follow-up, I had a question for Colette. 3 months ago, I think you went out of your way to guide Datacenter up sequentially. And for the July quarter, ex the Intel business going away, you're guiding revenue up 3% sequentially. Can you maybe provide some additional color for the individual segments? Jen-Hsun, can you talk about the adoption of GPU in the cloud? At the CES earlier this year, you guys announced GeForce. Now curious how the adoption of GeForce is going. Great. As a follow-up, with your win and success in Nintendo Switch, does that open up the console market with other console makers? Is that a business that is of interest to you? Jen-Hsun, can you give us like a State of the Union on process node and technology road maps you guys see? And tell me a pretty nice exposition of where they are in terms of their transistors and so on? So what's your comfort level as you see your process technology and your road maps for new GPUs? I just have one actually on the gaming side. I remember at CES, you had mentioned kind of a leasing model, almost effectively kind of target the low-end consumers of gaming products. So just wondering if that'll be some sort of catalyst the back half. So how do we think about gaming working out in terms of both the leasing model and the year-over-year comparisons getting a bit difficult? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q3 2018,1957,5603,923,"Thank you. Good afternoon, everyone and welcome to NVIDIA's conference call for the third quarter of fiscal 2018. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until November 16, 2017. The webcast will be available for replay up until next quarter's conference call to discuss Q4 and full year fiscal 2018 financial results. The contents of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 9, 2017, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO Commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. We had an excellent quarter with record revenue in each of our 4 market platforms. And every measure of profit hit record levels, reflecting the leverage of our model. Data center revenue of $501 million more than doubled from a year ago and the strong adoption of our Volta platform and early traction with our inferencing portfolio. Q3 revenue reached $2.64 billion, up 32% from a year earlier, up 18% sequentially and well above our outlook of $2.35 billion. From a reporting segment perspective, GPU revenue grew 31% from last year to $2.22 billion. Tegra processor revenue rose 74% to $419 million. Let's start with our gaming business. Gaming revenue was $1.56 billion, up 25% year-on-year and up 32% sequentially. We saw robust demand across all regions and form factors. Our Pascal-based GPUs remained the platform of choice for gamers as evidenced by our strong demand for GeForce GTX 10-Series products. We introduced the GeForce GTX 1070 Ti, which became available last week. It complements our strong holiday lineup ranging from the entry-level GTX 1050 to our flagship GTX 1080 Ti. A wave of great titles is arriving for the holidays, driving enthusiasm in the market. We collaborated with Activision to bring Destiny 2 to the PC earlier in the month. PlayerUnknown's Battlegrounds, popularly known as PUBG, continues to be one of the year's most successful titles. We are closely aligned with PUBG to ensure that GeForce is the best way to play the game, including bringing shadow play highlights to its 20 million players. Last weekend, Call of Duty: WWII had a strong debut, and Star Wars Battlefront II will be out [soon]. eSports remains one of the most important secular growth drivers in the gaming market with a fan base that now exceeds 350 million. Last weekend, the League of Legends World Championship was held in Beijing's National Stadium, the Bird's Nest, where the 2008 Olympics games were held. More than 40,000 fans attended live, and online viewers were set to break last year's record of 43 million following in 18 languages. GPU sales also benefited from continued cryptocurrency mining. We met some of this demand with a dedicated board in our OEM business and a portion with GeForce GTX boards, though it's difficult to quantify. We remain nimble in our approach to the cryptocurrency market. It is volatile, does not and will not distract us from focusing on our core gaming market. Lastly, Nintendo Switch console continues to gain momentum since launching in March and also contributed to growth. Moving to data center. Our data center business had an outstanding quarter. Revenue of $501 million more than doubled from last year and rose 20% on the quarter amid strong traction of the new Volta architecture. Shipments of the Tesla V100 GPU began in Q2 and ramped significantly in Q3 driven primarily by demand from cloud service providers and high-performance computing. As we have noted before, Volta delivers 10x the deep learning performance of our Pascal architecture, which has been introduced just a year earlier, far outpacing Moore's Law. The V100 is being broadly adopted with every major server OEM and cloud provider. In China, Alibaba, Baidu and Tencent announced that they are incorporating V100 in their data centers and cloud server service infrastructures. In the U.S., Amazon Web Services announced that V100 inferences are now available in 4 of its regions. Oracle Cloud has just added Tesla P100 GPUs to its infrastructure offerings and plans to expand to the V100 GPUs. We expect support from V100 from other major cloud providers as well. In addition, all major server OEMs announced support for the V100, Dell EMC, Hewlett Packard Enterprise, IBM and Supermicro are incorporating it in servers. And China's top server OEMs, Huawei, Inspur and Lenovo have adopted our HGX server architecture to build a new generation of accelerated data centers with V100 GPUs. Our new offerings for the AI inference market are also gaining momentum. The recently launched TensorRT 3 programmable inference acceleration platform opens a new market opportunity for us, improving the performance and reducing the cost of AI inferencing in order -- by orders of magnitude compared with CPUs. It supports every major deep learning framework, every network architecture and any level of network complexity. More than 1,200 companies are already using our inference platform including Amazon, Microsoft, Facebook, Google, Alibaba, Baidu, JD.com, iFLYTEK, Hikvision and Tencent. During the quarter, we announced that the NVIDIA GPU Cloud container registry, or NGC, is now available through Amazon's cloud and will be supported soon by other cloud platforms. NGC helps developers get started with deep learning development through no-cost access to a comprehensive, easy-to-use, fully optimized deep learning software stack. It enables instant access to the most widely used GPU-accelerated frameworks. We also continue to see robust growth in our HPC business. Next-generation supercomputers, such as the U.S. Department of Energy's Sierra and Summit systems expected to come online next year, leverage Volta's industry-leading performance, and our pipeline is strong. The past weeks have been exceptionally busy for us. We have hosted 5 major GPU Technology Conferences in Beijing, Munich, Taipei, Tel Aviv and Washington with another next month in Tokyo. In a strong indication of the growing importance of GPU-accelerated computing, more than 22,000 developers, data scientists and others will come this year to our GTCs, including the main event in Silicon Valley. That's up 10x in just 5 years. Other key metrics show similar gains. Over the same period, the number of NVIDIA GPU developers has grown 15x to 645,000, and the number of CUDA downloads this year are up 5x to 1.8 million. Moving to professional visualization. Third quarter revenue grew to $239 million, up 15% from a year ago and up 2% sequentially driven by demand for high-end real-time rendering, simulation and more powerful mobile workstations. The defense and automotive industries grew strongly as the demand for professional VR solutions driven by Quadro P5000 and P6000 GPUs. Among key customers, Audi and BMW are deploying VR in auto showrooms. And the U.S. Army, Navy and Homeland Security are using VR for mission training. Last month, we announced early access to NVIDIA Holodeck, the intelligent VR collaboration platform. Holodeck enables designers, developers and their customers to come together virtually from anywhere in the world in a highly realistic, collaborative and physically simulated environment. Future updates will address the growing demand for the development of deep learning techniques in virtual environments. In automotive, revenue grew to $144 million, up 13% year-over-year and up slightly from last quarter. Among key developments this quarter, we announced DRIVE PX Pegasus, the world's first AI computer for enabling Level 5 driverless vehicles. Pegasus will deliver over 320 trillion operations per second, more than 10x its predecessor. It's powered by 4 high-performance AI processors in a supercomputer that is the size of a license plate. NVIDIA DRIVE is being used by over 25 companies to develop fully autonomous robotaxis, and DRIVE PX Pegasus will become the path to production. It is designed for ASIL D certification, the industry's highest safety level and will be available in the second half of 2018. We also introduced the DRIVE IX SDK for delivering intelligent experiences inside the vehicle. DRIVE IX provides a platform for car companies to create and always engage AI co-pilot. It uses deep learning networks to track head movement and gaze, and it will have a conversation with the driver using advanced speech recognition, lipreading and natural language understanding. We believe this will set the standard for the next generation of infotainment systems, a market that is just beginning to develop. Finally, we announced that DHL, the world's largest mail and package delivery service, and ZF, one of the world's leading automotive suppliers, will deploy a test fleet of autonomous delivery trucks next year using the NVIDIA DRIVE PX platform. DHL will outfit electric light trucks with the ZF ProAI self-driving system based on our technology. Now turning to the rest of the income statement. Q3 GAAP gross margins was 59.5% and non-GAAP was 59.7%, both up sequentially and year-over-year, reflecting continued growth in value-added platforms. GAAP operating expenses were $674 million, and non-GAAP operating expenses were $570 million, consistent with our outlook and up 19% year-on-year. Investing in our key market opportunities is essential to our future, including gaming, AI and self-driving cars. GAAP operating income was a record $895 million, up 40% from a year ago. Non-GAAP operating income was $1.01 billion, up 42% from a year ago. GAAP net income was a record $838 million, and EPS was $1.33, up 55% and 60%, respectively, from a year earlier. Non-GAAP net income was $833 million, and EPS was $1.33, up 46% and 41%, respectively from a year earlier, reflecting revenue strength as well as gross margin and operating margin expansion. We have returned $1.16 billion to shareholders so far this fiscal year through a combination of quarterly dividends and share repurchases. We have announced an increase to our quarterly dividend of $0.01 to an annualized $0.60 effective with our Q4 fiscal year '18 dividend. We are also pleased to announce that we intend to return another $1.25 billion to shareholders for fiscal 2019 through quarterly dividends and share repurchases. Our quarterly cash flow from operations reached record levels, surpassing $1 billion for the first time to $1.16 billion. Now turning to the outlook for the fourth quarter of fiscal 2018. We expect revenue to be $2.65 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 59.7% and 60%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $722 million and $600 million, respectively. GAAP and non-GAAP OI&E are both expected to be nominal. GAAP and non-GAAP tax rates are both expected to be 17.5%, plus or minus 1%, excluding discrete items. Further financial details are included in the CFO Commentary and other information available on our website. We will now open the call for questions. (Operator Instructions) Operator, we will -- would you please pool for questions? Thank you.","Yes, as you know, we started ramping very strongly Volta this last quarter, and we started the ramp the quarter before. And since then, every major cloud provider, from Amazon, Microsoft, Google to Baidu, Alibaba, Tencent and even recently, Oracle, has announced support for Volta and we'll be providing Volta for their internal use of deep learning as well as external public cloud services. We also announced that every major server computer maker in the world has now supported Volta and in the process of taking Volta out to market. HP and Dell and IBM and Cisco and Huawei in China, Inspur in China, Lenovo, have all announced that they will be building servers -- families of servers around the Volta GPU. And so I think we -- this ramp is just the first part of supporting the build out of GPU-accelerated servers from our company for data centers all over the world as well as cloud service providers all over the world. The applications for these GPU servers has now grown to many markets. I've spoken about the primary segments of our Tesla GPUs. There are 5 of them that I talk about regularly. The first one is high-performance computing where the market is $11 billion or so. It is one of the faster-growing parts of the IT industry because more and more people are using high-performance computing for doing their product development or looking for insights or predicting the market or whatever it is. And today, we represent about 15% of the world's top 500 supercomputers. And I've repeatedly said and I believe this completely and I think it's becoming increasingly true that every single supercomputer in the future will be accelerated somehow. So this is a fairly significant growth opportunity for us. The second is deep learning training, which is very, very much like high-performance computing. And you need to do computing at a very large scale. You're performing trillions and trillions of iterations. The models are getting larger and larger. Every single year, the amount of data that we're training with it is increasing. And the difference between a computing platform that's fast versus not could mean the difference between building a $20 million data center or high-performance computing servers for training to $200 million. And so the money that we save and the capability we provide is really, the value is incredible. The third segment, and this is the segment that you just mentioned, has to do with inference, which is when you're done with developing this network, you have to put it down into the hyperscale data centers to support the billions and billions of queries that consumers make to the Internet every day. And this is a brand-new market for us. 100% of the world's inference is done on CPUs today. We announced very recently, this last quarter in fact, that TensorRT 3 inference acceleration platform and in combination with our Tensor Core GPU instruction set architecture, we're able to speed up networks by a factor of 100. Now the way to think about that is imagine whatever amount of workload that you've got, if you could speed up using our platform by a factor of 100, how much you can save. The other way to think about that is because the amount of -- the networks are getting larger and larger, and they're so complex now. And we know that every network on the planet will run on our architecture because they were trained on our architecture today. And so whether it's CNNs or RNNs or GANs or autoencoders or all of the variations of those, irrespective of the precision that you need to support, the size of the network, we have the ability to support them. And so you could either scale out your hyperscale data center to support more traffic or you can reduce your cost tremendously or simultaneously, both. The fourth segment of our data center is providing all of that capability, what I just mentioned, whether it's HPC, training or inference and turning it inside out and making it available in the public cloud. There are thousands of start-ups now that are in -- are started because of AI. Everybody recognizes the importance of this new computing model. And as a result of this new tool, this new capability, all these unsolvable problems in the past are now interestingly solvable. And so you can see start-ups cropping up all over the west, all over the east, and there's just -- there are thousands of them. And these companies don't either -- would rather not use their scarce financial resources to go build high-performance computing centers, or they don't have the skill to be able to build out a high-performance platform the way these Internet companies can. And so these cloud providers, cloud platforms are just a fantastic resource for them because they could rent it by the hour. We created in conjunction with that, and I mentioned all the cloud service providers have taken it to market, in conjunction with that, we created a registry in the cloud that containerizes these really complicated software stacks. Every one of these soft frameworks with the different versions of our GPUs and different acceleration layers and different optimization techniques, we've containerized all of that for every single version and every single type of framework in the marketplace. And we put that up in the registry -- cloud registry called the NVIDIA GPU Cloud. And so all you had to do was download that into the cloud service provider that we've got certified and tested for, and with just one click, you're doing deep learning. And then the last -- and so that's the cloud service providers. If you -- the way to guess that -- estimate that is there are obviously tens of billions of dollars being invested in these AI start-ups. And some large proportion of their investment fund raise will ultimately have to go towards high-performance computing, whether they build it themselves or they rent it in the cloud. And so I think that's a multibillion dollar opportunity for us. And then lastly, this is probably the largest of all the opportunities, which is the vertical industries. Whether it's automotive companies that are developing their supercomputers to get ready for self-driving cars or health care companies that are now taking advantage of artificial intelligence to do better diagnostics of -- diagnosis of disease, to manufacturing companies to -- for in-line inspection, to robotics, large logistics companies. Colette mentioned earlier DHL. But the way to think about that is all of these planning -- all of these companies doing planning to deliver products to you through this large network of delivery systems, it is the world's largest planning problem. And whether it's Uber or Didi or Lyft or Amazon or DHL or UPS or FedEx, they all have high-performance computing problems that are now moving to deep learning. And so those are really exciting opportunities for us. And so the last one is just vertical industries. I mean, all of these segments, we're now in a position to start addressing because we've put our GPUs in the cloud, all of our OEMs are in the process of taking these platforms out to market and we have the ability now to address high-performance computing and deep learning training as well as inference using one common platform. And so I think the -- we've been steadfast with the excitement of accelerated computing for data centers, and I think this is just the beginning of it all. Let's see. There's -- I'll answer the last one first and then work towards the first one. I think the guidance that we provided, we feel comfortable with. But if you think about Volta, it is just in the beginning of the ramp, and it's going to ramp into the market opportunities I talked about. And so my hope is that we continue to grow, and there's every evidence that the markets that we serve, that we're addressing with Volta is -- are very large markets. And so there's a lot of reasons to be hopeful about the future growth opportunities for Volta. We've primed the pump. So cloud service providers are either announce the availability of Volta or they announced the soon availability of Volta. They're all racing to get Volta to their cloud because customers are clamoring for it. The OEMs are -- we've primed the pump with the OEMs, and some of them are sampling now and some of them are racing to get Volta to production in the marketplace. And so I think the foundation, the demand is there. The urgent need for accelerated computing is there because Moore's Law is not scaling anymore, and then we've primed the pump. So the demand is there. There's a need -- the need is there, and the foundations for getting Volta to market is primed. With respect to gaming, what drives our gaming business? Remember, our gaming business is sold one at a time to millions and millions of people. And what drives our gaming business is several things. As you know, eSports is incredibly, incredibly vibrant, and what drives -- the reason why eSports is so unique is because people want to win and having better gear helps. The latency that they expect is incredibly low, and performance drives down latency and they want to be able to react as fast as they can. People want to win, and they want to make sure that the gear that they use is not the reason why they didn't win. The second growth driver for us is content, the quality of content. And boy, if you look at Call of Duty or Destiny 2 or PUBG, the content just looks amazing. The AAA content looks amazing. And one of the things that's really unique about video games is that in order to enjoy the content and the fidelity of the content, the quality of the production value at its fullest, you need the best gear. It's very different than streaming video, it's very different than watching movies where streaming videos, it is what it is. But for video games, of course, it's not. And so when AAA titles comes out in the later part of the year, it helps to drive platform adoption. And then lastly, increasingly, social is becoming a huge part of the growth dynamics of gaming. People are -- they recognize how beautiful these video games are, and so they want to share their brightest moments with people. They want to share the levels they discover. They want to take pictures of the amazing graphics that's inside. And it is one of the primary drivers, the leading driver, in fact, of YouTube and people watching other people play video games, these broadcasters. And now with our Ansel, the world's first in-game virtual reality and surround and digital camera, we have the ability to take pictures and share that with people. And so I think all of these different drivers are helping our gaming business, and I'm optimistic about Q4. It looks like it's going to be a great quarter. Yes, thanks a lot, C.J. Well, everything that we build is complicated. Volta is the single largest processor that humanity has ever made, 21 billion transistors, 3D packaging, the fastest memories on the planet and all of that in a couple hundred watts, which basically says it's the most energy-efficient form of computing that the world has ever known. And one single Volta replaces hundreds of CPUs. And so it's energy-efficient. It saves an enormous amount of money. And it gets this job done really, really fast, which is one of the reasons why GPU-accelerated computing is so popular now. With respect to the outlook for our architecture, as you know, we are a one-architecture company, and it's so vitally important. And the reason for that is because there are so much software and so much tools created on top of this one architecture. On the inference side -- on the training side, we have a whole stack of software and optimizing compilers and numerics libraries that are completely optimized for one architecture called CUDA. On the inference side, the optimizing compilers that takes these large, huge computational graphs that come out of all of these frameworks, and these computational graphs are getting larger and larger and their numerical precision differs from one type of network to another -- from one type of application to another. Your numerical precision requirements for a self-driving car, where lives are at stake, to detecting where -- counting the number of people crossing the street, counting something versus trying to track -- detect and track something very subtle in all kinds of weather conditions is a very, very different problem. And so numeric -- the type of networks are changing all the time. They're getting larger all the time. The numerical precision is different for different applications. And we have different computing -- computer performance levels as well as energy availability levels that these inference compilers are likely to be some of the most complex software in the world. And so the fact that we have one singular architecture to optimize for, whether it's HPC for numeric -- molecular dynamics and computational chemistry and biology and astrophysics, all the way to training to inference gives us just enormous leverage. And that's the reason why NVIDIA could be an 11,000-people company and arguably, performing at a level that is 10x that. And the reason for that is because we have one singular architecture that's -- that is accruing benefits over time instead of 3, 4, 5 different architectures where your software organization is broken up into all these different small subcritical mass pieces. And so it's a huge advantage for us, and it's a huge advantage for the industry. So people who support CUDA knows that the next-generation architecture will just get a benefit and go for the ride that technology advancement provides them and affords them. Okay. So I think it's an advantage that is growing exponentially, frankly, and I'm excited about it. Congratulations on the strong results and the consistent execution. Jensen, in the last few months, we have seen a lot of announcements from Intel, from Xilinx and others describing other approaches to the AI market. My question is how does a customer make that decision whether to use a GPU or an FPGA or an ASIC, right? What is -- what can remain your competitive differentiator over the longer term? And does your position in the training market also then maybe give you a leg up when they consider solution for the inference part of the problem? Yes, thank you, Vivek. So first of all, we have one architecture and people know that our commitment to our GPUs, our commitment to CUDA, our commitment to all of the software stacks that run on top of our GPUs, every single one of the 500 applications, every numerical solver, every CUDA compiler, every tool chain across every single operating system in every single computing platform, we are completely dedicated to it. We support the software for as long as we shall live, and as a result of that, the benefits to their investment in CUDA just continues to accrue. I -- you have no idea how many people send me notes about how they literally take out their old GPU, put in a new GPU and without lifting a finger, things got 2x, 3x, 4x faster than what they were doing before, incredible value to customers. The fact that we are singularly focused and completely dedicated to this one architecture and in an unwavering way allows everybody to trust us and know that we will support it for as long as we shall live. And that is the benefit of an architectural strategy. When you have 4 or 5 different architectures to support, that you offer to your customers and you ask them to pick the one that they like the best, you're essentially saying that you're not sure which one is the best. And we all know that nobody's going to be able to support 5 architectures forever. And as a result, something has to give, and it would be really unfortunate for a customer to have chosen the wrong one. And if there's 5 architectures, surely, over time, 80% of them will be wrong. And so I think that our advantage is that we're singularly focused. With respect to FPGAs, I think FPGAs have their place, and we use FPGAs here in NVIDIA to prototype things and -- but FPGA is a chip design. It's able to be a chip for -- it's incredibly good at being a flexible substrate to be any chip, and so that's its advantage. Our advantage is that we have a programming environment, and writing software is a lot easier than designing chips. And if it's within the domain that we focus on, like, for example, we're not focused on network packet processing, but we are very focused on deep learning. We're very focused on high performance and parallel numerics analysis. If we're focused on those domains, our platform is really quite unbeatable. And so that's how you think through that. I hope that was helpful. So in our results, in the OEM results, our specific crypto [boards] equated to about $70 million of revenue, which is the comparable to the $150 million that we saw last quarter. Yes, longer term, Atif -- well, first of all, thank you for that. The -- longer term, the way to think about that is, is crypto is small for us but not 0. And I believe that crypto will be around for some time, kind of like today. There will be new currencies emerging. Existing currencies will grow in value. The interest in mining these new emerging currency crypto algorithms that emerge are going to continue to happen. And so I think for some time, we're going to see that crypto will be a small but not 0, small but not 0 part of our business. The -- when you think about crypto in the context of our company overall, the thing to remember is that we're the largest GPU computing company in the world. And our overall GPU business is really sizable and we have multiple segments. And there's data center and I've already talked about the 5 different segments within data center. There's ProVis and even that has multiple segments within it. Whether it's rendering or computer-aided design or broadcast in a workstation, in a laptop or in a data center, the architectures are rather different. And of course, you know that we have high-performance computing. You know that we have autonomous machine business, self-driving cars and robotics. And you know, of course, that we have gaming. And so these different segments are all quite large and growing. And so my sense is that as -- although crypto will be here to stay, it will remain small but not 0. Just following up on that last question. You mentioned that some of the crypto market had moved to traditional gaming. What drives that? Is there a lack of availability of the specialized crypto product? Or is it just that there's a preference being driven for the gaming-oriented crypto solutions? Yes, Joe, I appreciate you asking that. Here's the reason why. So what happens is, is when a crypto -- when a currency -- digital currency market becomes very large, it entices somebody to build a custom ASIC for it. And of course, Bitcoin is the perfect example of that. Bitcoin is incredibly easy to design as a specialized chip form. But then what happens is a couple of different players starts to monopolize the marketplace and as a result, it chases everybody out of the mining market and it encourages a new currency to evolve -- to emerge. And the new currency, the only way to get people to mine it is if it's hard to mine, it's hard to mine, okay, you got to put some effort into it. However, you want a lot of people to try to mine it. And so therefore, the platform that is perfect for it, the ideal platform for digital -- new emerging digital currencies turns out to be a CUDA GPU. And the reason for that is because there are several hundred million NVIDIA GPUs in the marketplace. If you want to create a new cryptocurrency algorithm, optimizing for our GPUs is really quite ideal. It's hard to do. It's hard to do, therefore, you need a lot of computation to do it. And yet there's enough GPUs in the marketplace, it's such an open platform that the ability for somebody to get in and start mining is very low barriers to entry. And so it's the cycles of these digital currencies, and that's the reason why I say that digital currency crypto usage of GPUs, crypto usage of GPUs will be small but not 0 for some time. And it's small because when it gets big, somebody will go and build a custom ASIC. But if somebody builds a custom ASIC, there will be a new emerging cryptocurrency, so ebbs and flows. Jensen, congratulations on data center annualizing at $2 billion. It's a huge milestone. I wanted to follow up with a question on some of your comments regarding data center partners because as I look back over the last 5 years, I just don't see any precedent for the momentum that you have in the marketplace right now between your server partners, white box partners, hyperscale partners that are deploying it, hosted, et cetera. And so my question is relative to the doubling that we've seen year-on-year in each of the last 2 years, what does that partner expansion mean for data center's growth? And then if I could sneak one more in. 2 new products just announced in the gaming platform, 1070 Ti and a Collector's Edition on TITAN Xp. What do those mean for the gaming platform? Yes, Craig, thanks a lot. Let's see. We have never created a product that is as broadly supported by the industries and has grown 9 consecutive quarters, it has doubled year-over-year and with partnerships of the scale that we're looking at. We have just never created a product like that before, and I think the reason for that is several folds. The first is that it is true that CPU scaling has come to an end. That's just laws of physics. The end of Moore's Law is just laws of physics. And yet the world for software development and the world -- the problems that computing can help solve is growing faster than any time before. Nobody's ever seen a large-scale planning problem like Amazon before. Nobody's ever seen a large-scale planning problem like Didi before. The number of millions of taxi rides per week is just staggering. And so nobody's ever seen large problems like these before, large-scale problems like these before. And so high-performance computing and accelerated computing using GPUs has become recognized as the path forward. And so I think that that's at the highest level of the most important parameter. Second is artificial intelligence and its emergence and applications to solving problems that we historically thought were unsolvable. Solving the unsolvable problems is a real realization. I mean, this is happening across just about every industry we know, whether it's Internet service providers to health care to manufacturing to transportation and logistics, you just name it, financial services. And so I think artificial intelligence is a real tool, deep learning is a real tool that can help solve some of the world's unsolvable problems. And I think that our dedication to high-performance computing and this one singular architecture, our 7-year head start, if you will, in deep learning and our early recognition of the importance of this new computing approach, both the timing of it, the fact that it was naturally a perfect fit for the skills that we have and then the incredibly -- the incredible effectiveness of this approach, I think, has really created the perfect conditions for our architecture. And so I think -- I really appreciate you noticing that, but this is definitely the most successful product line in the history of our company. I appreciate that, Chris. So the way to think about that is as you know, we've really, really reduced our emphasis on infotainment even though that's the primary part of our revenues, so that we could take, literally, hundreds of engineers and including the processors that we're building now, a couple of 2,000, 3,000 engineers, working on our autonomous machine and artificial intelligence platform for this marketplace to take advantage of the position we have and to go after this amazing revolution that's about to happen. I happen to believe that everything that moves will be autonomous someday, and it could be a bus, a truck, a shuttle, a car. Everything that moves will be autonomous someday. It could be a delivery vehicle. It could be little robots that are moving around warehouses. It could be delivering a pizza to you. And we felt that those -- this was such an incredibly, incredibly great challenge and such a great computing problem that we decided to dedicate ourselves to it. Over the next several years, and if you look at our DRIVE PX platform today, there's over 200 companies that are working on it. 125 start-ups are working on it. And these companies are mapping companies. They're Tier 1s. They're OEMs. They're shuttle companies, car companies, trucking companies, taxi companies. And this last quarter, we announced an extension of our DRIVE PX platform to include DRIVE PX Pegasus, which is now the world's first auto-grade, full ASIL D platform for robotaxis. And so I think our position is really excellent, and the investment has proven to be one of the best ever. And so I think in terms of revenues, my expectation is that this coming year, we'll enjoy revenues as a result of the supercomputers that customers will have to buy for training their networks, for simulating the -- all these autonomous vehicles driving and developing their self-driving cars. And we'll see fairly large quantities of development systems being sold this coming year. The year after that, I think, is the year when you're going to see the robotaxis ramping, and our economics in every robotaxi is several thousand dollars. And then starting, I would say, late 2020 to 2021, you're going to start to see the first fully automatic autonomous cars, what people call Level 4 cars, starting to hit the road. And so that's kind of how I see it. Just next year is simulation environments, development systems, supercomputers, and then the year after that is robotaxis and then a year or 2 after that will be all the self-driving cars. Okay. Thanks, Matt, for the question. Yes, we've been on a steady stream of increasing the gross margins over the years. But this is the evolution of the entire model, the model of the value-added platforms that we sell and inclusive of the entire ecosystem of work that we do, the software that we enable in so many of these platforms that we bring to market. Data center is one of them. Our ProVis, another one and if you think about all of our work that we have in terms of gaming and that overall expansion of the ecosystem. So this has been continuing to increase our gross margin. Mix is more of a statement in terms of each quarter we have a different mix in terms of our products. So some of them have a little bit of seasonality. And depending on when some of those platforms come to market, we can have a mix change within some of those subsets. It's still going to be our focus as we go forward in terms of growing gross margins as best as we can, you can see in terms of our guidance into Q4, which we feel comfortable with that guidance that we will increase it as well. Yes, with respect to yield enhancement, the way to think about that is we do it in several ways. The first thing is I'm just incredibly proud of the technology group that we have in VLSI, and they get us ready for these brand new nodes, whether it's in the process readiness with all the circuit readiness, the packaging, the memory readiness. The readiness is so incredible -- incredibly important for us because these processors that we're creating are really, really hard. They're the largest things in the world. And so we get one shot at it. And so the team does everything they can to essentially prepare us. And by the time that we tape-out a product for real, we know for certain that we can build it. And so the technology team in our company is just world-class, absolutely world-class. There's nothing like it. Then once we go into production, we have the benefit of ramping up the products. And as yields improve, we'll surely benefit from the cost. But that's not really where the focus is. I mean, in the final analysis, the real focus for us is continue to improve the software stack on top of our processors. And the reason for that is each one of our processors carry with it an enormous amount of memory and systems and networking and the whole data center. Most of our data center products, if we can improve the throughput of a data center by another 50% or, in our case, often times we'll improve something from 2x to 4x, the way to think about that is that billion-dollar data center just improved its productivity by a factor of 2. And all of the software work that we do on top of CUDA and the incredible work that we do with optimizing compilers and graph analytics, all of that stuff then all of a sudden translates to value to our customers, not measured by dollars, but measured by hundreds of millions of dollars. And that's really the leverage of accelerated computing. Yes, thanks, Hans. Yes, listen, there's a lot of news out there. I guess some of the things I take away, first of all, Raja leaving AMD is a great loss for AMD. And it's a recognition by Intel probably that the GPU is just incredibly, incredibly important now. And the modern GPU is not a graphics accelerator. The modern GPU, we just left the word G in there -- the letter G in there. But these processors are domain-specific parallel accelerators, and they're enormously complex. They're the most complex processors built by anybody on the planet today. And that's the reason why IBM uses our processors for the world's largest supercomputers. That's the reason why every single cloud, every single -- every major cloud, every major server maker in the world has adopted NVIDIA GPUs: It's just incredibly hard to do. The amount of software engineering that goes on top of it is significant as well. And so if you look at the way we do things, we plan a road map about 5 years out. It takes about 3 years to build a new generation, and we build multiple GPUs at the same time. And on top of that, there are some 5,000 engineers working on systems software and numerics libraries and solvers and compilers and graph analytics and cloud platforms and virtualization stacks in order to make this computing architecture useful to all of the people that we serve. And so when you think about it from that perspective, it's just an enormous undertaking; arguably, the most significant undertaking of any processor in the world today. And that's the reason why we're able to speed up applications by a factor of 100. You don't walk in and have a new widget and a few transistors and all of a sudden speed up applications by a factor of 100 or 50 or 20. That's just something that's inconceivable unless you do the type of innovation that we do. And then lastly, with respect to the chip that they built together, I think it goes without saying now that the energy efficiency of Pascal GeForce and the Max-Q design technology and all of the software that we created has really set a new design point for the industry. It is now possible to build a state-of-the-art gaming notebook with the most leading-edge GeForce processors and be able to deliver gaming experiences that are many times greater than a console in 4K and have that be in a laptop that's 18 millimeters thin. The combination of Pascal and Max-Q has really raised the bar, and I think that that's really the essence of it. We had another great quarter. Gaming is one of the fastest-growing entertainment industries, and we are well positioned for the holidays. AI is becoming increasingly widespread in many industries throughout the world, and we're hoping to lead the way with all major cloud providers and computer makers moving to deploy Volta. And we're building the future of autonomous driving. We expect robotaxis, using our technology, to hit the road in just a couple of years. We look forward to seeing many of you at SC17 next week, and thank you for joining us.","Jensen, 3 months ago you described the July quarter as a transition quarter for your data center business. And clearly, you guys have ramped very well into October. But if you can talk a little bit about the outlook for the next couple of quarters in data center and particularly on the inferencing side. I know you guys are really excited about that opportunity. So if you can share customer feedback and what your expectations are into the next year in inferencing, that would be great. I had a question on your gaming seasonality into Q4. It's usually up a bit. I was wondering, do you see any, I guess, drivers that would drive a lack of normal seasonal trends given how strong it's been sequentially and year-over-year? And I guess as a related question, do you see your Volta volumes in Q4 exceeding Q3? I was hoping to sneak in a near-term and a longer-term question. On the near term, you talked about the health on demand side for Volta. Curious if you're seeing any sort of restrictions on the supply side, whether it's wafers or access to high-bandwidth memory, et cetera. And then the longer-term question really revolves around CUDA, and you've talked about that as being a sustainable competitive advantage for you guys entering the year. And now that we've moved beyond HPC and hyperscale training to more into inference and GPU as a service and you've hosted GTC around the world, curious if you could extrapolate on how you're seeing that advantage and how you've seen it evolve over the year and how you're thinking about CUDA as the AI standard. Colette, on the last call you mentioned crypto was $150 million in the OEM line in the July quarter. Can you quantify how much crypto was in the October quarter and expectations in the January quarter directionally? And just longer term, why should we think that crypto won't impact the gaming demand in the future? If you can just talk about the steps NVIDIA has taken with respect to having a different mode and all that. I have a question on the automotive market and the outlook there. And interestingly, with the other segments growing as quickly as they are, auto is becoming a smaller percentage of revenue now. And certainly, the design traction seems very positive. Can you talk about the ramp in terms of when the auto revenue, when we could see that as getting back to a similar percentage of revenue? Is that growing more quickly? Do you think that is likely to happen over the next year with some of these design wins coming out? Or is that something we should -- we'll be waiting for over several years? I have, I guess, a 2-part question on gross margin. Colette, I remember, I don't know, maybe 3 years ago, 3.5 years ago at an analyst day, you guys were talking about gross margins in the mid-50s and that was inclusive of the Intel payment. And now you're hitting numbers at 60% excluding that. I want -- if you could talk a little bit about how mix of the data center business and some others drives gross margin going forward. And maybe, Jensen, you could talk a little bit about -- you mentioned Volta being such a huge chip in terms of transistor count. How you're thinking about taking costs out of that product as you ramp it into gaming next year and the effects on gross margin. Jensen, can you comment on some of the issues this week regarding Intel and their renewed interest in getting into the graphics space and their relationship at the chip level with AMD? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q2 2019,2539,4694,1014,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the Second Quarter of Fiscal 2019. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until August 23, 2018. The webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2019. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. Forward-looking statements are made as of today, August 16, 2018, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. This is a big week for NVIDIA. We just announced the biggest leap in GPU architecture in over a decade. We can't wait to tell you more about it, but first, let's talk about the quarter. We had another strong quarter, led by data center and gaming. Q2 revenue reached $3.12 billion, up 40% from a year earlier. Each market platform -- gaming, data center, pro visualization and automotive hit record levels -- with strong growth both sequentially and year-on-year. These platforms collectively grew more than 50% year-on-year. Our revenue outlook had anticipated cryptocurrency-specific products declining to approximately $100 million while actual crypto-specific product revenue was $18 million, and we now expect a negligible contribution going forward. Gross margins grew nearly 500 basis points year-on-year while both GAAP and non-GAAP net income exceeded $1 billion for the third consecutive quarter. Profit nearly doubled. From a reporting segment perspective, GPU revenue grew 40% from last year to $2.66 billion. Tegra processor revenue grew 40% to $467 million. Let's start with our gaming business. Revenue of $1.8 billion was up 52% year-on-year and up 5% sequentially. Growth was driven by all segments of the business with desktop, notebook and gaming consoles up all strong double-digit percentages year-on-year. Notebooks were a standout this quarter, with strong demands for thin and light form factors based on our Max-Q technology. Max-Q enables gaming PC OEMs to pack a high-performance GPU into a slim notebook that is just 20 millimeters thick or less. All major notebook OEMs and ODMs have adopted Max-Q for their top-of-the-line gaming notebooks, just in time for back-to-school. And we expect to see 26 models based on Max-Q in stores for the holidays. The gaming industry remains vibrant. The eSports audience now approaches 400 million, up 18% over the past year. The unprecedented success of Fortnite and PUBG has popularized this new battle royale genre and expanded the gaming market. In fact, the battle royale mode is coming to games like the much-anticipated Battlefield V. We are thrilled to partner with EA to make GeForce the best PC-gaming platform for the release of Battlefield V in October. We have also partnered with Square Enix to make GeForce the best platform for its upcoming Shadow of the Tomb Raider. Monster Hunter: World arrived on PCs earlier this month and it was an instant hit. And many more titles are lined up for what promises to be a big holiday season. It's not just new titles that are building anticipation. The gaming community is excited over the Turing architecture announced earlier this week at SIGGRAPH. Turing is our most important innovation since the invention of the CUDA GPU over a decade ago. The architecture includes new dedicated ray-tracing processors or RT cores and new Tensor Cores for AI inferencing, which together will make real-time ray-tracing possible for the first time. We will enable the cinematic quality gaming, amazing new effects powered by neural networks and fluid interactivity on highly complex models. Turing will reset the look of video games and open up the 250 billion visual effects industries to GPUs. Turing is the result of more than 10,000 engineering years to -- of effort. It's -- delivers up to 6x performance increase over Pascal for ray-traced graphics and up to 10x boost for peak inference flops. This new architecture will be the foundation of a new portfolio of products across our platforms going forward. Moving to data center. We had another strong quarter with revenue of $760 million, accelerating to 83% year-on-year growth and up 8% sequentially. This performance was driven by hyperscale demand as Internet services used daily by billions of people increasingly leverage AI. Our GPUs power real-time services such as search, voice recognition, voice synthesis, translation, recommender engines, fraud detection and retail applications. We also saw a growing adoption of our AI and high-performance computing solutions by vertical industries, representing one of the most fastest areas of growth in our business. Companies in sectors ranging from oil and gas to financial services through transportation are harnessing the power of AI and our accelerating computing platform to turn data into actionable insights. Our flagship Tensor Core GPU, the Tesla V100, based on Volta architecture, continue to ramp for both AI and high-performance computing applications. Volta has been adopted by every major cloud provider and hyperscale data center operator around the world. Customers have quickly moved to qualify the new version of V100, which doubled the on-chip DRAM to 32 gig to support much larger data sets and neural networks. Major server OEMs, HP Enterprise, IBM, Lenovo, Cray and Supermicro also brought the V100 32-gig to market in the quarter. We continue to gain traction with AI inference solution, which helped expand our addressable market in the data center. During the quarter, we released our TensorRT 4 AI inference accelerator software for general availability. While prior versions of the TensorRT optimized image and video-related workloads, TensorRT 4 expands the aperture to include more use cases, such as speech recognition, speech synthesis, translation and recommendation systems. This means we can now address a much larger portion of deep learning inference workloads, delivering up to 190x performance speed-up, relative to CPUs. NVIDIA and Google engineers have integrated TensorRT into the TensorFlow deep learning framework, making it easier to run AI inference on our GPUs. And Google Cloud announced that NVIDIA Tesla P4 GPU, our small form factor GPU for AI inference and graphic virtualization, is available on Google Cloud Platform. Data center growth was also driven by DGX, our fully optimized AI server, which incorporates V100 GPUs, our proprietary high-speed interconnect and our fully optimized software stack. The annual run rate for DGX is in the hundreds of millions of dollars. DGX-2, announced in March at our GPU Technology Conference, is being qualified by customers and is on track to ramp in the third quarter. At GTC Taiwan in June, we announced that we are bringing DGX-2 technology to our HGX-2 server platform. We make HGX-2 available to OEM and ODM partners so they can quickly deploy our newest innovations in their own server designs. In recent weeks, we announced partnerships with NetApp and Pure Storage to help customers speed AI deployment from months to days or even hours, with highly integrated optimized solutions that combine DGX with the company's all-flash storage offerings and third-party networking. At GTC Taiwan, we also revealed that we are -- set 5 speed records for AI training and inference. Key to our strategy is our software stack. From CUDA to our training and inference SDKs as well as our work with our developers to accelerate their applications, it is the reason we can achieve such dramatic performance gains in such a short period of time. And our developer ecosystem is getting stronger. In fact, we just passed 1 million members in our developer program, up 70% from 1 year ago. One of our proudest moments this quarter was the launch of the Summit AI supercomputer in Oak Ridge National Laboratory. Summit is powered by over 27,000 Volta Tensor Core GPUs and helped the U.S. reclaim the #1 spot on the TOP500 Supercomputer list for the first time in 5 years. Other NVIDIA power systems joined the TOP500 list were Sierra at Lawrence Livermore National Laboratory in the third spot, and the ABCI, Japan's fastest supercomputer, in the fifth spot. NVIDIA now powers 5 of the world's 7 fastest supercomputers, reflecting the broad shift in supercomputing to GPUs. Indeed, the majority of the computing performance added to the latest TOP500 list comes from NVIDIA GPUs, and more than 550 HPC applications are now GPU accelerated. With our Tensor Core GPUs, supercomputers can now combine simulation with the power of AI to advance many scientific applications from molecular dynamics to seismic processing to genomics and materials science. Moving to pro visualization. Revenue grew to $281 million, up 20% year-on-year and 12% sequentially, driven by demand for real-time rendering and mobile workstations as well as emerging applications like AI and VR. These emerging applications now represent approximately 35% of pro visualization sales. Strength extended across several key industries, including health care, oil and gas and media and entertainment. Key wins in the quarter include Raytheon, Lockheed, GE, Siemens and Philips Healthcare. In announcing the Turing architecture at SIGGRAPH, we also introduced the first Turing-based processors, the Quadro RTX 8000, 6000 and 5000 GPUs, bringing interactive ray tracing to the world years before it has been predicted. We also announced that the NVIDIA RTX server, a full ray-tracing global illumination rendering server that will give a giant boost to the world's render farms as Moore's Law ends. Turing is set to revolutionize the work of 5 -- 50 million designers and artists, enabling them to render photorealistic scenes in real time and add new AI-based capabilities to their work flows. Quadro GPUs based on the Turing will be available in the fourth quarter. Dozens of leading software providers, developers and OEMs have already expressed support for Turing. Our pro viz partners view it as a game-changer for professionals in the media and entertainment, architecture and manufacturing industries. Finally, turning to automotive. Revenue was a record $161 million, up 13% year-on-year and up 11% sequentially. This reflects growth in our autonomous vehicle production and development engagements around the globe as well as the ramp of next-generation AI-based smart cockpit infotainment solutions. We continue to make progress on our autonomous vehicle platform with key milestones and partnerships announced this quarter. In July, Daimler and Bosch selected DRIVE Pegasus as the AI brain for their Level 4 and Level 5 autonomous fleets. Pilot testing will begin next year in Silicon Valley. This collaboration brings together NVIDIA's leadership in AI and self-driving platforms, Bosch's hardware and systems expertise as the world's largest Tier 1 automotive supplier and Daimler's vehicle expertise and global brand synonymous with safety and quality. This quarter, we started shipping development systems for DRIVE Pegasus, an AI supercomputer designed specifically for autonomous vehicles. Pegasus delivers 320 trillion operations per second to handle diverse and redundant algorithms and is architected for safety as well as performance. This automotive-grade functionally safe production solution uses 2 NVIDIA Xavier SoCs and 2 next-generation GPUs designed for AI and visual processing, delivering more than 10x greater performance and 10x higher data bandwidth compared to the previous generation. With co-designed hardware and software, the platform is created to achieve ASIL D ISO 26262, the industry's highest level of automotive functional safety. We have created a scalable AI car platform that spans the entire range of automated and autonomous driving from traffic jam pilots to Level 5 robo-taxis. More than 370 companies and research institutions are using NVIDIA's automotive platform. With this growing momentum and accelerating revenue growth, we remain excited about the intermediate and long-term opportunities for autonomous driving business. This quarter, we also introduced our Xavier platform for Jetson for the autonomous machine market. With more than 9 billion transistors, it delivers over 30 trillion operations per second, more processing capability than a powerful workstation while using 1/3 the energy of a lightbulb. Jetson Xavier establishes customers to deliver AI computing at the edge, powering autonomous machines like robots or drones with applications in manufacturing, logistics, retail, agricultural, health care and more. Lastly, in our OEM segment, revenue declined by 54% year-on-year and 70% sequentially. This was primarily driven by the sharp decline of cryptocurrency revenues to fairly minimal levels. Moving to the rest of the P&L. Q2 GAAP gross margin was 63.3% and non-GAAP was 63.5%, in line with our outlook. GAAP operating expenses were $818 million. Non-GAAP operating expenses were $692 million, up 30% year-on-year. We can continue to invest in the key platforms driving our long-term growth, including gaming, AI and automotive. GAAP net income was $1.1 billion and EPS was $1.76, up 89% and 91%, respectively, from a year earlier. Some of the upside was driven by a tax rate near 7% compared to our outlook of 11%. Non-GAAP net income was $1.21 billion and EPS was $1.94, up 90% and 92%, respectively, from a year ago, reflecting revenue strength as well as gross and operating margin expansion and lower taxes. Quarterly cash flow from operations was $913 million. Capital expenditures were $128 million. With that, let me turn to the outlook for the third quarter of fiscal 2019. We are including no contribution from crypto in our outlook. We expect revenue to be $3.25 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 62.6% and 62.8%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $870 million and $730 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of $20 million. GAAP and non-GAAP tax rates are both expected to be 9%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $125 million to $150 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, I'd like to highlight some of the upcoming events for the financial community. We'll be presenting at the Citi Global Technology Conference on September 6 and meeting with the financial community at our GPU Technology Conferences in Tokyo on September 13 and Munich on October 10. And our next earnings calls to discuss our financial results is in the third quarter of 2019 will take place on November 15. We will now open the call for questions. (Operator Instructions) And operator, would you please poll for questions?","Yes, Mark. So first of all, Turing, as you know, is the world's first ray-tracing GPU. And it completes our new computer graphics platform, which is going to reinvent computer graphics altogether. It unites 4 different computing modes: rasterization, accelerated ray tracing, computing with CUDA and artificial intelligence. It uses these 4 basic methods to create imagery for the future. There's 2 different -- 2 major ways that we'll experience the benefits right away. The first is for the markets of visualization today, they require photorealistic images. Whether it's an IKEA catalog or a movie or architectural engineering or product design, car design, all of these types of markets require photorealistic images. And the only way to really achieve that is to use ray tracing with physically based materials and lighting. The technology is rather complicated. It's been computing-intensive for a very long time. And it wasn't until now that we've been able to achieve it in a productive way. And so Turing has the ability to do ray tracing, accelerated ray tracing, and it also has the ability to combine very large frame buffers because these data sets are extremely large. And so that marketplace is quite large and it's never been served by GPUs before until now. All of that has been run on CPU render farms, gigantic render farms in all these movie studios and service centers and so on and so forth. The second area where you're going to see the benefits of ray tracing, we haven't announced. Yes, Mark, the -- at GTC this last year in March, GDC and GTC, we announced a brand-new platform called NVIDIA RTX. And this platform has those 4 computation methods that I described for generating images. We put that platform out with the support of Microsoft. They call it the Microsoft DirectX Raytracing and the major game engine companies. Epic has implemented real-time raytracing and the RTX into the Epic engine, the Unreal Engine. And at GDC and GTC, we demonstrated, for the very first time, on 4 Volta GPUs, on 4 Volta GPUs, the ability to do that. And it was the intention of -- to get this platform out to all of the game developers. And we've been working with game developers throughout this time. Thus, this week at SIGGRAPH, we announced Quadro, which is the first -- the Quadro RTX 8000, 6000 and 5000 -- the world's first accelerated ray-tracing GPUs. And I demonstrated one Quadro running the same application that we demonstrated on 4 Volta GPUs running in March. And the performance is really spectacular. And so I think the answer to your question is developers all have access to RTX. It's in Microsoft's DirectX. It's in the most popular game engine in the world, and you're going to start to see developers use it. On the workstation side, on the professional visualization side, all of the major ISPs have jumped on to adopt it. And at SIGGRAPH this year, there you could see a whole bunch of developers demonstrating the NVIDIA RTX with accelerated ray tracing, generating photorealistic images. And so I would say that in no platform in our history has, on day 1 of announcement, had so many developers jump onto it. And stay tuned, we've got a lot more stories to tell you about RTX. Sure. Thanks for your question. So when you look at our inventory on the balance sheet, I think it's generally consistent with what you have seen over the last several months in terms of what we will be bringing to market. Turing is an extremely important piece of architecture, and as you know, it will be with us for some time. So I think the inventory balance is getting ready for that. And don't forget our work in terms of data center and what we have for Volta is also a very, very complex computer, in some cases, in terms of what we have also in terms of there. So just those things together, plus our Pascal architecture is still here, makes up almost all of what we have there in terms of inventory. Matt, on the channel inventory side, we see inventory in the lower ends of our stack. And that inventory is well positioned for back-to-school and the building season that's coming up on Q3. And so I feel pretty good about that. The rest of our product launches and the ramp-up of Turing is going really well. And so I think the rest of the announcements we haven't made, but stay tuned. The RTX family is going to be a real game-changer for us and the reinvention of computer graphics altogether has been embraced by so many developers. We're going to see some really exciting stuff this year. Actually, just a clarification and then a question. On the clarification, Colette, if you could also help us understand the gross margin sequencing from Q2 to Q3. And then, Jensen, how would you contrast the Pascal cycle, the Turing cycle? Because I think in your remarks, you mentioned Turing is a very strong advancement over what you had before. But when you launched Pascal, you had guided to very strong Q3s and then Q4s. This time, the Q3 outlook, even though it's good on an absolute basis, on a sequential and a relative basis, it's perhaps not as strong. So if you could just help us contrast the Pascal cycle with what we should expect with the Turing cycle. Sure, thanks for that -- for the question. Let me start first with your question regarding gross margins. We have essentially reached, as we move into Q3, a normalization of our gross margins. I believe, over the last several quarters, we have seen the impacts of crypto and what that can do to elevate our overall gross margins. We believe we have reached a normal period as we're looking forward to essentially no cryptocurrency as we go forward. Let's see, Pascal was really successful. Pascal, relative to Maxwell, was a leap in fact. And it was a really significant upgrade. The architectures were largely the same. They were both programmable shading, they were both at the same generation programmable shading. But Pascal was much, much more energy efficient. I think it was something like 30%, 40% more energy efficient than Maxwell, and that translated to performance benefits to customers. The success of Pascal was fantastic. There's just simply no comparison to Turing. Turing is a reinvention of computer graphics. It is the first ray-tracing GPU in the world. It's the first GPU that will be able to ray trace light in an environment and create photorealistic shadows and reflections and be able to model things like area lights and global illumination and indirect lighting. But the images are going to be so subtle and so beautiful, it -- when you look at it, it just looks like a movie. And yet it's backwards compatible with everything that we've done. This new hybrid rendering model, which extends what we've built before but added to it 2 new capabilities, artificial intelligence and accelerated ray tracing, is just fantastic. So everything of the past will be brought along and benefits, and it's going to create new visuals that were impossible before. We also did a good job on laying the foundations of the development platform for the developers. We've partnered with Microsoft to create DXR. Vulkan RT is also coming, and we have optics that are used by pro viz renderers and developers all over the world. And so we have the benefit of laying the foundation stack by stack by stack over the years. And as a result, on the day that Turing comes out, we're going to have a richness of applications that gamers will be able to enjoy. You mentioned guidance. I actually think that on a year-over-year performance, we're doing terrific. And I'm super excited about the ramp of Turing. It is the case that we benefited in the last several quarters from an unusual lift from crypto. In the beginning of the year, we thought and we projected that crypto would be a larger contribution through the rest of the year, but at this time, we consider it to be immaterial for the second half. And so that makes comparisons on a sequential basis on, I guess, a quarterly sequential basis harder. But on a year-to-year basis, I think we're doing terrific. Every single one of our platforms are growing. High-performance computing, of course, data centers is growing. AI, the adoption continues to sweep from one industry to another industry. The automation that's going to be brought about by AI is going to bring productivity gains to industries like nobody's ever seen before. And now with Turing, we're going to be able to reignite the professional visualization business, open us up to photorealistic rendering for the very first time, render farms and everybody who's designing products that has to visualize it photo realistically, to reinventing and resetting graphics for video games. And so I think we're in a great position, and I'm looking forward to reporting Q3 when the time comes. So as you know, we generally give our view on guidance for 1 quarter out. You are correct that our data center results that we see is always a tremendous unique mix every single quarter in terms of what we're seeing. But there's still some underlying points of that, that will likely continue. The growth in terms of use by the hyperscales, continued industry by industry coming on board, essentially just because the needs of accelerated computing for the workloads and for the data that they have is so essential. So we still expect, as we go into Q3, for data center to grow both sequentially and year-over-year. And we'll see probably a mix of both selling our Tesla V100 platforms but also a good contribution from our DGX. Yes, that's right. Atif, let me just add a little bit more to that. I think the one simple way to think about that is this. In the transportation industry, let's take one particular vertical. There are 2 dynamics that are happening that are very abundantly clear and that will transform that industry. The first, of course, is ride hailing and ride sharing. Those platforms, in order to make the recommendation of which taxi to bring to which passenger, to which customer, is a really large computing problem. It's a machine-learning problem. It's an optimization problem of very, very large scale. And in every -- in each and every one of those instances, you need high-performance computers to use machine learning to figure out how to make that perfect match or the most optimal match. On the second -- the second is self-driving cars. Every single car company that's working on robot taxis or self-driving cars needs to collect data, label data, train a neural network or train a whole bunch of neural networks and to run those neural networks in cars. And so you just make your list of how many people are actually building self-driving cars. And every single one of them will need even more GPU-accelerated servers. And that's just for developing the model. Then the next stage is to simulate the entire software. Because we know that the industry or the world travels 10 trillion miles per year, and the best we could possibly do is to drive several million normal miles. And what we really want to do is to be able to simulate and stress -- stress-test our software stack and the only way to do that is doing virtual reality. And so that's another supercomputer that you have to build for simulating all your software costs, those billions and billions of virtually created, challenging miles. And then lastly, before you OTA the software, you're going to have to re-sim and replay against all of the miles that you've collected over the years to make sure that you have no regressions before you OTA the new models into a fleet of cars. And so transportation is going to be a very large industry. Health care is the same way, from medical imaging that is now using AI just about everywhere to genomics that has discovered deep learning and the benefits of artificial intelligence; and in the future, pathology. The list goes on. And so industry after industry after industry, we're discovering the benefits of deep learning and the industries could be really, really revolutionized by it. We're expecting the channel inventory to work itself out. We are masters at managing our channel, and we understand the channel very well. As you know, the way that we go to market is through the channels around the world. We're not concerned about the channel inventory. As we ramp Turing, any -- whenever we ramp a new architecture, we ramp it from the top down. And so we have plenty of opportunities as the -- as we go back to the back-to-school and the gaming cycle to manage the inventory, so we feel pretty good about that. As a result, comparing Volta and Turing, CUDA's compatible. That's one of the benefits of CUDA. CUDA -- all of the applications that take advantage of CUDA that are written on top of cuDNN, which is our deep neural network platform, to TensorRT that takes advantage -- that takes the output of the frameworks and optimize it for run time. All of those tools and libraries run on top of Volta and run on top of Turing and run on top of Pascal. What Turing adds over Pascal is the same Tensor Core that is inside Volta. Of course, Volta is designed for large-scale training. 8 GPUs could be connected together. They have the fastest HBM2 memories, and it's designed for data center applications, has 64-bit double-precision ECC, high-resilience computing and all of the software and system software capability and tools that make Volta the perfect high-performance computing accelerator. In the case of Turing, it's really designed for 3 major applications. The first application is to open up pro visualization, which is a really large market that has historically used render farms and were really unable to use GPUs until we now have the ability to do full path trace, global illumination with very, very large data sets. So that's one market that's brand new as a result of Turing. The second market is to reinvent computer graphics, real-time computer graphics, for video games and other real-time visualization applications. When you see the images created by Turing, you're going to have a really hard time wanting to see the images of the past. It just looks amazing. And then the third, Turing has a really supercharged Tensor Core. And this Tensor Core is used for image generation. It's also used for high throughput deep learning inferencing for data centers. And so these applications for Turing, which suggests that there are multiple SKUs of Turing, which is one of the reasons why we have such a great engineering team, we could scale one architecture across a whole lot of platforms at one time. And so I hope that answers your question. The Tensor Core inference capability of Turing is going to be off the charts. I wonder if you could talk about cryptocurrency now that the dust has settled. You guys have done a good job of kind of laying out exactly how much of the OEM business has been driven by that. But there's also been, I think, some sense of -- some of the GeForce business was being driven by crypto. Can you -- looking backwards, can you size that for us? And I guess, I'm trying to understand the impact that crypto would have on the guidance for October, given that it seems like it was very small in the July quarter. Well, I think -- I mean, the second question is easier to answer, and the reason -- the first one is just -- it's ambiguous and hard to predict anyway. It's hard to estimate no matter what. But the second question, the answer is we're expecting -- we're projecting 0 basically. And for the first question, how much of GeForce could have been used for crypto, a lot of gamers at night, they could -- while they're sleeping, they could do some mining. And so did they buy it for mining or did they buy it for gaming, it's kind of hard to say. And some miners were unable to buy our OEM products, and so they jumped onto the market to buy it from retail. And that probably happened a great deal as well. And that all happened in the last -- the previous several quarters, probably starting from Q -- late Q3, Q4, Q1 and very little last quarter, and we're projecting no crypto mining going forward. Thanks, Toshiya. Inference is going to be a very large market for us. It is surely material now in our data center business. It's not the largest segment, but I believe it's going to be a very large segment of our data center business. There are 30 million servers around the world, let's kind of estimate, in the cloud, and there are a whole lot more in enterprises. I believe that almost every server in the future will be accelerated. And the reason for that is because artificial intelligence and deep learning software and neural net models are going to -- prediction models, are going to be infused into software everywhere. And acceleration has proven to be the best approach going forward. We've been laying the foundations for inferencing for a couple 2, 3 years. And as we've described at GTCs, inference is really, really complicated. And the reason for that is you have to take the output of these massive, massive networks that are output of the training frameworks and optimize it. This is the -- probably the largest computational graph optimization problem the world's ever seen. And this is brand-new invention territory. There are so many different network architectures from CNNs to RCNNs to autoencoders to RNNs and LSTMs, there is just so many different species of neural networks these days, and it's continuing to grow. And so the compiler technology is really, really complicated. And this year, we announced 2 things. Earlier this year, we announced that we've been successful in taking the Tesla P4 low-profile, high-energy efficiency inference accelerator into hyperscale data centers. And we announced our fourth generation TensorRT optimizing compiler -- neural network optimizing compiler. And TRT 4 goes well beyond CNNs and image recognition in the beginning. It now allows us to support and optimize for voice recognition or speech recognition, natural language understanding, recommendation systems, translation. And all of these applications are really pervasive from Internet services all over the world. And so now from images to video to voice to recommendation systems, we now have a compiler that can address it. We are actively working with just about every single Internet service provider in the world to incorporate inference acceleration into their stack. And the reason for that is because they need high throughput and, very importantly, they need low latency. Voice recognition is only useful if it responds in a relatively short period of time. And our platform is just really, really excellent for that. And then this last week -- this week, we announced Turing. And I announced that the inference performance of Turing is 10x the inference performance of Pascal, which is already a couple of hundred times the inference performance of CPUs. And so you take a look at the rate at which we're moving, both in the support of new neural networks, the ever-increasing optimization and performance output of the compilers and the rate at which we're advancing our processors, I think we're raising the bar pretty high, okay? So with that, Colette? Yes. So when you look at our overall segments, as you will have seen our results in terms of this last Q2, there was growth across every single one of our platforms from a year-over-year standpoint. We probably possibly see that again in our Q3 guidance, the year-over-year growth across each and every one of those platforms. Of course, our OEM business will be down likely year-over-year, again just due to the absence of cryptocurrency in our forecast. When we think about sequentially, our hopes is absolutely, our data center will grow and we'll likely see the growth of our gaming business as well. It's still early, still we've got many different scenarios and -- on our pro viz and auto. But definitely, our gaming and our data center are expected to grow sequentially. Two on gross margin. Colette, I just want to make sure I understood, July to October gross margins down. I know you've been getting a benefit from crypto, but it's pretty de minimis in July. Just is there any other moving pieces? And then kind of longer picture here, how do you think about the ramp of Turing affecting gross margins? You're obviously enabling a lot of capabilities. You get paid for it, 12-nanometers, fairly stable. Just kind of curious how to think about over the next couple of quarters gross margin with that ramp. Yes. So let me take your first part of the question regarding our gross margins and what we had seen from crypto. Although crypto revenue may not be large, it still has a derivative impact on our stock in terms of what we are selling in to both replenish the overall channel and such. So over the last several quarters that we had stabilizing that overall channel, we did get the great effect of selling just about everything, and our margin's really been able to benefit from that. Again, when we look at the overall growth year-over-year for Q2, you have 500 basis points in terms of growth. We're excited about what we have now here for Q3 as well, which is also significant growth year-over-year. Of course, we have our high value-added platforms as we move forward, both those in data center, those in terms of what we expect the effects of Turing in terms of -- on our Quadro piece as well. But that will take some time for that all to partake. So we'll see how that goes. We haven't announced anything further at this time. But yes, we'll see probably over the longer term, the effects of what Turing can do. Yes, Aaron, I think that if you look at the -- if you start from first principles, here's the simple way to look at it. Demand is continuing to grow at historical levels of 10x computing demand. Computing demand is increasing at historical levels of 10x every 5 years. 10x every 5 years is approximately Moore's Law. And computing demand continues to grow at 10x every 5 years, however, Moore's Law stopped. And so that gap in the world in high-performance computing, in medical imaging, in life sciences computing, in artificial intelligence, that gap -- because those applications demand more computing capability, that gap can only be served in another way. And NVIDIA's GPU accelerated computing that we pioneered really stands to benefit from that. And so at the highest level, whether it's supercomputing -- and this year, you heard Colette say earlier that NVIDIA GPUs represented 56% of all the new performance that came into the world's TOP500. The TOP500 is called the TOP500 because it reflects the future computing. And my expectation is that more and more, from one vertical industry after another -- and I mentioned transportation, I mentioned health care, the vertical industries go on and on -- that as computing demand continues at a factor of 10x every 5 years, developers are rational and logical to have jumped on NVIDIA's GPU computing to boost their demand. I think that's probably the best way to answer it. HGX-1 was, I guess, kind of the prototype of HGX-2. HGX-2 is doing incredibly well for all the reasons that you mentioned. It is -- and even the largest hyperscale data centers can't afford to create these really complicated motherboards at the scale that we're talking about. And so we created HGX-2 and it was immediately adopted by several most important hyperscalers in the world. And we were at GTC Taiwan, and we announced basically all of the leading server OEMs and ODMs supporting HGX-2 and are ready to take it to market. So we're in the process of finishing the -- finishing HGX-2 and ramping them into production. And so I think HGX-2 is a huge success for exactly the reasons that you mentioned. We could use it for essentially a standard motherboard like the ATX motherboard for PCs that could be used for hyperscalers, it could be used for HPC, it could be used for data centers. And it's really -- it's a really fantastic design. It just allows people to adopt this really complicated and high performance and really high-speed interconnect motherboard in a really easy way. Sure. Well, the crypto mining market is very different today than it was 3 years ago. And even though new cards -- at the current prices, it doesn't make much sense for new cards to be sold into the mining market. The existing capacity is still being used, and you could see that the hash rates continue. And so my sense is that the installed base of miners will continue to use their cards. And then probably the more important factor though is that we're in the process of announcing a brand-new way of doing computer graphics. And with the -- with Turing and the RTX platform, computer graphics will never be the same. And so I think this -- our new generation of new GPUs is really going to do great. I also think that -- I appreciate Elon's comments about our company, and I also think Tesla makes great cars, and I drive them very happily. And with respect to the next generation, it is the case that when we first started working on autonomous vehicles, they needed our help. And we used the 3-year-old Pascal GPU for the current generation of autopilot computers. And it is very clear now that in order to have a safe autopilot system, we need a lot more computing horsepower. In order to have safe computing -- in order to have safe driving, the algorithms have to be rich, it has to be able to handle corner conditions in a lot of diverse situations. And every time that there's more and more corner conditions or more subtle things that you have to do or you have to drive more smoothly or be able to take turns more quickly, all of those requirements require greater computing capability. And that's exactly the reason why we built Xavier. Xavier is in production now. We're seeing great success, and customers are super excited about Xavier. And that's exactly the reason why we built it. And I think it's super hard to build Xavier and all the software stack on top of it. And if it doesn't turn out -- for whatever reason, it doesn't turn out for them, he can give me a call, and I'd be more than happy to help. We had a great quarter. Our core platforms exceeded expectations even as crypto largely disappeared. Each of our platforms -- AI, gaming, pro viz and self-driving cars -- continue to enjoy great adoption. These markets are -- we are enabling are some of the most impactful to the world today. We launched Turing this week. It was 10 years in the making and completes the NVIDIA RTX platform. And NVIDIA RTX with Turing is the greatest advance since CUDA nearly a decade ago. I'm incredibly proud of our company for tackling this incredible challenge, reinventing the entire graphic stack and giving the industry a surge of excitement as we reinvent computer graphics. Stay tuned as we unfold the exciting RTX story. See you guys next time.","The question's on ray tracing, to what extent is this creating new markets versus enabling greater capabilities in your existing markets? If I could have a follow-up on the gaming side. Where do you think the industry is on creating content that leverages that kind of capability? Colette, I had a couple of questions about inventory, the first of which is, I understand you've launched a new product set in pro viz, and the data center business is obviously ramping really strongly. But if you look at the balance sheet, I think the inventory level is up by around mid-30s percent sequentially and you're guiding revenue up 3% or so. Maybe you could help us sort of walk through the contribution to that inventory and what it might mean for future products. And secondly, if you could talk a little bit about the gaming channel in terms of inventory, how things are looking in the channel as you guys see it during this period of product transition. Colette, I have a question on data center. In your prepared remarks, you talked about AI and high-performance computing driving new verticals and some of these verticals are fastest growing. Some of your peers have talked about enterprise spending slowing down in the back half of this year and so unit demand, and you guys are not a unit play but more of an AI adoption. Just curious in terms of your thinking about second half data center growth. I guess, short term and the long term. So for short term, as you think about your gaming guide, are you embedding any drawdown of channel inventory there? And then longer term, as you think about Turing Tensor Cores, can you talk a bit about differentiation versus Volta V100, particularly as you think about 8-bit integer and the opportunities there for inferencing? I had one for Jensen and one for Colette. Jensen, I was hoping you could remind us how meaningful your inference business is today within data center and how you would expect growth to come about over the next 2 years as you -- as your success at accounts like Google proliferate across a broader set of customers. And then for Colette, if you can give directional guidance for each of your platforms. I know you talked about data center a little bit, but if you can talk about the other segments. And on gaming specifically, if you can talk about whether or not new products are embedded in that guide. I'm curious as we look at the data center business, if you can help us understand the breakdown of demand between hyperscale, the supercomputing piece of the business and the AIPs. And I guess, on top of that, I'm just curious, one of the metrics that's pretty remarkable over the last couple of quarters is you've seen significant growth in China. I'm curious if that's related to the data center business or what's really driving that as kind of a follow-up question. When we think about cloud and hyperscale, we tend to think about the top guys, right? They're designing their own platform using your Tesla-based products or sometimes even designing their own chips for AI and deep learning. But there's a larger base of medium to smaller cloud and hyperscale customers out there who don't have the R&D scale. And I think that's where your HGX platform seems to be focused on. So Jensen, can you just give us an update on the uptake of your first-generation HGX-1 reference platform and the initial interest on the HGX-2? Actually, I had 2 questions, Jensen, both for you. First, now that crypto has fallen off, I'm curious what you think the potential is that maybe we see a slug of cards that get resold on eBay or some other channel and that could cannibalize new Pascal sales. Is that something that keeps you up at night? Number one. Number two, obviously, the stories about gaming and data center, and I know that you don't typically talk about customers, but since Tesla did talk about you on their call, I'm curious what your comments are about the development for Hardware 3 and their own efforts to move away from your drive platform. PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q4 2018,2428,5056,928,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the Fourth Quarter of Fiscal 2018. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until February 16, 2018. The webcast will be available for replay up until next quarter's conference call to discuss our fiscal first quarter financial results. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the report that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, February 8, 2018, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures and our CFO commentary, which is posted on our website. With that, I will turn the call over to Colette. Thanks, Simona. We had an outstanding quarter and fiscal 2018 led by strong growth in our gaming and data center businesses. Q4 revenue reached $2.91 billion, up 34% year-on-year, up 10% sequentially, and above our outlook of $2.65 billion. All measures of profitability set records. They also hit important milestones. For the first time, gross margins strongly exceeded 60%, non-GAAP operating margins exceeded 40% and net income exceeded $1 billion. Fiscal 2018 revenue was $9.71 billion, up 41% or $2.8 billion above the previous year. Each of our platforms posted record full year revenue, with data center growing triple digits. From a reporting segment perspective, Q4 GPU revenue grew 33% from last year to $2.46 billion. Tegra Processor revenue rose 75% to $450 million. Let's start with our gaming business. Q4 revenue was $1.74 billion, up 29% year-on-year and up 11% sequentially with growth across all regions. Driving GPU demand were a number of great titles during the holiday season, including Player's Battleground (sic) [PlayerUnknown's Battlegrounds], PUBG, Destiny 2, Call of Duty: WWII, Star Wars Battlefront II. PUBG continued its remarkable run, reaching almost 30 million players and recording more than 3 million concurrent players. These games deliver stunning visual effects that require strong graphics performance which has driven a shift toward the higher end of our gaming portfolio and adoption of our Pascal architecture. eSports continues to grow, expanding the overall industry and our business. In one sign of their popularity, Activision's Overwatch League launched in January and reached 10 million viewers globally in its first week. We had a busy start to the year with a number of announcements at the annual Consumer Electronics Show in Las Vegas. We introduced NVIDIA BFGDs, big-format gaming displays, in a partnership with Acer, ASUS and HP. These high-end 65-inch 4K displays enable ultralow latency gaming and integrate our SHIELD streaming device, offering popular apps such as Netflix, gaming video (sic) [Amazon Video], YouTube and Hulu. The BFGD won 9 Best of Show awards for various publications. We expanded the free beta of GeForce NOW beyond Macs to Window-based PCs, and we enhanced GeForce Experience with new features, including NVIDIA freestyle for customizing gameplay with various filters. And updated NVIDIA Ansel's photo mode and support for new titles with ShadowPlay highlights for capturing gaming achievements. Additionally, the Nintendo Switch gaming console contributed to our growth as it became the fastest-selling console of all time in the U.S. Strong demand in the cryptocurrency market exceeded our expectations. We met some of this demand with a dedicated board in our OEM business, and some was met with our gaming GPUs. This contributed to lower than historical channel inventory levels of our gaming GPUs throughout the quarter. While the overall contribution of cryptocurrency to our business remains difficult to quantify, we believe it was a higher percentage of revenue than the prior quarter. That said, our main focus remains on our core gaming market as cryptocurrency trends will likely remain volatile. Moving to data center. Revenue of $606 million was up 105% year-on-year and up 20% sequentially. This excellent performance reflected strong adoption of Tesla V100 GPUs based on our Volta architecture which began shipping in Q2 and continued to ramp in Q3 and Q4. V100s are available through every major computer maker and have been chosen by every major cloud provider to deliver AI and high-performance computing. Hyperscale and cloud customers adopting the V100 include Alibaba, Amazon Web Services, Baidu, Google, IBM, Microsoft Azure, Oracle and Tencent. We continued our leadership in AI-trending markets where our GPUs remain the platform of choice for training deep learning networks. During the quarter, Japan's Preferred Networks trained the ResNet-50 neural network for image classification in a record 15 minutes by using 1,024 Tesla P100 GPUs. Our newer-generation V100s delivered even higher performance, with the Volta architecture offering 10x the deep learning performance of Pascal. We also saw a growing traction in the AI inference market where NVIDIA's platform can improve performance and efficiency by orders of magnitude over CPUs. We continue to view AI inference as a significant new opportunity for our data center GPUs. Hyperscale inference applications that run on GPUs include speech recognition, image and video analytics, recommender systems, translation, search and natural language processing. The data center business also benefited from strong growth in high-performance computing. The HPC community has increasingly moved to accelerated computing in recent years as Moore's Law has begun to level off. Indeed, more than 500 HPC applications are now GPU-accelerated, including all of the top 15. NVIDIA added a record 34 new GPU-accelerated systems to the latest TOP500 supercomputer list, bringing our total to 87 systems. We increased our total petaflops of list by 28%, and we captured 14 of the top 20 spots on the Green500 list of the world's most energy-efficient supercomputers. During the quarter, we continued to support the buildout of major next-generation supercomputers. Among them is the U.S. Department of Energy's Summit system, expected to be the world's most powerful supercomputer when it comes online later this year. We also announced new wins such as Japan's fastest AI supercomputer, the ABCI system, which leverages more than 4,000 Tesla V100 GPUs. Importantly, we are starting to see the convergence of HPC and AI as scientists embrace AI to solve problems faster. Modern supercomputers will need to support multi-precision computation for applying deep learning together with simulation and testing. By combining AI with HPC, supercomputers can deliver increased performance that is orders of magnitudes greater in computations ranging from particle physics to drug discovery to astrophysics. We are also seeing traction for AI in a growing number of vertical industries, such as transportation, energy, manufacturing, smart cities and health care. We announced engagements with GE Health and Nuance in medical imaging; Baker Hughes, a GE company, in oil and gas; and Japan's Komatsu in construction and mining. Moving to professional visualization. Fourth quarter revenue grew a record -- to a record $254 million, up 13% from a year ago, up 6% sequentially, driven by demand for real-time rendering as well as emerging applications like AI and VR. These emerging applications now represent approximately 30% of pro visualization sales. We saw strength across several key industries including defense, manufacturing, energy, health care and Internet service providers. Among key customers, high-end Quadro products are being used by GlaxoSmithKline for AI and by Pemex oil and gas for seismic processing and visualization. Turning to automotive. In automotive, for the fourth quarter, revenue grew 3% year-on-year to $132 million and was down 8% sequentially. The sequential decline reflects our transition from infotainment, which is becoming commoditized, to next-generation AI cockpit systems and complete top-to-bottom self-driving vehicle platforms built on NVIDIA hardware and software. At CES, we demonstrated our leadership position on autonomous vehicles with several key milestones and new partnerships that point to AI self-driving cars moving from deployment to production. In a standing-room only keynote that drew nearly 8,000 attendees, Jensen announced that DRIVE Xavier, the world's first autonomous machine processor, will be available to customers this quarter. With more than 9 billion transistors, DRIVE Xavier is the most complex system on a chip ever created. We also announced that NVIDIA DRIVE is the world's first functionally safe AI self-driving platform, enabling automakers to create autonomous vehicles that can operate safely, a necessary ingredient for going to market. Additionally, we announced a number of collaborations at CES, including with Uber, which has been using NVIDIA technology for the AI computing system in its fleets of self-driving cars and freight trucks. We announced that ZF and Baidu are using NVIDIA DRIVE self-driving technologies to create a production-ready AI autonomous vehicle platform for China, the world's largest automotive market. Production vehicles utilizing this technology, including those from Chery, are expected on the road by 2020. We also announced a partnership with Aurora, which is working to create a modular, scalable, Level 4 and Level 5 self-driving hardware platform, incorporating the NVIDIA DRIVE Xavier processor. Jensen was joined on stage by Volkswagen CEO, Herbert Diess. They announced the new generation of intelligent VW vehicles will use the NVIDIA DRIVE intelligent experience, or DRIVE IX, platform to create the new AI-infused cockpit experiences and improved safety. Later at CES, Mercedes Benz announced that MBUX, its new AI-based smart cockpit uses NVIDIA's graphics and AI technologies. The MBUX user experience, which includes beautiful touchscreen displays and a new voice-activated assistant, debuted last week at Mercedes-Benz A-Class compact car and will ship this spring. And earlier this week, we announced a partnership with Continental to build AI self-driving vehicle systems from enhanced Level 2 to Level 5 for production in 2021. There are now more than 320 companies and research institutions using the NVIDIA DRIVE platform. That's up 50% from a year ago and encompasses virtually every carmaker, truck maker, robotaxi company, mapping company, sensor manufacturer and software startup in the autonomous vehicle ecosystem. With this growing momentum, we remain excited about the intermediate to long-term opportunities for autonomous driving. Now turning to the rest of the P&L. Q4 GAAP gross margins was 61.9%, and non-GAAP was 62.1%, records that reflect continued growth in our value-added platforms. GAAP operating expenses were $728 million, and non-GAAP operating expenses were $607 million, up 28% and 22% year-on-year, respectively. We continue to invest in the key platforms driving our long-term growth, including gaming, AI and automotive. GAAP EPS was $1.78, up 80% from a year earlier. Some of the upside was driven by a lower-than-expected tax rate as a result of U.S. tax reform and excess tax benefits related to stock-based compensation. Our fourth quarter GAAP effective tax rate was a benefit of 3.7% compared with our expectation of a tax rate of 17.5%. Non-GAAP EPS was $1.72, up 52% from a year ago, reflecting a quarterly tax rate of 10.5% compared with our expectation of 17.5%. We returned $1.25 billion to shareholders in the fiscal year through a combination of quarterly dividends and share repurchases. Our quarterly cash flow from operations reached record levels at $1.36 billion, bringing our fiscal year total to a record $3.5 billion. Capital expenditures were $416 million for the fourth quarter, inclusive of $335 million associated with the purchase of our previously financed Santa Clara campus building. Let me take a moment to provide a bit more detail on the impact of U.S. corporate tax reform on the quarter and our go-forward financials. In Q4, we recorded a GAAP-only one-time net tax benefit of $133 million or $0.21 per diluted share. This is primarily related to provisional tax amounts for the transition tax on accumulated foreign earnings and remeasurement of certain deferred tax assets and liabilities associated with the Tax Cuts and Jobs Act. We previously accrued for taxes on a portion of forward earnings in excess of the provisional tax amount recorded for the transition tax, hence, the one-time benefit. For fiscal 2019, we expect our GAAP and non-GAAP tax rates to be around 12%, which is down from approximately 17% previously. This does not take into effect the excess tax benefit from stock-based compensation which, depending on stock price and vesting schedule, could increase or decrease our tax rate and GAAP in a given quarter. In terms of our capital allocation priorities, we continue to focus first and foremost on investing in our business as we see significant opportunities ahead. Our lower tax rate strengthens our ability to invest in both OpEx, such as adding engineering talent; as well as CapEx, such as investing in supercomputers for internal AI development. In addition, we remain committed to returning cash to shareholders, with our plan remaining at $1.25 billion for fiscal 2019. With that, let me turn to the outlook for the first quarter of fiscal 2019. We expect revenue to be $2.9 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 62.7% and 63%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $770 million and $645 million, respectively. GAAP and non-GAAP OI&E are both expected to be nominal. GAAP and non-GAAP tax rates are both expected to be 12%, plus or minus 1%, excluding discrete items. For the full fiscal year 2019, we expect our operating expenses to grow at a similar pace as in Q1. Further financial details are included in the CFO commentary and other information available in our IR website. In closing, I'd like to highlight a few upcoming events for the financial community. We'll be presenting at the Goldman Sachs Technology & Internet Conference on February 13 and at the Morgan Stanley Technology, Media & Telecom Conference on February 26. We will also be hosting our annual Investor Day on March 27 in San Jose, on the sidelines of our annual GPU Technology Conference, which we are very excited about. We will now open the call for questions. Operator, will you poll for questions, please?","Which way is more conservatively, C.J.? When you say conservatively, which direction were you saying it was. Are you implying up or down? We model crypto approximately flat. Well, there's a lot of dynamics going on in gaming. One dynamic, of course, is that there's a fairly sizable pent-up demand going into this quarter. But I think the larger dynamics that are happening relate to just the really amazing games that are out right now. PUBG is just -- is doing incredibly well, as you might have known, and it's become a global phenomenon. And whether it's here in the United States or in Europe, or in China, in Asia, PUBG is just doing incredibly well. And we expect other developers to come up with similar genre, like PUBG, that are going to be coming out in the near future. And I'm super excited about these games. And then, of course, there's Call of Duty, there's Star Wars. There's just so many great games that are out in the marketplace today, Overwatch and League of Legends, still doing well. There's just a countless number of great franchises that are out in the marketplace. And the gaming market is growing, and production value is going up. And that's driving increased unit sales of GPUs as well as ASPs of GPUs. And so I think those are -- that's probably the larger dynamic of gaming. Yes, first of all, I appreciate you asking a Tensor Core question. It is probably the single biggest innovation we had last year in data centers. Our GPUs, the equivalent performance to one of our GPUs -- one of our Volta GPUs would take something along the lines of 20-plus CPUs or 10-plus nodes. And so 1 GPU alone would do deep learning so fast that it would take 10-plus CPU-powered server nodes to keep up with. And then Tensor Core comes along last year, and we increased the throughput of deep learning, increased the computational throughput of deep learning by another factor of 8. And so Tensor Core really illustrates the power of GPUs. It's very unlike a CPU where the instruction set remains locked for a long time, and it's hard -- it's difficult to advance. In the case of our GPUs and with CUDA, that's one of its fundamental advantages, we can continue to -- year in and year out, continue to add new capabilities to it. And so Tensor Core's boost of the original great performance of our GPU has really raised the bar last year. And as Colette said earlier, our Volta GPU has now been adopted all over the world, whether it's in China with Alibaba, Tencent and Baidu, iFLYTEK, to here in the United States, Amazon and Facebook and Google and Microsoft and IBM and Oracle in Europe, in Japan. The number of cloud service providers that have adopted Volta has been terrific, and I think everybody really appreciates the work that we did with Tensor Core. And all of the updates that are now coming out from the frameworks, Tensor Core is a new instruction set, it's a new architecture. And the deep learning developers have really jumped on it. And almost every deep learning framework is being optimized to take advantage of Tensor Core. And on the inference side, and that's where it would play a role in video games, you could use deep learning now to synthesize and to generate new art. And we've been demonstrating some of that at GTC, if you've seen some of that. Whether it's improve the quality of textures, generating artificial characters, animating characters, whether it's facial animation with -- for speech or body animation, the type of work that you can do with deep learning for video games is growing. And that's where Tensor Core could be a real advantage. If you take a look at the computational capability that we have in Tensor Core, compare that to a nonoptimized GPU or even a CPU, it's now 2-plus orders of magnitude greater computational throughput. And that allows us to do things like synthesize images in real time, synthesize virtual worlds, animate characters, animate faces, bring a new level of virtual reality and artificial intelligence to these video games. Congratulations on the strong growth and the consistent execution. Jensen, just a near- and longer-term question on the data center. Near term, you had, had a number of strong quarters in data center. How is the utilization of these GPUs? And how do you measure whether you're over or under from a supply perspective? And then longer term, there seems to be a lot of money going into startups developing silicon for deep learning. Is there any advantage they have in taking a clean-sheet approach? Or is GPU the most optimal answer? Like if you were starting a new company looking at AI today, would you make another GPU? Or would you make another ASIC or some other format? Just any color would be helpful. Sure. In the near term, the best way to measure customers that are already using our GPUs for deep learning is repeat customers. When they come back another quarter, another quarter, and they continue to buy GPUs, that would suggest that their workload has continued to increase. The -- with existing customers that already have a very deep penetration, another opportunity for us would be using our GPUs for inference, and that's an untapped growth opportunity for our company that's really, really exciting, and we're seeing traction there. For companies that are not at the forefront, the absolute forefront, of deep learning, which -- with the exception of 1 or 2 or 3 hyperscalers, almost everybody else I would put in this category, and their deployment, their adoption of deep learning applying deep learning to all of their applications is still ongoing. And so I think the second wave of customers is just showing up. And then there's the third wave of customers which is -- they're not hyperscalers, they -- they're Internet service applications, Internet applications for consumers. They have enormous customer bases and -- that they could apply artificial intelligence to. But they run their application in hyperscale clouds. That third phase of growth is now really spiking, and I'm excited about that. And so that's kind of the way to think about it. There's the pioneers, the first phase, are the returning customers. Then there's the second phase that's now ramping. The third phase that's now ramping. And then for everybody, we have an opportunity to apply our GPUs for inference. If I had all the money in the world and I had, for example, billions and billions of dollars of R&D, I would give it to NVIDIA's GPU team, which is exactly what I do. And the reason for that is because the GPU was already inherently the world's best high-throughput computational processor. A high-throughput processor is a lot more complicated than linear algebra done that you instantiate from a synopsis tool, it's not quite that easy. The computation throughput, keeping everything moving through your chip with supreme levels of energy efficiency with all of the software that's needed to keep the data flowing, with all of the optimizations that you do with each and every one of the frameworks, the amount of complexity there is just really enormous. The networks are changing all the time. It started out with just basically CNNs, and then all kinds of versions of CNNs now. It started out with RNNs and simple RNNs, and now there's all kinds of LSTMs and gated RNNs, and all kinds of interesting networks that are growing. It started out with just 8 layers, and now it's 152 layers going to 1,000 layers. It started with mostly recognition, and now it's moving to synthesis with GANs. And there's so many versions of GANs. And so all of these different types of networks are really, really hard to nail down. And we're still at the beginning of AI. So the ability for our GPUs to be programmable to all of these different architectures and networks is just an enormous advantage. You don't ever have to guess whether NVIDIA GPUs could be used for one particular network or another. And so you could buy our GPUs at will and know that every single GPU that you buy gives you an opportunity to reduce the number of servers in your data center by 22 nodes, by 10 nodes, 22 CPUs. And so the more GPUs you buy, the more money you save. And so I think that capability is really quite unique. And then if I could just give you one example from last year or from previous year, we introduced 16-bit mix precision, we introduced 8-bit integer, we introduced NVLink the year before this last year. This year -- this last year, we introduced Tensor Core, which increased it by another factor of nearly 10. Meanwhile, our GPUs get more complex, energy-efficient. Efficiency gets better and better every single year, and the software richness gets more amazing. And so it's a much harder problem than just a multiply accumulator. Artificial intelligence is the single most complex mode of software that the world has ever known. That's the reason why it's taken us so long to get here. And these high-performance supercomputers is an essential ingredient and an essential instrument in advancing AI. And so I don't think it's nearly as simple as linear algebra. But if I had all the money in the world, I would invest it in the team that we have. So let me comment on the first one. We did talk about our overall crypto business last quarter as well. We indicated how much we had in OEM boards, and we also indicated that there was definitely some also in our GTX business. Keep in mind, that's very difficult for us to quantify down to the end customer. It is. But yes, there is also some in our Q3, and we did comment on it. So here we are commenting in terms of what we saw in terms of Q4. It's up a bit from what we saw in Q3, and we do again expect probably going forward. I'll let Jensen answer regarding the demand for gamers as we move forward. Yes. So if you -- one way to think about the pent-up demand is we typically have somewhere between 6 to 8 weeks of inventory in the channel. And I think you would ascertain that globally right now the channel is relatively lean. We're working really hard to get GPUs down to the marketplace for the gamers, and we're doing everything we can to advise Etailers and system builders to serve the gamers. And so we're doing everything we can. But I think the most important thing is we just got to catch up with supply. Yes, it just depends on mix. I think the -- for autonomous vehicles that still have drivers, passenger cars, branded cars, ASPs anywhere from $500 to $1,000 make sense. For robot taxis, where they're driverless, they're not autonomous vehicles, they're actually driverless vehicles, the ASP will be several thousand dollars. And in terms of timing, I think that you're going to see larger and larger deployments starting this year and going through next year for sure, especially with robot taxis. And then with autonomous vehicles, cars that have autonomous driving capability, automatic driving capability starts late 2019. You could see a lot more in 2020. And just almost every premium car by 2022 will have autonomous automatic driving capabilities. Yes, thanks a lot, Toshi. First of all, just a comment about inference. The way that it works is you take the output of these frameworks. And the output of these frameworks is a really complex, large computational graph. When you think about these neural networks, and they have millions of parameters, millions of anything is very complex. And these parameters are waves and activation layers and -- activation functions, and there are millions of them. And it's millions of them that composes -- consists of this computational graph. And this computational graph has all kinds of interesting and complicated layers. And so you take this computational graph that comes out of each one of these frameworks, and they're all different. They're in different formats, they're in different styles, they have different architectures. They're all different. And you take these computational graphs, and you have to find a way to compile it, to optimize this graph, to rationalize all of the things that you could combine and fold, reduce the amount of conflict across all of the resources that are in your GPUs -- or in your processor. And these conflicts could be on-chip memory and register files and data paths, and it could be the fabric, it could be the frame buffer interface, it could be the amount of memory. I mean you got -- this computer is really complicated across all these different processors and the interconnect between GPUs, the network that connects multiple nodes. And so you've got to figure out what all these different conflicts are, resources are, and compile and optimize to take advantage of it to keep it moving all the time. And so TensorRT is basically a very sophisticated optimizing graph compilation -- graph compiler. And it targets each one of our processors. The way it targets Xavier is different to the way it targets Volta, the way it targets our inference, the way it targets for low energy, for different precisions. All of that targeting is different. And so first of all, TensorRT, the software of inference, that's really where the magic is. Then the second thing that we do, we optimize our GPUs for extremely high throughput and to support different precisions because some networks could afford to have 8-bit integer or even less, some really could barely get by with a 16-bit floating point and some, you really would like to keep it at 32-bit floating point so that you don't have to second-guess about any precision that you lost along the way. And so we created an architecture that consists of this optimizing graph, computational graph compiler, to processors that are very high throughput, that are mix precision. Okay, so that's kind of the background. We start -- we've been sampling our Tesla P4, which is our data center inference processor, and I -- we're seeing just really exciting response. And this quarter, we started shipping. We -- looking outwards, my sense is that the inference market is probably about as large in the data centers as training. And the wonderful thing is everything that you train on our processor will inference wonderfully on our processors as well. And the data centers are really awakening to the observation that the more GPUs they buy for offloading inference and training, the more money they save. And the amount of money they save is not 20% or 50%, it's factors of 10. The money savings for all of these data centers that are becoming increasingly capital constrained is really quite dramatic. And then the other inference opportunity for us is autonomous machines, which is self-driving cars. TensorRT also targets Xavier. TensorRT targets our Pegasus robot taxi computer. And they all have to inference incredibly efficiently so that we can sustain real time, keep the energy level low and keep the cost low for car companies, okay? So I think inference is a very important work for us. It is very complicated work, and we're making great progress. Just kind of curious, as you look at the Gaming business -- I've kind of lost track of what seasonality is. You clearly have a big ramp ahead of you. I'm kind of curious, as you think about Pascal versus seasonality ahead of Volta, if you can just kind of extrapolate as you look out into April and maybe July. I -- well, we haven't announced anything for April or July. And so the best way to think about that is Pascal is the best gaming platform on the planet. It is the most feature-rich, the best software, the most energy-efficient. And from $99 to $1,000, you could buy the world's best GPUs, the most advanced GPUs. And if you buy Pascal, you know you've got the best. Seasonality is a good question and increasingly because gaming is a global market and because people play games every day. It's just part of their life. There's no -- I don't think there's much seasonality in TV or books or music. People just -- whenever new titles come out, that's when a new season starts. And so in China, there's iCafes and there's Singles' Day, November 11, there's Back to School in the United States, there's Christmas, there's Chinese New Year. Boy, there are so many seasons that it's kind of hard to imagine what the exact seasonality is anymore. And so hopefully, over time, it becomes less of a matter. But the most important thing is that we expect Pascal to continue to be the world's best gaming platform for the foreseeable future. Yes, thanks a lot, Harlan. The NVIDIA TensorRT is really the only optimizing inference compiler in the world today, and it targets all of our platforms. And we do inference in the data center that I mentioned earlier. In the embedded world, the first embedded platform we're targeting is self-driving cars. In order to drive the car, you basically inference or try to predict or perceive what's around you all the time. And that's a very complicated inference matter. It could be extremely easy, like detecting the car in front of you and applying the brakes, or it could be incredibly hard which is trying to figure out whether you should stop at an intersection or not. If you look at most intersections, you can't just look at the lights to determine where do you stop. There are very few lines. And so using scene understanding and using deep learning, we have the ability to recognize where to stop and whether to stop. And then for Jetson, we have a platform called Metropolis. And Metropolis is used for very large scale smart cities where cameras are deployed all over to keep cities safe. And we've been very successful with smart cities. Just about every major smart city provider, and what is called intelligent video analysis company, whether -- almost all over the world is using NVIDIA's platform to do inference at the Edge, AI at the Edge. And then we've announced recently success with FANUC, the largest manufacturing and robotics company in the world; Komatsu, one of the largest construction equipments company in the world to apply AI at the Edge for autonomous machines. Drones, we have several industrial drones that are inspecting pipelines and inspecting power lines, flying over large spans of farms to figure out where to spray insecticides more accurately. There's all kinds of applications. So you're absolutely right that inference at the Edge or AI at the Edge is a very large market opportunity for us, and that's exactly why TensorRT was created. You had mentioned how lean the channel is in terms of gaming cards. There's been an observable increase in prices at retail. And I'm just curious, is that a broad-based phenomenon? And is there any economic ramifications to you? Or is that just sort of retailers bringing prices up in a shortage environment? We don't set prices at the end of the market. And the best way for us to solve this problem is work on demand -- excuse me, work on supply. The demand is great. And it's very likely the demand will remain great as we look throughout -- through this quarter. And so we just have to keep working on increasing supply. We have -- our suppliers are the world's best and the largest semiconductor manufacturers in the world, and they're responding incredibly, and I'm really grateful for everything they're doing. We just got to catch up to that demand which is just really great. Sure. We are -- we're just constrained. Obviously, we're 10x larger of a GPU supplier than the competition. And so we have a lot more suppliers supporting us and a lot more distributors taking our products to market and a lot more partners distributing our products all over the world. And so we -- I don't know how to explain it aside from the demand is just really great. And so we've just got to keep our nose to it and catch up to the demand. With respect to Quadro, Quadro is a workstation processor. The entire software stack is designed for all of the applications that the workstation industry uses. And it's used -- the quality of the rendering is, of course, world-class because of NVIDIA and -- but the entire software stack has been designed so that mission-critical applications or long-life industrial applications and companies that are enormous and gigantic manufacturing and industrial companies in the world could rely on an entire platform which consists of processors and system and software and middleware and all the integrations into all of the CAD tools in the world to know that the supplier is going to be here and can be trusted for the entire life of the use of that product which could be several years, but the data that is generated from it has to be accountable for a couple of decades. You need to be able to pull up an entire design of a plane or a train or a car a couple decades after it was sent to production to make sure that it's still in compliance, and if there are any questions about it, that it can be pulled up. NVIDIA's entire platform was designed to be professional class, professional grade, long lived. Now the thing that's really exciting about artificial intelligence is we now can use AI to improve images. Like, for example, you could fix a photograph using AI. You could fill in damaged parts of a photograph or parts of the image that hasn't been rendered yet, you want to use AI to fill in the dots, predict the future, rendering results, which we announced and which we demonstrated at GTC recently. You could use that to generate designs. You sketch up a few strokes of what you want a car to look like. And based on the inventory, safety, physics, it could -- it has learned how to fill in the rest of it, okay, design the rest of the chassis on your behalf. It's called generative design. We're going to see generative design in product design, in building design and just about everything. The last, if you will, 90% of the work is after the initial inspiration or the conceptual design is done. That part of it can be highly automated through AI. And so Quadro could be used as a platform that designs as well as generatively designs. And then lastly, a lot of people are using our workstations to also train their neural networks for these generative designs. And so you could train and develop your own networks and then apply it in the applications, okay? So AI, think of AI really as, in the final analysis, the future way of developing software. It's a brand-new capability where computers can write its own software. And the software that's written is so complex and so capable that no humans could write it ourselves. And so you could teach, you could use the data to teach a software to figure out how to write the software by itself. And then when you're done developing that software, you could use it to do all kinds of stuff, including design products. And so for workstations, that's how it's used. Congratulations on the very good quarterly execution. A lot of near-term items here on gaming. So I'll switch it to longer term. Jensen, at CES, I think you said that there are now 200 million GeForce users globally. And if my math is correct, then that would be up about 2x over the last 3 to 4 years. So the question is, is there anything that you can see that would preclude that kind of growth over a similar period? And given the recent demand dynamics, I think we've seen that NVIDIA's direct channels have been very good sources for GPUs at the prices that you intend. So as we look ahead, should we expect any change in channel management from the company? Yes. Thanks a lot, Craig. In the last several years, several dynamics happened at the same time. And all of it were the favorable contributions to today. First of all, gaming became a global market, and China became one of the largest gaming markets in the world. But second, because the market became so big, developers could invest extraordinary amounts into the production value of a video game. They could invest a few hundred million dollars and know that they're going to get the return on it. Back when the video game industry was quite small or when PC industry -- PC gaming was small, it was too risky for a developer to invest that much. And so now an investor, a developer could invest hundreds of millions of dollars and create something that is just completely photorealistic and immersive and just beautiful. And so the production -- when a production value goes up, the GPU technology that's needed to run it well goes up. It's very different than music, it's very different than watching movies. Everything in video games is synthesized in real time. And so when the production value goes up, the ASP or the technology has to go up. And then lastly, the size of the market, people have wondered how big the video game market is going to be. And I've always believed that the video game market is going to be literally everyone. In 10 years' time, 15 years' time, there's going to be another 1 billion people on Earth. And those people are going to be gamers. We're going to see more and more gamers. And not to mention that, almost every single sport could be a virtual-reality sport. So video games is every sport. So eSport can be any sport and every sport and every type of sport. And so I think when you consider this and put that in your mind, I think the opportunity for video games is going to be quite large, and that's essentially what we are seeing. Yes. Thanks a lot, Will. I wish I had more precision for you, but here are some of the dynamics that I believe in. I believe that autonomous capabilities -- autonomous driving, is the single greatest dynamic next to EVs in the automotive industry. And transportation is a $10 trillion industry. Between cars and shuttles and buses, delivery vehicles, I mean, it's just an extraordinary, extraordinary market. And everything that's going to move in the future will be autonomous. That's for sure. And it will be autonomous fully, or it will be autonomous partly. The size of this marketplace is quite large. In the near term, I -- our path to that future, which I believe starts in 2020 -- 2019, 2020, but starts very strongly in 2022, I believe the path to that, in our case, has several elements. The first element is that in order for all these companies, whether they're Tier 1s or startups or OEMs or taxi companies or ride-hailing companies or tractor companies or shuttle companies or pizza delivery shuttles, in order to deliver -- in order to create their autonomous driving capability, the first thing you have to do is train a neural network. And we've created a platform we call the NVIDIA DGX that allows everybody to train their neural networks as quickly as possible. So that's first. The development of the AI requires GPUs, and we benefit first from that. The second is -- which we'll start this year and next year, is development platforms for the cars themselves for the vehicles themselves. And finally, Xavier's here. We have first silicon at Xavier's, the most complex SOC that was ever made. And we're super excited about the state of Xavier, and we're going to be sampling it in Q1. And so now we'll be able to help everybody create development systems. And there'll be thousands and tens of thousands of quite expensive development systems based on Xavier and based on Pegasus that the world is going to need. And so that's the second element. The third element, in the near term, will be development agreements. Each one of these projects are engineering-intensive, and there's a development agreement that goes along with it. And so these 3 elements, these 3 components, are in the near term. And then hopefully, starting from 2019, going forward and very strongly going from 2022 and beyond, the actual car revenues and economics will show up. Appreciate that question. And I think this is our last question, yes? Well, we had a record quarter, wrapping up a record year. We have a strong -- we had strong momentum in our gaming, AI, data center and self-driving car businesses. It's great to see adoption of NVIDIA's GPU computing platform increasing in so many industries. We accomplished a great deal this last year, and we have big plans for this coming year. Next month, the brightest minds in AI and the scientific world will come together at our GPU Technology Conference in San Jose. GTC has grown tenfold in the last 5 years. This year we expect more than 8,000 attendees. GTC is the place to be if you're an AI researcher or doing any field of science where computing is your essential instrument. There will be over 500 hours of talks of recent breakthroughs and discoveries by leaders in the field, such as Google, Amazon, Facebook, Microsoft and many others. Developers from industries ranging from health care to transportation to manufacturing and entertainment will come together and share state-of-the-art and AI. This is going to be a great GTC. I hope to see all of you there.","I guess first question, when I think about normal seasonality for gaming, that would imply data center potentially north of $700 million-plus into the coming quarter. And so curious if I'm thinking about that right or whether crypto is being modeled more conservatively by you guys, and so would love to hear your thoughts there. Yes? Sorry. Well, just curious to hear your thoughts there. Okay. And then I guess as part of a larger question, how are you thinking about seasonality for gaming into the ensuing quarter? The first question, the checks we've done indicate that the Tensor Cores you put into Volta give it a huge advantage in neural network applications in the data center. And I'm wondering whether the Tensor Cores might also have a similar kind of utility in the gaming market. I have a question for Colette. So if I correct for the Switch revenue growth in the quarter, it means the gaming business [x], which was up, I don't know maybe $140 million, $150 million. In your Q3 commentary, you did not call out crypto as a driver, you are calling it out in Q4. Is it fair to say that like that incremental growth is all crypto? And I guess going forward, you mentioned pent-up demand. Normally, your seasonality for gaming will be down probably double digits. Do you think that pent-up demand is enough to reverse that normal seasonal pattern -- or normally down? And frankly, do you think gamers can even find GPUs at retail at this point to buy in order to satisfy that pent-up demand? I actually want to circle back on the autos, since I was at CES. So it's still kind of on track for calendar -- towards calendar year '19, at the end of that, where we see the autonomous kind of ASP uplift. And just to clarify, the expected ASP uplift is somewhere around $1,000. Is that about right? Great. Jensen, I was hoping to ask a little bit about inferencing. How big was inferencing within data center in Q4 or fiscal '18? And more importantly, how do you expect it to trend over the next 12 to 18 months? Congratulations on the solid results and the execution. I know somebody asked a question about inferencing for the data center markets. But on inferencing for embedded and Edge applications, on the software and firmware side, you talked about TensorRT framework; on the hardware side, you've got the Jetson TX platform; for embedded and Edge inferencing applications, things like drones and factory automation and transportation. What else is the team doing in the embedded market to capture more of the TAM opportunity there going forward? Great quarter. So just to clarify, Jensen, on pent-up demand. One of your GPU competitors basically said that the constraint was memory. I just want to make sure that, that was correct. And then in the CFO commentary, you mentioned opportunities for professional vis, like AI and deep learning. Can you talk about that, and what kind of applications you would use, Quadro versus Volta or GeForce? I'm hoping we can touch on automotive a little bit more. In particular, I think, in the past, you've talked about expecting sort of a low revenue growth in this market until roughly the 2020 time frame when autonomous driving kicks in, in a more meaningful way. But of course, you have the AI copilot that seems to be potentially ramping sooner, and you have at least 1 marquee customer that is ramping now, I guess, but volumes aren't quite that large on the autonomous driving side. So any guidance as to when we might see these 2 factors start to accelerate revenue in that end market? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q1 2019,2186,5410,1371,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the First Quarter of Fiscal 2019. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until May 16, 2018. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2019. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10-K, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 10, 2018, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO Commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. We had an excellent quarter, with growth across all our platforms, led by gaming and data center. Q1 revenue reached a record $3.21 billion, up 66% year-on-year, up 10% sequentially, and above our outlook of $2.9 billion. Once again, all measures of profitability set records, with GAAP gross margins at 64.5%, operating margins at 40.4%, and net income at $1.24 billion. From a reporting segment perspective, Q1 GPU revenue grew 77% from last year, to $2.77 billion. Tegra Processor revenue rose 33% to $442 million. Let's start with our gaming business. Revenue was $1.72 billion, up 68% year-on-year and down 1% sequentially. Demand was strong and broad-based across regions and products. The gaming market remains robust and the popular Battle Royale genre is attracting a new wave of gamers to the GeForce platform. We also continued to see demand from upgrades, with about 35% of our installed base currently on our Pascal architecture. The launch of popular titles, like Far Cry 5 and FANTASY -- FINAL FANTASY XV, continued to drive excitement in the quarter. Gamers are increasingly engaging in social gameplay and gaming is rapidly becoming a spectator sport, while the production value of games continues to increase. This dynamic is fueling a virtuous cycle that expands the universe of gamers and drives a mix shift to higher end GPUs. At the recent Game Developers Conference, we announced our real-time ray-tracing technology, NVIDIA RTX. Ray tracing is movie-quality rendering technique that delivers lifelike lighting, reflections and shadows. It has long been considered the Holy Grail of graphics and we've been working on it for over 10 years. We look forward to seeing amazing, cinematic games that take advantage of this technology come to the market later this year, with the pipeline building into next year and beyond. And we expect RTX, as well as other new technologies like 4K and virtual reality, to continue driving gamers' requirements for higher GPU performance. While supply was tight earlier in the quarter, the situation is now easing. As a result, we are pleased to see that channel prices for our GPUs are beginning to normalize, allowing gamers who had been priced out of the market last quarter to get their hands on the new GeForce GTX at a reasonable price. Cryptocurrency demand was again stronger than expected, but we were able to fulfill most of it with crypto-specific GPUs, which are included in our OEM business, at $289 million. As a result, we could protect the vast majority of our limited gaming GPU supply for use by gamers. Looking into Q2, we expect crypto-specific revenue to be about 1/3 of its Q1 level. Gaming notebooks also grew well, driven by an increasing number of thin and light notebooks based on our Max-Q design. And Nintendo Switch contributed strongly to year-on-year growth, reflecting that platform's continued success. Moving to data center. We had another phenomenal quarter, with revenue of $701 million, up 71% year-on-year, up 16% sequentially. Demand was strong in all market segments, and customers increasingly embraced our GPUs and CUDA platform for high-performance computing and AI. Adoption of our Volta architecture remained strong across a wide range of verticals and customers. In the public cloud segment, Microsoft Azure announced general availability of Tesla V100 instances, joining Amazon, IBM and Oracle. And Google Cloud announced that the V100 is now publicly available in beta. Many other hyperscale and consumer Internet companies also continued their ramp of Volta, which delivers 5x the deep learning performance of its predecessor, Pascal. Volta has been chosen by every major cloud provider and server maker, reinforcing our leadership in AI deep learning. In high-performance computing, strength from the broad enterprise vertical more than offset the ramp down of major supercomputing projects such as the U.S. Department of Energy's Summit System. We see a strong pipeline across a number of vertical industries from manufacturing to oil and gas, which has helped sustain the trajectory of high-performance computing next quarter and beyond. Traction is also increasing in AI inference. Inference GPU shipments to cloud service providers more than doubled from last quarter. And our pipeline is growing into next quarter. We dramatically increased our inference capabilities with the announcement of the TensorRT 4 AI Inference Accelerator Software at our recent GPU Technology Conference in San Jose. TensorRT 4 accelerates deep learning inference up to 190 times faster than CPUs for common applications such as computer vision, neural machine translation, automatic speech recognition, speech synthesis and recommendation systems. It also dramatically expands the use cases prepared with the prior version. With TensorRT 4, NVIDIA's market reach has expanded to approximately 30 million hyperscale servers worldwide. At GTC, we also announced other major advancements in our deep learning platform. We doubled the memory of Tesla V100 to 32 GB DRAM, which is a key enabler for customers building large neural networks through larger data sets. And we announced a new GPU interconnect fabric called NVIDIA NVSwitch. (inaudible) 16 Pascal V100 GPUs at a speed of 2.4 terabytes per second, or 5x faster than the best PCIe switch. We also announced our DGX-2 system, which leverages these new technologies and its updated, fully-optimized software stack to deliver a 10x performance boost beyond last year's DGX. DGX-2 is the first single server capable of delivering 2 petaflops of computational power. We are seeing strong interest from both hyperscale and (inaudible) customers, and we look forward to bringing this technology to cloud customers later this year. At our Investor Day in March, we updated our forecast for the data center and the rest of the market. We see the data center opportunity as very large, fueled by growing demand for accelerated computing and applications ranging from AI (inaudible) multiple market segments and vertical industries. We estimate the TAM at $50 billion by 2023, which extends our previous forecast of $30 billion by 2020. We see strong momentum in the adoption of our accelerated computing platform and the expansion of our development ecosystem to serve this rapidly growing market. About 8,500 attendees registered for GTC, up 18% from last year. CUDA downloads have continued to grow, setting a fresh record in the quarter. And our total number of developers is well over 850,000, up 72% from last year. Moving to pro visualization. Revenue grew to $251 million, up 22% from a year ago and accelerating from last quarter, driven by demand for real-time rendering as well as emerging applications like AI and VR. Strength extended across several key industries, including public sector, health care and retail. Key wins in the quarter included Columbia University, using high-end Quadro GPUs for AI, and Siemens, using them for CT and ultrasound solutions. At GTC, we announced the Quadro GV100 GPU with NVIDIA RTX technology, capable of delivering real-time ray tracing to the more than 25 million artists and designers throughout the world. RTX makes computational intensive ray tracing possible in real time when running professional design and content creation applications. This allows media and entertainment professionals to see and interact with their creations with correct light and shadows and do complex renders up to 10x faster than a GPU -- a CPU alone. And the NVIDIA OptiX AI denoiser built into RTX delivers almost 100x the performance of CPUs for real-time noise-free rendering. This enabled customers to replace racks of servers in traditional render farms with GPU servers at 1/5 the cost, 1/7 the space, and 1/7 the power. Lastly, automotive. Revenue grew 4% year-on-year to a record $145 million. This reflects the ongoing transition from our infotainment business to our growing autonomous vehicle development and production opportunities around the globe. At GTC and Investor Day, we made key product announcements on the advancement of autonomous vehicles and established a total addressable market opportunity of 60 billion by 2035. We believe that every vehicle will be autonomous one day. By 2035, this will encompass 100 million autonomous passenger vehicles and 10 million robo-taxis. We also introduced NVIDIA DRIVE Constellation, a platform that will help car companies, carmakers, tier 1 suppliers, and others developing autonomous vehicles test and validate their systems in a virtual world across a wide range of scenarios before deploying on the road. Each year, 10 trillion miles are driven around the world. Even if test cars can eventually cover millions of miles, that's an insignificant fraction of all the scenarios that require testing to create a safe and reliable autonomous vehicle. DRIVE Constellation addresses this challenge by (inaudible) cars to safely drive billions of miles in virtual reality. The platform has 2 different servers. The first is loaded with GPUs and simulates the environment that the car is driving in, as in a hyper-real video game. The second contains the NVIDIA DRIVE Pegasus Autonomous Vehicle Computer, which possesses the simulated data, as if it were coming from the sensors of a car driving on the road. Real-time driving command from the DRIVE Pegasus are fed back to the simulation for true hardware-in-the-loop verification. Constellation will enable autonomous vehicle industry for safety test and validate their AI self-driving systems in ways that are not practical or possible with on-road testing. We also extended our product roadmap to include our next-generation DRIVE Autonomous Vehicle Computer. We have created a scalable AI car platform that spans the entire range of autonomous driving, from traffic jams, pilots, to level 5 robo-taxis. More than 370 companies and research institutions are now using NVIDIA's automotive platform. With this growing momentum, we remain excited about the intermediate and long-term opportunities for autonomous driving business. Now moving to the rest of the P&L. Q1 GAAP gross margins were 64.5% and non-GAAP was 64.7%, records that reflect continued growth in our value-added platforms. GAAP operating expenses were $773 million. Non-GAAP operating expenses were $648 million, up 25% year-on-year. We continue to invest in key platforms driving our long-term growth, including gaming, AI and automotive. GAAP net income was a record $1.24 billion and EPS was $1.98, up 145% and 151% respectively from a year earlier. Some of the expenses (inaudible) by a tax rate of 5% compared to our guidance of 12%. Non-GAAP net income was $1.29 billion and EPS was $2.05, both up 141% from a year ago, reflecting the revenue strength as well as gross margins and operating margin expansion on slightly lower tax. Our quarterly cash flow from operations reached record levels at $1.45 billion. Capital expenditures were $118 million. With that, let me turn to the outlook for the second quarter of fiscal 2019. We expect revenue to be $3.1 billion plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 63.6% and 63.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $810 million and $685 million, respectively. GAAP... (technical difficulty) Capital expenditures are expected to be approximately $130 million to $150 million. Further financial details are included in the CFO Commentary and other information available on our IR website. In closing, I'd like to highlight a few upcoming events for the financial community. We'll be presenting at the JPMorgan Technology Conference next week on May 15, and at the Bank of America Global Technology Conference on June 5. We will also hold our Annual Meeting of Stockholders online on May 16. We will now open the call for questions. Simona and I are here in Santa Clara and Jensen is dialing in from the road. Operator, would you please poll for questions? Thank you.","Jensen, why don't you start on the question for Stacy, and I'll follow-up afterwards, after you speak. Okay. Stacy, so let's see. Q1, as you probably know, Fortnite and PUBG are global phenomenons. The success of Fortnite and PUBG are just beyond, beyond comprehension, really. Those 2 games, a combination of Hunger Games and Survivor, has just captured imaginations of gamers all over the world. And we saw the uptick and we saw the demand on our GPUs from all over the world. Surely, there was scarcity as you know. Crypto miners bought a lot of our GPUs during the quarter, and it drove prices up. And I think that a lot of the gamers weren't able to buy into the new GeForces as a result. And so we're starting to see the prices come down. We monitor spot pricing every single day around the world. And the prices are starting to normalize. It's still higher than where they should be. And so obviously, the demand is still quite strong out there. But my sense is that there's a fair amount of pent-up demand still. Fortnite is still growing in popularity. PUBG is doing great. And then, we've had some amazing titles coming out. And so my sense is that the overall gaming market is just really -- is super healthy. And our job is to make sure that we work as hard as we can to get supply out into the marketplace. And hopefully, by doing that, the pricing will normalize and the gamers can buy into their favorite graphics card at a price that we hope they can get it at. And so I think there's a fair -- so I mean, the simple answer to your question is Fortnite and PUBG. And the demand is just really great. They did a great job. No wonder -- Colette had talked about the inference doubling in sales quarter-over-quarter with cloud. Can you just talk about where you're seeing the early applications for inference? Is that sort of as-a-service business? Or are you looking at internal cloud workloads? And just any color you can give us on where you guys are sitting in the inference space. Sure. Joe, so as you know, there are 30 million servers around the world. And they were put in place during the time when the world didn't have deep learning. And now with deep learning and with machine learning approaches, the accuracy of prediction, the accuracy of recommendation has jumped so much that just about every Internet service provider in the world that has a lot of different customers and consumers are jumping onto this new software approach. And in order to take this newer network -- and the software that's written by deep learning, these frameworks, are massive software. The way to think about these deep neural nets is, it has millions and millions and millions of parameters in it, and these networks are getting larger every year. And they're enormously complex. And the output of these neural nets had to be optimized for the computing platform that it targets. How you would optimize the neural network for a CPU or a GPU is very, very different. And how you optimize for different neural networks, whether it's image recognition, speech recognition, natural language translation, recommendation systems, all of these networks have different architectures, and the optimizing compiler that's necessary to make the neural network run smoothly and fast is incredibly complex. And so that's why we created TensorRT. That's what TensorRT is. TensorRT is an optimizing graph neural network compiler. And it optimizes for our -- each one of our platforms. And each -- even each one of our platforms has very different architectures. For example, we invented recently -- reinvented the GPU and it's called the Tensor Core GPU, and the first of its kind is called Volta. And so TensorRT 4.0 now supports, in addition to image recognition, all of the different types of neural network models. The answer to your question is internal consumption. Internal consumption is going to be the first users. Video recognition, detecting for inappropriate video, for example, all over the world, making recommendations from the videos that you search or the images that you're uploading, all of these types of applications are going to require an enormous amount of computation. Jensen, I have 2 questions about the data center. One from a growth, and the second, from a competition perspective. So from the growth side, you guys are doing about, say, $3 billion or so annualized, but you have outlined a market that could be $50 billion. What needs to happen for the next inflection? Is it something in the market that needs to change? Is it something in the product set that needs to? How do you go and address that $50 billion market, right? Because you're only a few percent penetrated today in that large market. So what needs to change for the next inflection point? And then, on the competition side, as you are looking at that big market, how should we think about competition that is coming from some of your cloud customers, like a Google announcing a TPU 3 or perhaps others looking at other competing technologies? So any color on both sort of how you look at growth and competition would be very helpful. Thanks, Vivek. First of all, at its core, this is something we all know now, that CPU scaling has really slowed. And if you think about the several hundred billion dollars worth of computer equipment that's been installed in the cloud, in data centers all over the world, and as these applications for machine learning and high-performance computing approaches come along, the world needs a solution. CPU scaling has slowed. And so here's the approach that we pioneered 1.5 decades ago called GPU computing. And we've been determined to continue to advance it during this time because we saw this day coming and we really believed that it was going to end. I mean, you can't deny physics. And so we find ourselves in a great position today. And as Colette already mentioned, we have something close to 1 million developers on this platform now. It is incredibly fast, speeding up CPUs by 10, 20, 50, 100x, 200x sometimes, depending on the algorithm. It's everywhere. The software ecosystem is just super rich. And as Colette mentioned, that there is already almost 1 billion -- 1 million developers around the world, that's grown 70% year-over-year. And so I think at the core, it's about the fact that the world needs a computing approach going forward. With respect to the -- our ability to address the TAM, there are 3 major segments. There's more than that, but there's 3 major segments. One is, of course, training for deep learning. The other is inferencing, and TRT 4 is intended to do just that, to expand our ability to address all of the different types of algorithms, machine learning algorithms that are now coming -- that are running in the data centers. The third is high-performance computing, and that's molecular dynamics, to medical imaging, to earth sciences, to energy sciences. The type of algorithms that are being run in supercomputers all over the world is expanding. And we're doing more and more of our product designs in virtual reality. We want to simulate our products and simulate its capabilities in simulation in this computer rather than build it in the beginning. And then, the last category would be graphics virtualization. We've taken with GRID and our Quadro virtual workstation and now with Quadro -- with NVIDIA RTX, we turned the data center into a powerful graphic supercomputer. And so these are the various applications and segments of data center that we see. I think, in the case of training, we're limited by the number of deep learning experts in the world. And that's growing very quickly. The frameworks are making it easier. There's a lot more open source and open documentation on sharing of knowledge. And so the number of AI engineers around the world is growing super fast. The second is inference. And I've already talked about that. It's really limited by our optimizing compilers and how we can target these neural network models to run on our processors. And if we could do so, we're going to save our customers enormous amounts of money. We speed up applications, we speed up these neural network models 50x, 100x, 200x over a CPU. And so the more GPUs they buy, the more they're going to save. And high-performance computing, the way to think about that is, I think, at this point, it's very clear that going forward, supercomputers are going to get built with accelerators in them. And because of our long-term dedication to CUDA and our GPUs for acceleration, of all these codes and the nurturing of the ecosystem, I think that we're going to be -- we're going to do super well in the supercomputing world. And so these are the different verticals. With respect to competition, I think it all starts with the core. And the core is that the CPU scaling has slowed. And so the world needs another approach going forward. And surely, because of our focus on it, we find ourselves in a great position. Google announced GPU 3 and it's still behind our Tensor Core GPU. Our Volta is our first generation of a newly reinvented approach of doing GPUs. It's called Tensor Core GPUs. And we're far ahead of the competition. And -- but more than that, more than that, it's programmable. It's not one function. It's programmable. Not only is it faster, it's also more flexible. And as a result of the flexibility, developers could use it in all kinds of applications, whether it's medical imaging or weather simulations or deep learning or computer graphics. And as a result, our GPUs are available in every cloud and every data center, everywhere on the planet. And which developers need, so that accessibility so that they could develop their software. And so I think that on the one hand, it's too simplistic to compare a GPU to just one of the many features that's in our Tensor Core GPU. But even if you did, we're faster. We support more frameworks. We support all neural networks. And as a result, if you look at GitHub, there are some 60,000 different neural network research papers that are posted that runs on NVIDIA GPUs. And it's just a handful for the second alternative. And so it just kind of gives you a sense of the reach and the capabilities of our GPUs. Yes. Thanks for the question, Toshiya. At the core, the program was about making sure that gamers who buy graphics cards knows exactly the GPU brand that's inside. And the reason for that is because we want gamers to -- the gaming experience of a graphics card depends so much on the GPU that is chosen. And we felt that using one gaming brand, a graphics card brand and interchanging the GPU underneath causes it to be less -- causes it to be more opaque and less transparent for gamers to choose the GPU brand that they wanted. And most of the ecosystem loved it. And some of the people really disliked it. And so instead of all that distraction, we're doing so well, and we're going to continue to help the gamers choose the graphics cards like we always have, and things will sort out. And so we decided to pull the plug because the distraction was unnecessary and we had too much good stuff to go do. Yes. Thanks for the question. HPC. First of all, at the core, CPU scaling has stalled and it's reached the limits of physics. And the world needs another approach to go forward. We created the GPU computing approach 1.5 decades ago, and I think at this point, with the number of developers jumping on, the number of applications that's emerging, it's really clear that the future of HPC has accelerated. And our GPU approach, because of its flexibility, because of its performance, because of the value that we create, that as a result of the throughput of a data center, we save people so much money just in cables alone. Oftentimes, more than pays for the GPUs that they buy. And the reason for that is because the number of servers is reduced dramatically. And so I think the future of HPC is about acceleration, and the NVIDIA CUDA GPUs are really in a great position to serve this vacuum that's been created. With respect to benchmarks, you might have seen that earlier this week, we released 3 speed records. The fastest single GPU, the fastest single node -- single computer node. A definition of a computer node is something that fits in a box that runs one operating system, one node. And one instance, one cloud instance. We now have the fastest speed record for 1 GPU, 1 node and 1 instance. And so we love benchmarks. Nothing is more joyful than having a benchmark to demonstrate your leadership position. And in the world of deep learning, the number of networks is just growing so fast because the number of different applications that deep learning is able to solve is really huge. And so you need a lot of software capability and you're -- the versatility of your platform needs to be great. We also have a lot of expertise in the company and software. I mean, NVIDIA is really a full-stack computing company. From architecture, to system software, to algorithms, to applications, we have a great deal of expertise across the entire stack. And so we love these complicated benchmarking -- benchmarks that are out there, and I think this is a great way for us to simplify our leadership position. I think long term, the number of networks that are going to emerge will continue to grow. And so the flexibility of ASICs is going to be its greatest downfall. And if someone were to create a general-purpose parallel accelerating processor like ours, and had it designed to be incredibly good at deep learning, like recently what we did with our Tensor Core GPU, which is a reinvented GPU, and Volta is the first one, I -- it's going to be hard. It's going to be expensive, and we've been doing it for a long time. And so I think this is a -- it's a great time for us. Jensen, I wanted to ask on the inference side [about edge inference]. And beyond autos, when you look at sizing that TAM, what are the other big areas that you think you can penetrate with GPUs and inference, besides auto? Yes, Blayne. The largest inference opportunity for us is actually in the cloud and the data center. That's the first great opportunity. The reason for that is there's just an explosion in the number of different types of neural networks that are available. There are image recognition, there's video sequencing, there's video -- there's recommender systems. There's speech recognition, speech synthesis, natural language understanding. There's just so many different types of neural networks that are being created. And creating one ASIC that can be adapted to all of these different types of networks is just a real challenge. And by the time that you create such a thing, it's called a Tensor Core GPU, which is what we created. And so I think the first opportunity for us in -- large-scale opportunity will be in the data center and the cloud. The second will be in vertical markets. The vertical market that you mentioned is self-driving cars. And we see a great opportunity in autonomous vehicles, both in the creation of autonomous vehicles. And I mentioned that before, between now and the time that we ramp our AV computers we call DRIVE, we're going to be selling a whole lot of servers so that the companies could develop their neural network models for their self-driving cars as well as simulating the virtual reality -- in virtual reality, there are various test drives as well as testing their neural network and their self-driving car stack against billions and billions of miles of saved-up prerecorded videos. And so in the vertical markets, we're going to see inference both in the data center for developing the self-driving car stack as well as in the self-driving cars themselves. Now in the self-driving cars, the ASPs for level 2 could be a few hundred dollars to a level 5 self-driving car, taxi or driverless taxi being a few thousand dollars. And I expect that driverless taxis will start going to market about 2019, and self-driving cars probably somewhere between 2020 and 2021. And I think the size of the market is fairly well modeled. And the simple way to think about that is, I believe that every single -- everything that moves someday will be autonomous or have autonomous capabilities. And so the 100 million cars, the countless taxis, all the trucks, all the agriculture equipment, all the pizza delivery vehicles, you name it, everything is going to be autonomous. And the market opportunity is going to be quite large. And that's the reason why we're so determined to go create that market. We are expecting the -- we are expecting Q2 to be better than seasonality, if I understand your question. It should be -- we're expecting Q2 to be better than Q1 and we're expecting Q2 to be better than seasonality. Did that answer your question? Thanks so much for the question. When you think about our gross margins, just over this last quarter, as you know, we were working on stabilizing the overall supply that was out there in the market for consumer GPUs. We benefited from that with a higher gross margin as we filled and completed that. You've seen us absorb a significant amount of the component pricing changes that we have seen, particularly around the memory. We're not here to be able to forecast, generally, when those pricing of those components will stabilize. But we believe in terms of the value added that our platforms provide, the components are an important part of finishing that. But I think we have a tremendous amount more value that we are adding in terms of the software on top of our platforms, which is enabling our gross margins. Yes. So first of all, Volta is a reinvented GPU. Volta is the world's first GPU that has been designed to be incredibly good at deep learning. We call it the Tensor Core GPU. It has still retained all of the flexibilities of all -- everything that CUDA has ever run is backwards compatible with everything that runs on CUDA. But it has new architectures designed to be incredibly good at deep learning. We call it a Tensor Core GPU. And that's the reason why it has all of the benefits of our GPU, but none of the ASICs can catch up to it. And so Volta is really a breakthrough. We're going to be very successful with Volta. Every cloud will have it. The initial deployment is for internal consumption. Volta has been shipping to the cloud service -- cloud providers, the Internet service companies, for the vast majority of last quarter, as you guys know. And they're using it internally. And now they're starting to open up Volta for external consumption, their cloud customers. And they are moving as fast as they can. And my expectation is that you're going to see a lot more coming online this quarter. Colette, could you give me a brief version of that? It was kind of crackling on my side. I'm going to ask the operator if they could ask for the question again because it was also, on our side, a little crackly. Yes. Much better, Mark. I see. Thank you. DGX-2 and DGX-1 will both be in the market at the same time. And DGX is a few hundred million dollar business. It's -- it was introduced last year. So its growth rate is obviously very high. It's designed for enterprises where they don't -- they need to have their computers on-premise, but they don't want to build a supercomputer and they don't have the expertise to do so. And they would like to pull a supercomputer out of a box, plug it in, and start doing supercomputing. And so DGX is really designed for enterprises. It's designed for car companies. It's designed for health care companies doing life sciences work or medical imaging work. We recently announced a project called Project Clara, which basically takes medical imaging equipment, virtualizes them, containerizes the software, and turns it into a -- and most medical imaging equipment today are computational and they -- a lot of them run on NVIDIA CUDA anyways. And so we put that -- we can put that into the data center, we can virtualize their medical instruments and it gives them the opportunity to upgrade the millions of instruments that are out in the marketplace today. And so DGX is really designed for enterprises and we're seeing great success there. It's really super easy to use and it comes with direct support from HPC and AI researchers at NVIDIA. And the answer to your question at the end is, both of them will be in the marketplace at the same time. Sure. I'll take that question. Generally, our OEM business can be a little bit volatile. Because remember, OEM business incorporates our mainstream GPUs as well as our Tegra integrated. So we have development platforms that we sell on some of the Tegra piece of it. But they are slightly below, and I think you can go back and refer to our discussion at Investor Day as there's a slide there that talks about those embedded pieces and them being below. So yes, you're correct. Again, a very small part of our business right now. Well, we try to as transparently reveal our numbers as we can. And so the thing that we -- our strategy is to create a SKU that allows the crypto miners to fulfill their needs, and we call it CMP, and to the -- as much as possible, fulfill their demand that way. Sometimes it's just not possible because the demand is too great. And -- but we try to do so. And we try to keep the miners on the CMP SKUs as much as we can. And so I'm not exactly sure how other people do it, but that's the way we do it. Jensen, I just wanted to come back to an announcement that you made at GTC with ray tracing. Because the technology looked like it was very high fidelity, and I think you noted at that time that it was very computationally intensive. So the question is, as we think about the gaming business and the potential for ray tracing to enter that platform group, what does it mean for dynamics that we've seen in the past, for example, the ability to really push the high end of the market with high-end capability, 1070. Ti launched late last year. It was very successful. Does this give you further flexibility for those types of launches as you bring exciting and very high-end technology to market? Yes. I appreciate it. NVIDIA RTX is the biggest computer graphics invention in the last 15 years. It took us 1 decade to do. We literally worked on it continuously for 1 decade. And to put it into perspective, it's basically film rendering, cinematic rendering, except it's in real time. It merges the style of computer graphics, rasterization, and light simulation, what people call ray tracing, as well as deep learning and AI, merged it into one unified framework so that we can achieve cinematic rendering in real time. What it currently takes is a server about a few hours, depending on the scene, it might take as long as a full day, take a few hours to render one frame. So it takes a server, one node of a server, several hours to render one frame. And in order to render 30 frames per second, just imagine the number of servers you need. If it takes several hours per frame, and you need to render 30 frames per second in order to be real-time, it basically takes a high-performance computer, a supercomputer, a render farm, that's why they call it a render farm, it's a full data center designed just for rendering. And now we've created NVIDIA RTX which makes it possible to do in real time. We demonstrated RTX on 4 Quadro GV100s. It takes 4 of our latest generation Volta Tensor Core GPUs to be able to render 30 frames per second, the Star Wars cinematic that people enjoyed. And so the amount that we saved, we basically took an entire data center, reduced it into one node. And we're now doing it in real time. And so the amount of money that we can save people who create movies, people who do commercials, people who use film rendering to create the game content, almost every single game is done that way. There's quite a bit of offline rendering to create the imagery and the textures and the lighting. And then, of course, architectural design and car design, the number of applications, the number of industries that are built on top of modern computer graphics is really quite large. And I'm certain that NVIDIA RTX is going to impact every single one of them. And so that's our starting point, is to dramatically reduce the cost of film rendering, dramatically reduce the time that it takes to do it, and hopefully, more GPU servers will get -- will be purchased. And of course, better content will be created. Long-term, we've also now plotted the path towards doing it in real time. And someday, we will be able to put RTX into a GeForce gaming card and the transformation to the revolution to the gaming industry will be quite extraordinary. So we're super excited about RTX. Sure, Stacy. Let me see if I can bridge together Jensen and then some comments here. Unfortunately, they are moving quite fast to the next question, so I wasn't able to add on. But let me see if I can add on here and provide a little bit of clarity in terms of the seasonality. Remember, in Q1, we outgrew seasonality significantly. We left Q4 with very low inventory in terms of the channel. We spent Q1 working on establishing a decent amount of inventory available. We wanted to concentrate on our miners separately, and then you can see we did that in terms of Q1 by moving that to OEM and moving that to cryptocurrency-only boards. So we left Q1 at this point with healthy overall channel inventory levels as far as where we stand. So that then takes you now to Q2. But if we overshot in terms of seasonality in terms of Q1, we don't have to do those channel-fill dynamics again as we get into Q2. But we do have demand out there for our gamers that we can now address very carefully with the overall inventory that we now have available. So putting together Q1 and Q2 together, yes, we are within normal seasonality, again, for our guidance. And we'll see how we'll finish in terms of the quarter. But you should be in that range. So yes, from a normal seasonality, at a year-to-date inclusive of Q2, yes, we're on that overall seasonality. Always keep in mind, generally, our H2s are usually higher than our overall H1s, and that's what you should think about our overall guidance. Gaming is still strong. We have to comment that our overall drivers that have taken us to this place over the last 3 to 5 years with phenomenal growth and our ability to grow that overall market is still here and all of those things are together. We've just had a few quarters in terms of making sure that we get the overall channel correct and put our miners separately. I hope that clarifies in terms of where we are, in terms of gaming seasonality. Let me start off here, and I'll have Jensen finish up on the last part of that question. But overall, our data center business did phenomenal. Volta is doing extremely well. And even now with 32-bit, we're seeing tremendous adoption throughout. Again, remember it's very different than the overall consumer business. You have significant amount of time for qualification, and that is moving extremely fast based on a lot of other industries and their ability to qualify. So no, there is not a supply challenge at all in terms of our data center. And our overall growth in data center, we're extremely pleased with in terms of how the quarter came out. I'll turn it over to you, Jensen, and you can answer the rest of the part of it. Yes. The reason why miners love GeForce is because miners are everywhere in the world. One of the benefits of cryptocurrency is that it's not any sovereign currency. And it's, in the digital world, it's distributed. And GeForce is the single largest distributed supercomputing infrastructure on the planet. Every gamer has a supercomputer in their PC. And GeForce is so broadly distributed, it's available everywhere. And so GeForce is really a good candidate for any new cryptocurrency or any new cryptography algorithm that comes along. We try the best we can to go directly to the major miners, and the major miners. And they represent the vast majority of the demand. And to the best of our ability, serve their needs directly and we call that CMP, and that's why it's not called GeForce. They're called CMP. And we can serve those miners directly, hopefully, to take some of the demand pressure off of the GeForce market. Because ultimately, what we would like is, we would like the market for GeForce pricing to come down so that the gamers could benefit from the GeForces that we built for them. And the gaming demand is strong. I mean, the bottom line is, Fortnite is a home run. The bottom line is, PUBG is a home run. And the number of gamers that are enjoying these games is really astronomic, as people know very well. And it's a global phenomenon. These 2 games are equally fun in Asia as it is in Europe, as it is in the United States. And because you team up and this is a Battle Royale, you'd rather play with your friends. So it's incredibly social. It's incredibly sticky. And more and more -- more gamers that play, more of their friends join, and more of their friends join, more gamers that play. And so it's this positive feedback system, and the guys at Epic did a fantastic job creating Fortnite, and it's just a wonderful game genre that people are really enjoying. And so I think at the core of it, gaming is strong and we are looking forward to inventory normalizing in the channel so that pricing could normalize in the channel, so that gamers can come back to buy the GeForce cards that has now been in short supply for over a quarter. And so the pent-up demand is quite significant, and I'm expecting the gamers to be able to buy new GeForces pretty soon. Let's see here. Is it my turn again? It is. Okay. We had another great quarter. Record revenue, record margins, record earnings, growth across every platform. Data center achieved another record, with strong demand for Volta and AI inference. Gaming was strong. We're delighted to see prices normalizing and we can better serve pent-up gamer demand. At the heart of our opportunity is the incredible growth of computing demand of AI, just as traditional computing has slowed. The GPU computing approach that we've pioneered is ideal for filling this vacuum. And our invention of the Tensor Core GPU has further enhanced our strong position to power the AI era. I look forward to giving you another update next quarter. Thank you.","First, I had a question on gaming seasonality. It's usually down pretty decently in Q1. It was obviously flat this time as you were trying to fill up the channel. Now that's done. I was just wondering on with the supply dynamics -- supply-demand dynamics as well as like any thoughts on crypto might mean for typical -- the seasonality in the Q2 versus what would be typical or what would usually be down -- or usually be up pretty decently? How are you looking at that? And there's a question for Colette. Jensen, I had a question regarding your decision to pull the plug on your GeForce partner program. I think most of us read your blog from last Friday, I think it was. So we understand the basic background. But if you can describe what led to this decision, and perhaps talk a little bit about the potential implications, if any, in terms of your ability to compete or gain share, that will be really helpful. This is (inaudible) calling in for C.J. Muse. So I had a question on HPC. TSMC, on their recent call, raised their accelerator attach rate forecast in HPC to 50% from mid-teens. So I would love to get further details on what exactly NVIDIA is doing to software services, et cetera, that's kind of creating this competitive positioning in HPC and AI, basically. And then, if I could ask a follow-up, basically, in benchmarks. So there's been some news on AI benchmarks, whether it's Stanford's DAWNBench, et cetera. So I would love to get your thoughts on, a, the current state of benchmarks for AI workloads. And b, the relative positioning of ASICs versus GPUs, especially as we move towards newer networks like RNN and GAN, et cetera. I actually wanted to go back to the question about seasonality for gaming in June. Normal seasonal sounds like it's up mid-teens for June in gaming. But obviously, the comps are skewed a little bit because of the channel restock and the crypto stuff. So does the guidance for June assume that gaming is better or worse than that mid-teens normal seasonal? I have a question for Colette. Colette, first thank you for breaking out crypto sales in the OEM line and guide for us. I have a question on your gross margins. Your gross margins have been expanding on product mix, despite component pricing headwinds on the DRAM side. When do you expect component pricing to become a tailwind to your gross margin? My question is the progress on the deployment of Volta into the cloud service providers. You talked in your prepared remarks about 5 deployments, including the Google beta. Can you talk about the -- how soon we can expect to see some of those remaining deployments? And of those already launched, how far are they along? What -- I guess, to say proverbially, what inning are we in, in these deployments? I had a question about the DGX family of products. Our own fieldwork is indicating very positive reception for DGX. And I was wondering, Jensen, if can you help us understand, the high-growth we've seen in the data center business, to what extent is that being driven by the DGX? And what, when DGX-2 starts to ramp in the back half of the year, is that -- is it something that kind of layers on top of DGX? Does DGX-2 layer top of DGX? Are they (inaudible) segments, that you're kind of segmenting the market with these different products? Any color on how to think about those 2 products are -- should be -- would be helpful. Okay. Can you hear me better now? Okay. Sorry about that. I'm at the airport. So the question was on the DGX family of products. Our own fieldwork indicates very positive reception. I was wondering, Jensen, if you could help us understand the high-growth you've seen in the data center market. How much is DGX contributing to that? And then, when DGX-2 starts to ramp in the second half of the year, how do we think about DGX-1? Does it replace DGX -- the original DGX? Or you're going after different segments? Or do they layer on top of one another? Any color on that would be helpful. I'm actually going to go with a more nitty-gritty question just on the financial side, just to make sure I'm understanding this right. So the OEM beat was pretty material, given a lot of crypto revenue. Is it still the case that OEM is materially lower gross margin than your corporate average at this time? So your competitor thinks that just 10% of their sales were from crypto, or like $150 million, $160 million. And you guys did almost $300 million there. And perhaps I think there could actually be some in gaming as well, which would imply that you guys have 2/3 or more of that market? So I guess, what's going on there? Is there a pricing dynamic that's allowing you to have such share there? Or do you think it's your competitors that don't know what's actually being sold to miners versus gamers? Why such implied share in that market? This is a question for Colette. I want to follow up, again, on the seasonality, understanding the prior comments. Normal seasonal for Q2 for gaming would be up in the double digits. Given your commentary on the crypto declined into Q2, given your commentary on just the general drivers around data center and the Volta ramps, I can't bring that together with the idea of gaming being above seasonal within the guidance -- the context of your guidance envelope. So how should I reconcile those things? How are you actually thinking about seasonality for gaming into Q2 within the context of the scenarios that are currently contemplated in your guidance for next quarter? The question relates to the supply chain challenges that you've talked so much about in the gaming end market. I'm wondering if there's something particular to that end market that is making the shortages concentrated there? Or are, in fact, other end markets in particular that the data center end market also somewhat restricted from what growth they might have achieved if there weren't the shortages that are out there? And maybe talk about the pace of recovery of those. That be really helpful. PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q3 2019,2381,4636,1420,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2019. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It's also being recorded. You can hear a replay by telephone until November 22, 2018. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2019. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 15, 2018, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, I'd like to turn the call over to Colette. Thanks, Simona. Q3 revenue reached $3.18 billion, up 21% from a year earlier, with all 4 of our market platforms growing double digits. Data center, professional visualization and automotive all hit record levels. However, gaming was short of expectations as post crypto channel inventory took longer than expected to sell through. Gaming card prices, which were elevated following the sharp crypto falloff, took longer than expected to normalize. Our Q4 outlook for gaming reflects very little shipments in the midrange Pascal segment to allow channel inventory to normalize. In Q4, we also expect minimal sales of Tegra chips for game consoles due to the normal seasonal build cycle. While channel inventory situation presents a near-term headwind, it does not change our long-term fundamentals. Our competitive position is as strong as ever, and we have expanded our addressable market with Turing and our recent software announcements. We remain excited about the growth opportunities in ray-traced gaming, rendering, high-performance computing, AI and self-driving cars. GAAP gross margins grew 90 basis points year-on-year and non-GAAP gross margins rose 130 basis points. This reflects our continued shift toward higher-value platforms but also included a $57 million charge for prior architecture components and chips following the sharp falloff of crypto mining demand. Both GAAP and non-GAAP net income exceeded $1 million for the fourth consecutive quarter. From a reporting segment perspective, GPU revenue grew 25% from a year ago to $2.77 billion. Tegra processor revenue was down 3% to $407 million. Let's continue with our gaming business. Revenue of $1.76 billion was up 13% year-on-year and down 2% sequentially. Year-on-year growth was driven by initial sales of our new Turing-based GPUs as well as strong notebook sales, which more than offset gaming console declines. In mid-September, we began shipping GeForce RTX series, the first gaming GPUs based on our Turing architecture. Turing RTX technology delivers up to 2x the performance of its predecessor, Pascal, and 6x more for ray-traced graphics. These are the biggest generational jumps we have ever delivered in gaming GPUs. The first 2 GeForce RTX gaming cards to hit the shelves were the 2080 Ti and the 2080, delivering 4K HDR gaming and 60 frames per second on even the most advanced AAA titles, a major milestone for gamers. This is quickly becoming the new performance baseline as 4K displays are now reaching affordable price points. These 2 end -- 2 high-end cards were quickly followed by the rollout of the GeForce [27 D]. NVIDIA RTX technology brings games to life like never before. The highly anticipated Battlefield V launched this week with the first release of RTX ray-tracing, enabling lifelike reflections on GeForce RTX GPUs. With a pipeline of upcoming games supporting NVIDIA RTX features, RTX is well on its way to establishing itself as a game-changing architecture. Although the cryptocurrency wave has ended, the channel has taken longer than expected to normalize. Pascal high-end cards have largely sold through ahead of RTX. However, on midrange Pascal gaming cards, both channel prices and inventory levels remained higher than expected. Pascal is well positioned as the GPU of choice in the midrange for the holidays, and we expect to work down channel inventories over the next quarter or 2. Moving to data center. We had another strong quarter with revenue of $792 million, up 58% year-on-year and up 4% sequentially. Demand remains strong for Volta architecture products, including Tesla V100 and VGX systems, and our inference business continued to grow, benefiting from the launch of the Turing T4 Cloud GPU during the quarter. Just 2 months after its launch, the T4 has received the fastest adoption of any server GPU. It is integrated into 57 server designs and it is already on the Google Cloud Platform, its first cloud availability. The T4 delivers world record performance for deep learning inference and accelerates diverse cloud workloads, including high-performance computing, deep learning training and inference, machine learning, data analytics and graphics. We also announced an updated TensorRT software stack and NVIDIA TensorRT Hyperscale Platform. This new software includes 2 critically important capabilities that can drive deployment of the NVIDIA inference platform at scale in hyperscale data centers. First, it enables multiple models and multiple frameworks to run on the same GPU at the same time. This can drive higher data center utilization, directly translating to significant savings. Second, it integrates with Kubernetes, the leading orchestration layer for hyperscale data centers. Completing our inference platform, the new T4 GPU delivers 12x the peak inference performance of its T4 predecessor. All told, our inference platform delivers 40x faster performance in CPUs. And with the TensorRT software stack, it is ideally suited for hyperscale data centers. With this launch, NVIDIA is poised to take the data center inference market, targeting every server node in the hyperscale data centers. Another important launch for the quarter was the NVIDIA RTX Server reference architecture, which incorporates up to 8 Turing-based RTX 8000. With this product, Turing opens a new market to GPOs, photoreal rendering or the creation of computer-generated images that look real. Rendering is instrumental to large industries, such as media and entertainment, retail, product designs, manufacturing and architecture. Yet prior to Turing and its ray-tracing capabilities, GPUs were not able to address this workload. So most rendering at -- up to this point has been done on CPUs. An RTX-accelerated render farm compared with an equivalent performance CPU render farm is 1/4 the cost, 1/10 the space and 1/11 the power. NVIDIA's RTX platform has garnered major industry support, including from key developers such as Adobe, ANSYS, Autodesk, Dassault and many others. Lastly, NVIDIA announced a GPU acceleration platform for data science and machine learning called RAPIDS, which enables companies to analyze massive amounts of data and make accurate business predictions at unprecedented speed. Up until now, data analytics and machine learning has been the largest high-performance computing applications not to have been accelerated. Virtually all enterprise use data analytics to extract insight from big data for a wide range of use cases, such as predicting credit card fraud, forecasting retail inventory and understanding customer buying behavior. RAPIDS is an open source suite of libraries for GPU-accelerated analytics, machine learning and, soon, data visualization. With RAPIDS, NVIDIA GPUs can now accelerate machine learning, as we have done with deep learning, with performance up to 50x faster than CPUs. The RAPIDS launch opens up a $20 billion server market used for data analytics and machine learning workloads to GPUs, and it's received broad industry support, including from Oracle, IBM, SAP, Dell EMC, Hewlett Packard Enterprise, Microsoft Azure machine learning, Google, Q-Flow as well as the open source community. With one unified architecture and ecosystem, NVIDIA GPUs can address the redefined high-performance computing market, including scientific computing, deep learning and machine learning. Our GPUs and software stack accelerate a broad and diverse set of workloads, ranging from scale-up software in supercomputers to scale out deployments in hyperscale data centers. Just earlier this week, this capability was on display at Supercomputing Conference in Dallas, where the number of systems on the TOP500 supercomputer list using NVIDIA GPUs jumped 48% from last year, including the #1 and #2 systems in the world. Moving to pro visualization. Revenue reached a record $305 million, up 28% versus the prior year and up 9% sequentially. Strength extended across the desktop and mobile as well as several key industries, including the public sector, manufacturing and architecture, engineering and construction. At SIGGRAPH in August, we announced our Quadro RTX 8000, 6000 and 5000 GPUs based on the Turing architecture. And earlier this week, we introduced the Quadro RTX 4000, the most advanced professional GPU priced under $1,000. These GPUs will revolutionize the work of 50 million designers and artists by enabling them to render photorealistic scenes in real-time and leveraging AI in their workflows. The Quadro RTX series started shipping in Q3, with the server-grade, high-end products recognized in data center. We already engaged with a range of customers on RTX, including the major movie studios and game developers, and the reaction has been very positive. Finally, turning to automotive. Automotive sales in Q3 reached $172 million, up 19% from a year ago and up 7% sequentially. This reflects growth in our autonomous vehicle production and development engagement in addition to the ramp of next-generation, AI-based cockpit infotainment systems. At GTC Europe, we announced that Volvo Cars selected NVIDIA's DRIVE AGX Xavier next-generation -- for next-generation Volvo Cars. The initial production release slated for the early 2020s will deliver Level 2+ assisted driving features, integrating 360-degree surround perception and a driver monitoring system. This is our first Level 2 mass-market car design win. In addition to Volvo, global automotive suppliers, Continental and Veoneer, announced that they have selected NVIDIA DRIVE AGX for their autonomous driving systems. Lastly, our DRIVE AGX Xavier development kit started shipping in this quarter. This is the world's first autonomous driving platform, and it can run our NVIDIA DRIVE software for autonomous driving, including data collection, 360-degree surround perception, advanced driver monitoring and in-vehicle visualization. With this platform, customers have developed and test their autonomous driving solutions and then easily move into production. We are excited about the AV opportunity as we look into next year and beyond. Moving to the rest of the P&L and the balance sheet. Q3 gross margins was 60.4% and non-GAAP was 61%, below our outlook due to the $57 million charge for prior architecture components and chips following the sharp falloff in crypto demand. GAAP operating expenses were $863 million and non-GAAP operating expenses were $730 million, up 28% year-on-year. We continue to invest in the key platforms driving our long-term growth, including gaming, data center and automotive. GAAP net income was $1.23 billion, and EPS was $1.97, up 48% from a year earlier. GAAP net income benefited from the reduction of $138 million in our U.S. tax reform transition tax amount as well as other discrete tax items. Non-GAAP net income was $1.15 billion and EPS was $1.84, up 38% from a year ago, reflecting revenue growth and gross margin expansion as well as lower income tax expense. Accounts receivable was $2.22 billion compared to $1.66 billion in the prior quarter as Turing's RTX shipments began in the latter part of the quarter. Inventory at the end of the quarter was $1.42 billion compared to $1.09 billion in the prior quarter, reflecting the ramp in production of Turing products. Quarterly cash flow from operations was $487 million. Capital expenditures were $150 million. This fiscal year, we have returned $1.13 billion to shareholders through the end of Q3. We've announced a $0.01 increase in our quarterly dividend to $0.16 effective in Q4 of fiscal 2019. We are also pleased to announce an increase of $7 billion to our share repurchase authorization and that we intend to return an additional $3 billion to shareholders by the end of fiscal 2020. With that, let me turn to the outlook for the fourth quarter of fiscal 2019. As noted earlier, our revenue outlook is impacted by the expected work-down of Pascal midrange gaming card inventory in the channel. In addition, we expect a decline in our gaming console revenue given seasonal build patterns. Keep in mind that the midrange desktop portfolio is typically about 1/3 of our gaming business. Our outlook assumes that channel inventory weeks approach normal levels exiting Q4 and that gaming and demand increases in Q4 compared with Q3. Now in total, we expect revenue to be $2.7 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 62.3% and 62.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $915 million and $755 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of $21 million. GAAP and non-GAAP tax rates are both expected to be 8%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $190 million to $210 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, I'd like to highlight some upcoming events for the financial community. We'll be presenting at the Barclays Global Technology, Media and Telecommunications Conference on December 6, and we will be meeting with the financial community at the Consumer Electronics Show in Las Vegas from January 8 through 11. And our next earnings call to discuss our financial results for the fourth quarter of fiscal 2019 will take place on February 14. With that, we will now open the call for questions. Operator, will you please poll for questions?","Yes, we expect to continue to do well in data centers. The -- if you look at the background of what's happening, we know that Moore's Law has ended. And while demand for computing continues to grow and more and more of the data center is running machine learning algorithms, which is computationally really intensive, the only way to increase computational demand -- or computational resource is to buy more computers, buy more CPUs because each one of those CPUs aren't getting much faster. And so as a result of that, the data center CapEx would have to go up. The alternative, which is the alternative that we offer and is one of the reasons why the adoption of NVIDIA's accelerated computing platform is growing so fast, is because the approach that we provide allows for a path forward beyond Moore's Law. There are several things that we have done this last quarter that I think is really fantastic. The first is the introduction of a new computing platform, new accelerated platform called RAPIDS. And as you know very well that the vast majority of the industry today, although are super excited about deep learning, deep learning as a method for artificial intelligence is very data-intensive. And in areas where there's a lot of domain expertise, where there's -- whether it's in retail or whether it's in financial services or health care, logistics, there's a fair amount of domain expertise, and the amount of data that they have to fuse together to train a model is quite high. The approach using traditional machine learning is quite successful. That has never been accelerated before. And we worked with the open source community over the course of the last several years to pull together an entire stack that starts from Apache Arrow, the Dask parallel distributed computing engine, and then all of our CUDA and all of our algorithms that run on top of that. We now have an accelerated machine learning platform. That's a brand-new platform, and the excitement around that is really quite incredible. The second thing is the Turing architecture allows us to do film rendering at a much, much more affordable way than Moore's Law would have allowed. And then the third, which we just announced recently, is our first Turing-based T4 Cloud GPU. And along with all of the software stack that we've put on top of it, Kubernetes, the Docker, the TRT inference engine, our second-generation Tensor Core, AI accelerator, all of that together has created a lot of excitement in data center. So I'm expecting our data center business to be -- to continue to do quite well. Yes, the Turing launch happened towards the end of the quarter, and it's the biggest generational leap we've ever had. It introduced real-time ray-tracing. It's the first GPU to -- gaming GPU to include artificial intelligence. At every single price point it serves, it is substantially higher performance than the last and it's the highest performance GPU in the world. And all the great content are coming. Today, I think it is -- or yesterday, I think it was, the Battlefield V was released with real-time ray-tracing, the world's first application to support real-time ray-tracing. So I -- we expect Turing to do really well. As we go on, surely, we'll bring Turing deeper into the mainstream. And so we don't have anything to announce today, but as usual, we want to bring a brand-new architecture to as many gamers as possible. Yes. So I commented about the overall size of what we have traditionally seen in terms of the midrange over a fairly large period of time because keep in mind, the launches of products quarter-to-quarter have changed that. So it has been about 1/3 consistently over, let's say, about an 8-quarter period of time. And we think that's a good number for you to use as we look at our guidance in terms of Q4. From an overall console perspective, we again have seen about -- in the hundreds and sometimes more than that in prior quarters. What we're seeing is just a normal where they build ahead of the holidays in Q3, and that slows down as we move into Q4 when we're in the middle of the holidays. I expect ProVis to grow. The ProVis platform, more -- there's more content ever being created that is digitally created. And most photographs -- what appears to be photographs are rendered in software. Almost every catalog, every video, every movie, every TV show now has a great deal of digital rendering in it. And until now, it's not been possible to do the rendering in an accelerated way. So RTX is the world's first accelerated ray-tracing GPU, and the enthusiasm from the digital content creation market is really, really great. I surely expect that -- and as I was mentioning earlier, that it's been close to a decade that the workstation industry has not had a fundamental platform architecture change. And so RTX is the first one. So I'm expecting ProVis to do really well. I'm curious, Jen-Hsun, what needs to happen to work down this midrange Pascal inventory? Is it pricing? Is it something else? Because the thinking was that this could be cleared within the October quarter, but it hasn't. Do you think people were waiting for Turing to come out and maybe that created some kind of pause? And then as part of -- part B of that question, maybe Colette, how should we think about seasonality in the April quarter given that you mentioned it could take 1 or 2 quarters to work down this inventory? Yes, the -- well, we came into Q3 with excess channel inventory post the crypto hangover. We expected the pricing in the marketplace to decline. It declined slower than we expected and -- but while it was declining, we were expecting sales volume to grow, demand to grow and for pricing to be -- for volume to be elastic with pricing. I think it just took longer than -- the pricing took longer than we expected, and the volume increase took longer than we expected. At this point, most of the pricing has come down to its -- and slightly below its prelaunch levels. And so I'm hoping that -- I'm hopeful that now that pricing has stabilized, that customers will come back and buy. I guess when pricing is volatile in the channel, it probably freezes some people waiting for prices to stabilize, and that took longer than we expected frankly. But now that it's at the right levels, our expectation is that the market will return to normal. 1060 is the #1 selling graphics card in the world, and we decided not to sell any more into the channel for the upcoming quarter to give the channel an opportunity to sell through the inventory it has. And so we'll keep our eyes on it, but our expectation is that inventory levels will come back to normal by the end of the quarter. Okay. Vivek, to answer your question also regarding Q1 in terms of what we're going to see in terms of the expectation, as the channel inventory normalizes at the end of Q4, we do believe going into Q1, we will be probably up from where we end in terms of Q4. So we won't follow that normal seasonality between Q1 and Q4. We do expect to be up as we go into Q1. The last question, I'm not sure I understand. The -- I think the answer to your first question is yes. You framed it nicely. It's -- the answer is yes. The last question was what again? The last question was regarding our midrange. Is there any statement about future Turing products that were taken into account? We -- yes, we haven't announced our future Turing product, but it would be expected for us to create a Turing GPU that serves the mainstream parts of the marketplace. And so we're not announcing anything, but it would be conventional of us to do that. No, we're really not shipping into the midrange segment of Pascal so that we give the channel an opportunity to sell through the product that has. And we would like to see channel inventory get normalized by the end of Q4, and then we'll get back to doing our work. Stacy, yes. I think the answer to that is yes. In aggregate, yes, we do believe the rest of the business will grow sequentially. That is correct. Let's see. The -- we were surprised, obviously. I mean, we're surprised by it, as anybody else. The crypto hangover lasted longer than we expected. Prices started to drift down, and we expected to come down much more quickly than it did and -- but -- and when it went down, we expected demand to come up much more quickly than it did. And so I think the channel wanted to protect its price. People were uncertain about crypto, and demand was uncertain about when the price will be stabilized. And so all of that uncertainty, I think, froze the market a little longer than we expected. Pricing is now down to below prelaunch normal levels. And so I am hopeful that we're going to see demand come back and the sell-through will happen through the holidays. And we're seeing that. And so that's -- the first one is that we -- just we didn't expect it either and we didn't realize the magnitude of it until towards the end of the quarter. What was the other question? Was there another question? I think that, that was it. With regards to Turing ramp, I guess, how is that going relative to your expectations? It seems like availability is quite a bit better now. And where do you stand with DLSS support? I know you've announced a number of games that will have DLSS support by year-end. Like how many of those are already supporting that technology? Yes, the ramp is going great, and I think this is the biggest generational leap we've ever had. This is the most substantial new technology that computer graphics has seen in a decade. Real-time ray-tracing is something that everybody had dreamed about for a long time. It's never seen before. And today with Battlefield V, people are enjoying real-time ray-tracing for the very, very first time. And the images are beautiful. So the ramp is great. Of course, Turing ramped into -- towards the end of the quarter and into a much more different situation than any GPU of the past. But nonetheless, the demand on the high-end products are fantastic. The 2080 Tis are largely sold out. I think it's still sold out everywhere. And so I think that the demand is great. I'm expecting it to be just a fantastic new generation. In terms of the content, you saw the first one. FINAL FANTASY is also out. And we have a pipeline, about 30 of them. We're working hard on that. And so when they -- when these games get released, RTX will be enabled. But I will say one last thing, which is content aside, RTX is higher performing at the same price point than any graphics card on the planet. And so at every single graphics -- every single price point, it is the highest performing graphics card. So it is unambiguously the highest performance GPU in the world and then -- and of course, all of these great new features will be coming. I'm trying to figure out what the first part of the question was. I think the channel has more than 12 weeks of inventory between us and the other brand. One of the things that is hard to estimate is how much inventory the other brands have. And our quarter is 1 month later. And so whatever action we take, whatever we see in the channel is 1 month after their end of the quarter. The amount of inventory is not just us. It's also the other brands. And our ability to see the other brand's inventory is just much harder. We try our best to estimate it, but obviously, we didn't estimate it well enough. And so the answer to your question is yes, I think there's about -- from our perspective, about 12 weeks of our inventory to sell through at this point. Turing is the highest performance GPU at every single price point. And so it played no role in its transition. It's all about crypto hangover. This is the new experience as we made this transition. If you look at Turing on -- just on the basis of Turing, it had a great launch. We ramped it at the end of the quarter, as we expected. It was back-end loaded, as we expected, and the ramp was great. Everybody did a great job. And the performance is fantastic, and the excitement is great. And so I think Turing's ramp was a big success. It's -- underneath Turing was choppy, as we're talking about. And we really didn't see that until towards the end of the quarter. And as we looked out into this quarter, this coming quarter, we came to the conclusion that the best thing to do is just not to ship any more products into this segment of the marketplace because there's a fair amount of inventory and let the channel sell through the midrange Pascals. And then a quarter's time, we'll get back to business. And so I think -- I knew this is surely a setback, and I wish we had seen it earlier, in the final analysis can't be exactly sure what we would have done different. But between the unexpected, unanticipated slow decline of pricing in the channel and even after the prices came down, it took a little longer than we expected for volume to kick up. And the other brand's inventory in the marketplace, those factors kind of compounded and made it a lot worse than we expected. Okay. Let me take the second one first. Our data center business is doing great. I mean, the fundamental dynamics of accelerated computing is spot on. And with Moore's Law coming to an end, it's the path forward. Take a look at the number of systems in the TOP500, 127 systems, I think, this year was, a growth of nearly 50% year-over-year. We're the #1 system in United States and the world, in Europe, in Japan. We're 22 out of the top 25 most energy-efficient computers in the world. And then this quarter, we announced 3 new initiatives that's going to expand us into a broader part of the high-performance computing market with machine learning, which is -- as we know, is the largest part of artificial intelligence today, which has not been accelerated and now it is. The second is the ability to do rendering for film, photorealistic rendering for the first time. And then the third is a brand-new cloud GPU, we call T4, that the enthusiasm around it is just incredible. And from the time that we went to production to the time that Google put it in their cloud was literally 30 days. It's just an incredible speed of adoption. And so I expect T4 to do quite well. So I think our data center business dynamics are really quite great. In terms of forecast, we'll just see how it turns out. But I think the fundamental dynamics are great. Back to your question about gaming. So the statement came in regarding, you've bumped up the overall gaming somewhere in mid of the year to about a $1.7 billion gaming business, where maybe if you look back 2 years, you were at about $1.1 billion. At this stage, when you come out of the setback that we have here to get through the overall channel inventory, where will you come out after that? And what type of growth could we expect? Yes, I'm going to let you guys do the modeling, but let me just say this. There's nothing fundamentally different about the gaming market that we know. Cryptocurrency is an extraordinary factor that we all have to just internalize as it is. And we thought we had done a better job managing the cryptocurrency dynamics. But when the prices came down -- started to come down and we hoped the demand would start to reflect the declining price, it just took longer than we expected. And that's what we're experiencing. In terms of the gaming marketplace, if you take a look at some of the dynamics, our notebook gaming, which is not affected by crypto, grew 50% year-over-year in China. And so the gaming market seems quite robust. RTX is going to unquestionably redefine gaming computer graphics. And so I think that the dynamics are good. We have to work through the channel inventories. This quarter, of course, we had the simultaneous decision of not shipping any more midrange products into the channel as well as seasonal -- normal seasonal console build plans. And they tend to build out a quarter before the holiday season. So you have these 2 simultaneous effects. But there's nothing about the gaming marketplace or the gaming business that we see that's fundamentally different. Yes. To kind of add to that, think about our gaming business in several pieces that we talked about in terms of the tremendous strength that is also continuing. In terms of our success in terms of Turing, our notebooks for gaming are growing extremely strong, and our overall console business is also extremely healthy as well. So to think about all of the different components, we just have a piece of channel inventory at the midrange, but overall, as you can see, gaming is also growing quite well. The ramp of T4 is completely related to customers porting their model on top of our platform. And the inference model is really complicated. This is one of the things that I've talked about in the past that on the one hand, people think that inference appears to be simple because there are so many ASICs built being talked about. The vast majority of the complexity of inference is actually in the optimizing compiler on top. The TensorRT, fifth-generation optimizing compiler that we announced just recently took 3, 4 years to build. And then on top of that, in order to get it to scale as quickly as what people saw in Google -- Google's cloud requires us to build something called a TRT server, an inference server, that allows multiple models to run on top of Kubernetes in the cloud. That piece of software is also super complicated to write. And so the pieces of technology that we're putting together have come together. And now we're engaged with Internet companies around the world to port their most heavy workload applications on top -- or models on top of T4. And so we're working hard on that. And when that happens, it comes down to their decision of how many they would like to buy, and that tells us about our adoption rate. I think from a high-level perspective, if we step back for a second, the high-level -- the way to look at it is this: that we know for a fact that Moore's Law has come to an end, and at the same time, we also know that more and more data centers are deploying deep learning models and machine learning models into their data center. And it's computationally really intensive. And at this time and as we look out into the horizon, the T4 Cloud GPU is just unquestionably the most effective. It can run models, whether it's an image model or a recommendation model or a speech synthesis model. It is the highest throughput processor in the world at 70 watts, which fits into a hyperscale data center OCP server. It is also the lowest latency of any processor at inferencing in the world at less than 1 millisecond. And so between the architecture, all of the software technology and all of the software capabilities we put in place and the fact that the conditions would suggest that Internet companies need an accelerated path forward, I think T4 is really well positioned. And I look forward to coming back and telling you guys about success. Yes, Jetson is designed for edge AI. One version of Jetson, which is a functional save, high-performance with a lot of complicated software, one version of that, you could say, is self-driving cars. And this quarter, we announced winning our first mass-market Level 2. We've been really successful in robot taxis and Level 4s and trucks and shuttles and high-end systems, where the number of processors, the number of sensors, the combination of LiDARs and surround cameras requires a lot of computation and -- but we've never been successful until now with taking the DRIVE platform all the way down to Level 2 mass-market cars. Volvo is our first announcement, our first win in high-volume, early 2020s production ramp. And I'm expecting many more. And I think we've positioned and created a solution that is both highly useful and easy to use as well as could deliver Level 2 capability in a single chip for the very first time. And Xavier is in production. It is the only single-chip, autonomous vehicle processor in production today. Then you take that same platform and you could apply to all kinds of other edge AI devices. It could be manufacturing picking robots. It could be autonomous retail, basically AI retail, and autonomous warehouses. So -- or medical instruments, medical imaging instruments that, in the device itself, recognizes and identifies anomalies. And so all of this type of applications are leaning towards AI, and that's the reason why we built Jetson. I'll ask a clarification and then a question. The clarification is just on the inventory issue, and thanks for all the color. But one, are Pascal 1070s and 1080s and in Ti flavors still selling? And if so, could they present any kind of inventory risk either later this year or in early fiscal '20? And then the question really, Jen-Hsun, is trying to get a better understanding of how you see the intermediate-term growth rate of the data center business. You had spectacular high-performance compute TOP500 accelerator penetration performance up 50-plus percent. That about matches the growth in the data center business. Those may be somewhat coincidental, but can you just talk about where you see penetration across key end markets like HPC, like cloud and hyperscale and like enterprise? Which offer you the best growth from here? And where do you feel like your penetration may be more mature? I'm just trying to get a sense if there's an acceleration coming off of the 50% year-on-year growth that we're seeing now or if, consistent with the recent trend, we might be moderating potentially down into the 40% or 30% range as we go into the next calendar year. Our high-end Pascal GPUs are largely sold. And we did a fairly good job making sure that -- with that transition before we ramped up the high-end Turing products. The -- our data center business, I would say that Q2 -- or excuse me, Q3, this last quarter, the inventory setback aside, I actually have to say that it's one of the best quarter we've ever experienced. And the reason for that is because our data center position, our accelerated computing position as a company, which is the foundation of this company, the accelerated computing focus of our company expanded in really several ways. For the first part of our journey into accelerated computing, it was really following scientific computing, simulating first principle laws of physics for scientific computing and high-performance computing codes. About 5 years ago, deep learning came into the fore, and we were alert and agile to have invested a great deal and mobilized the company to go help the world put deep learning into software developers' hands all over the world. The area where I'm super excited about right now are the 3 that I've mentioned that we've opened up in this last quarter with the launch of Turing and with the launch of RAPIDS. The first is our film rendering opportunity is -- we think that there's about 10 million CPU nodes around the world that are used for film rendering. They can now benefit from accelerated computing as Moore's Law comes to an end. The second is opening up inference. The hyperscale data center marketplace is something along the lines of 15 million CPUs sold this year. And it was growing -- let's call it, growing at about 15% per year and -- the number of CPUs. And we know for a fact that Moore's Law has come to an end and those servers are going to have to be accelerated going forward. And so I think that T4 is just ideal for that. It was designed from the ground up to deliver computing in a very, very compressed and very condensed and power-sensitive environment, which these hyperscale data centers tend to be. And the software stack from Kubernetes to containers, to a TRT compiler, to the TRT inference server and our NGC cloud with all of the stacks fully accelerated and containerized in the cloud, certifying all of the major cloud providers around the world for our containers, that process took us several years and it's put us in a really great place. So T4 is really fantastic. And so that's -- the second segment of high-performance computing is deep learning. The third and potentially the largest currently is machine learning. This is where Hadoop goes. This is where Spark goes. This is where scikit-learn, Python, pandas. All of the data scientists around the world in retail, in transportation and logistics, in health care, in financial services that are using algorithms like random forest and XGBoost and k-nearest and k-means and PCA and all of these different buzzwords have never had the opportunity to have accelerated computing until now. And this took a couple, 2, 3 years for us to pull together. RAPIDS has been open sourced. You can go into the NGC cloud, download it. IBM is going to integrate it into their machine learning platform. SaaS, SAP, Oracle, the cloud providers are all integrating the RAPIDS open source SDK into their machine learning platform. And so this is a new segment for us. The answer about our growth rate is, I believe, that our accelerated computing, our data center opportunity has significantly expanded during the quarter. Between the T4 hyperscale cloud GPU and RAPIDS machine learning platform, it has -- and our RTX server film rendering, we surely have expanded our data center opportunity. And so I fully expect us to continue to do well in accelerated computing for data centers. Thanks, everyone. To sum up, the crypto hangover has left the industry with excess inventory -- excess channel inventory. It will take 1 or 2 quarters to work through it. This is an unexpected near-term setback and doesn't change the fundamental dynamics of our company. The end of Moore's Law has cleared a way for NVIDIA accelerated computing as a great path forward. Turing opens up 3 exciting markets for us with ray-tracing games, film rendering and hyperscale inference. And with our first win in mainstream Level 2 self-driving cars with Volvo, our DRIVE AV platform is gearing up for the mass market, and our competitive position has never been stronger. We look forward to updating you on our progress. Thank you.","Within your guidance for the January quarter, is the team anticipating continued sequential growth in your data center business? There seems to be some concern around a near-term slowdown in cloud spending. But on the flip side, we're hearing that the NVIDIA team is actually seeing pretty strong demand, near term, from some of your China cloud customers for your Tesla-based products. So I just wanted to get your views on cloud data center dynamics and the trajectory into the January quarter. Great. And then just on the high-end Turing products that the team started rolling out in October, early demand actually seems to be quite strong. And I think part of it is just the lineup of AAA-rated games. eSports continue to be strong as well, obviously a big motivator for your enthusiast-class gamers. I know the team is, near term, kind of working down midrange Pascal cards, but do you anticipate your Turing-based RTX product families to drive sequential growth in the January quarter just what appears to be pretty strong demand pull for these products? I had a question on the gaming outlook and as it relates to channel inventory. Colette, you mentioned that typically, the midrange is about 1/3 of gaming. How much was it in the October quarter? And are you effectively assuming close to 0 in the January quarter? And related to that, I think with your game console business, I'm estimating you did something around $200 million to $250 million in October. Again, is that coming down pretty hard into January? Great. And then as a follow-up, you saw nice acceleration in growth in your ProVis business this quarter. Jen-Hsun, I think you've talked extensively about the RTX and the long-term opportunity there. What are your expectations going into 2019 for that segment? I guess, a follow-up question on the channel inventory side. It looks like it's roughly $600 million kind of a drawdown here. And just curious, does that sound right? Number one. Number two, does that fit with what you are hearing from your channel partners in terms of what's excess? And then as part of that, are you drawing down inventory in the current quarter ahead of Turing architecture launch into the mainstream? I guess, the question was whether you were drawing down inventory perhaps below normalized levels in the current quarter. My first one is for Colette. I just wanted to be a little more explicit. If I think about your business split into sort of gaming and non-gaming, are you looking for the non-gaming pieces in aggregate to grow sequentially into Q4? Got it. I guess that fits with the kind of 1/3 you're talking because that implies the gaming down 30%-plus. So that is what your kind of a magnitude that you're thinking about at this point. Got it. For my second question, I just wanted to -- for the last several quarters, the idea that the channel could be getting full is not necessarily a new worry. And yet the last several quarters, you've been saying, like on this call, that you guys felt like you had a really good handle on the channel, and yet it seems like maybe that wasn't exactly the case. Can you give us a feeling, maybe a better feeling for what changed and when you saw it in the quarter? Was this something that happened kind of like late in the quarter that you realized it? Or did you go into the quarter knowing that the inventories were high and needed to be corrected? Like what happened? Because this tone is a little different from what we've heard over the last few earnings calls from you. I'm still trying to put my head around the magnitude of this channel inventory drawdown because if you don't ship like the midrange cards for a full quarter, that means your inventory today is more than a quarter, more than 12 weeks of sales. And so my first question would be, am I right thinking that? Are you available a full quarter of sales in inventories? And then my second question is while you are drawing down this inventories, I would have expected to see, like, the Turing high-end card, the 2080 and the 2080 Ti ramping in Q4. And it looks like, excluding the inventory, the rest of the business -- the rest of the GPU business in gaming would be more like flattish sequentially. The first question was whether the midrange of Pascal had more than 12 weeks of inventory, if it's going to take more than a quarter to bring it down. The -- I was hoping you could contrast this product cycle transition to Turing to the product transition you had to Pascal. And is the only -- or is the main difference the crypto hangover? Or is there something else impacting the transition, do you think? I mean, you've described Turing as the greatest generational leap, and I'm wondering if that larger delta has an impact to the transition as well. Maybe I can ask the question a little bit differently on the gaming business. If I look back over the past several quarters, let's say you've been running at roughly a $1.6 billion to $1.8 billion revenue level since the October '17 quarter. Prior to that, you were at $1.1 billion, $1.2 billion. We look like we're now going back to that level. I guess, the question is, do we build off of that level? Do we bring back half of the inventory burn? How do I think about the return of year-over-year growth in that gaming piece of the business as we start to look into fiscal 2020? And then a quick second question. Over the past few years, you've had really strong seasonal sequential growth in the data center business, in Q4, about 20% sequentially. I'm just curious, how is your guide factoring in the sequential growth in that piece of the business into this current quarter? A question with regard to inferencing and what we can expect from that for both Q4 and going forward. And perhaps I don't know if it's a valid comparison to compare what we might expect from inferencing after the new Quadro launches to what happened in training after the Volta launches. Is there any comparison there in terms of magnitude for how the ramp goes? First, Jen-Hsun, I appreciate all the details on the T4 for inference in the data center. Could you likewise highlight the current traction you're seeing and the long-term growth expectations for the Jetson product that's designed for really, I think, a different market? It's inference at the edge, right? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q2 2020,2012,5013,1091,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the Second Quarter of Fiscal 2020. With me on the call today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2020. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, August 15, 2019, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. Q2 revenue was $2.58 billion, in line with our outlook, down 17% year-on-year and up 16% sequentially. Starting with our gaming business. Revenue of $1.31 billion was down 27% year-on-year and up 24% sequentially. We are pleased with the strong sequential growth in the quarter when we launched our RTX SUPER lineup for desktop gamers, wrapped up our greatest ever number of gaming laptops and launched our new RTX studio laptops for creators. In July, we unveiled 3 GeForce RTX SUPER GPUs, delivering the best-in-class gaming performance and power efficiency and real-time ray tracing for both current and next-generation games. These GPUs delivered a performance boost of up to 24% from our initial Turing GPUs launched a year earlier. The SUPER lineup strengthens our leadership in the high end of the market, and the response has been great. We look forward to delighting gamers with the best performance in ray tracing as we get into the back to school and holiday shopping seasons. Ray tracing is taking the gaming industry by storm and have quickly come to define the modern era of computer graphics. A growing number of blockbuster AAA titles have announced support for NVIDIA RTX ray tracing, including Call of Duty: Modern Warfare, Super Punk 2077 (sic) [Cyberpunk 2077], Watch Dogs: Legion and Wolfenstein: Youngblood. Excitement around these titles is tremendous. GameSpot called Cyberpunk one of the most anticipated games of the decade. NVIDIA GeForce RTX are the only graphic cards in the market with hardware support for ray tracing. They deliver a 2 to 3x performance speed up over GPUs without a dedicated ray tracing core. The laptop business continues to be a standout growth driver as OEMs are ramping a record 100-plus gaming laptop models ahead of the back to school and holiday season. The combination of our energy-efficient Turing architecture and Max-Q technology enables beautifully crafted thin and light form factors that can deliver the performance of high-end gaming desktop or our next-generation console. At Computex in May, we unveiled NVIDIA RTX Studio laptops, a new design artist platform that extends our reach to the large, underserved market of creators. In the age of YouTube, creators and freelancers are rapidly growing population, but they have traditionally not had access to professional-grade workstations through online and retail channels. RTX Studio laptops are designed to meet their increasing complex workflows such as photorealistic ray tracing, AI image enhancement and ultra high-resolution video. Powered by our RTX GPUs and optimized software, RTX Studio laptops deliver performance that's up to 7x faster than that of the MacBook Pro. A total of 27 RTX Studio models have been announced by major OEMs. Sequential growth also benefited from the production ramp of the 2 new models of Nintendo Switch gaming console. We are expecting our console business to remain strong in Q3 before the seasonal production slowdown in Q4 when console-related revenue is expected to be fairly minimal, similar to last year. Moving to data center. Revenue was $655 million, down 14% year-on-year and up 3% sequentially. In the vertical industries portion of the business, expanding AI workload drove sequential and year-over-year growth. In hyperscale portion, we continue to be impacted by relatively weak overall spending at a handful of CPU -- CSPs. Sales of NVIDIA GPUs for use in the cloud were solid. While sales of internal hyperscale use were muted, the engineering focus on AI is growing. Let me give some color on each of these areas. We are building a broad base of customers across multiple industries as they adopt NVIDIA's platforms to harness the power of AI. Public sectors, higher education and financial services were among the key verticals driving growth this quarter. In addition, we won Lighthouse account deals in important industries that are on the cusp of being transformed by AI. For example, in retail, Walmart is using NVIDIA GPUs to run some of its product demand forecasting models, slashing the time to do so in just 4 hours from several weeks on CPUs. By accelerating its data science workflow, Walmart can improve its algorithms, reduce development cycles and test new features. Earlier this week, we announced breakthroughs for the fastest training and inference of the state-of-the-art model for natural language process understanding called BERT, or Bidirectional Encoder Representations of -- from Transformers, a breakthrough AI language model that achieves a deeper sense of language, context and meaning. This can enable mere human comprehension in real-time by chat box, intelligent personal assistants and search engines. We are working with Microsoft as an early adopter of these advances. AI computing leadership is a high priority for NVIDIA. Last month, we set records for training deep learning neural network models on the latest MLPerf benchmarks, particularly in the most demanding areas. In just 7 months, we have achieved up to 80% speed-ups enabled by new algorithms and software optimizations across the full stack while using the same hardware. This is a direct result of the productive programming environment and flexibility of CUDA. Delivering AI at scale isn't just about silicon. It's about optimizing across the entire high-performance computing system. In fact, the NVIDIA AI platform is getting progressively faster. Every month, we publish new optimization and performance improvements to CUDA-X AI libraries, supporting every AI framework and development environment. All in, our ecosystem of developers is now 1.4 million strong. In setting these MLPerf records, we leveraged our new DGX SuperPOD AI supercomputer, demonstrating that leadership in AI research demands leadership in computing infrastructure. This system debuted in June at #22 on the TOP500 list of the world's fastest supercomputers at the annual International Supercomputing Conference. Used to meet the massive demand for autonomous vehicle development program, it is powered by more than 1,500 NVIDIA V100 Tensor Core GPUs linked with Mellanox interconnects. We've made DGX SuperPOD available commercially to customers, essentially providing them with the turnkey supercomputer that they can assemble in weeks rather than months. It is roughly 400x smaller in size than other similarly performing TOP500 systems, which are built from thousands of servers. Also at the conference, we announced that by next year's end, we will make available to the ARM ecosystem NVIDIA's full stack of AI and HPC software, which accelerates more than 600 HPC applications and all AI frameworks. With this announcement, NVIDIA will accelerate all major CPU architectures, including x86, POWER and ARM. Lastly, regarding our pending acquisition of Mellanox, we have received regulatory approval in the U.S. and are engaged with regulators in Europe and China. The approval process is progressing as expected, and we continue to work toward closing the deal by the end of this calendar year. Moving to pro visualization. Revenue reached $291 million, up 4% from our prior year and up 9% sequentially. Year-on-year and sequential growth was led by record revenue for mobile workstations with strong demand for new thin and light form factors. We had a great showing at SIGGRAPH, the computer graphics industry's biggest annual conference held in Los Angeles. Our researchers won several Best in Show awards. In just a year since the launch of RTX ray tracing, over 40 design and creative applications with RTX technology had been announced by leading software vendors, including Adobe, Autodesk and Dassault systems and many others. NVIDIA RTX technology has reinvigorated the computer graphics industry by enabling researchers and developers to take a leap in photorealistic rendering, augmented reality and virtual reality. Finally, turning to automotive. Q2 revenue was $209 million, up 30% from a year ago and up 26% sequentially. This reflects growing adoption of next-generation AI cockpit solutions and autonomous vehicle development projects, including 1 particularly sizable development services transaction that was recognized in the quarter. In addition, in June, we announced a new partnership with the Volvo Group to develop AI and autonomous trucks utilizing NVIDIA's end-to-end AI platform for training, simulation and in-vehicle computing. The strategic partnership will enable Volvo Group to develop a wide range of autonomous driving solutions for freight transport, recycling collection, public transport, construction, mining, forestry and more. This collaboration is a great validation of our long-held position that every vehicle, not just cars but also trucks, shuttles, business, taxis and many others, will have autonomous capability 1 day. Autonomous features can bring enormous value to the trucking industry, in particular as the demand of online shopping put ever greater stress on the world's transport systems. Expectations for overnight or same-day deliveries create challenges that can only be met by autonomous trucks, which can operate 24 hours a day. To help address these needs, NVIDIA has created an end-to-end platform for autonomous vehicles from AI computing infrastructure to large-scale simulation to in-car computing. Multiple customers from OEMs like Mercedes-Benz, Toyota and Volvo to Tier 1s like Bosch, Continental and ZF are already onboard. We see this as a $30 billion addressable market by 2025. Moving to the rest of the P&L. Q2 GAAP gross margins was 59.8% and non-GAAP was 60.1%, up sequentially, reflecting higher automotive development services, a favorable mix in gaming and lower component cost. GAAP operating expenses were $970 million, and non-GAAP operating expenses were $749 million, up 19% and 8% year-on-year, respectively. We remain on track for high single-digit OpEx growth in fiscal 2020 while continuing to invest in the key platforms driving our long-term growth, namely graphics, AI and self-driving cars. GAAP EPS was $0.90, down 49% from a year earlier. Non-GAAP EPS was $1.24, down 36% from a year ago. With that, let me turn to the outlook for the third quarter of fiscal 2020. We expect revenue to be $2.9 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 62% and 62.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $980 million and $765 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of approximately $25 million. GAAP and non-GAAP tax rates are both expected to be 10%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $100 million to $120 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We will be at the Jefferies conference, hardware and communications infrastructure summit, on August 27 and at the Citi Global Technology Conference on September 25. With that, we will now open the call for questions. Operator, would you please poll for the questions?","Sure. Colette, why don't you take the Switch question? And then I'll take the rest of the RTX questions. Sure. From a gaming perspective, the overall Switch or the overall console business definitely is a seasonal business. We usually expect to see production ramping in Q2 and in Q3, with it coming down likely in Q4. So you should see Switch to be a portion definitely of our gaming business in Q3. Yes. C.J., thanks for the question. RTX, as you know, is -- first of all, RTX is doing great. I think we've put all the pieces in place to bring ray tracing into the future of games. The number of games, the blockbuster games that adopted RTX is really snowballing. We announced several -- 6 games in the last couple of months. There's going to be some exciting announcements next week at gamescom. It's pretty clear now the future of gaming will include ray tracing. The number of software developers that create -- with creative tools that adopted RTX is really quite spectacular. We now have 40 -- over 40 ISV tools that was announced at SIGGRAPH that have accelerated ray tracing and video editing. And some of the applications' amazing AI capabilities for image optimization enhancement support RTX. And so looking forward, this is what I expect. I expect that ray tracing is going to drive a reinvigoration of gaming graphics. I expect that the over 100 laptops that we have RTX designed -- RTX GPUs designed into is going to contribute our growth. Notebook gaming is  one of the fastest-growing segments of the gaming platform world. The number of notebooks that are able to game is only a few percent, so it's extremely underexposed. And yet, we know that gamers are -- like the rest of us, they like thin and light notebooks, but they like it to be able to run powerful games. And so this is an area that has grown significantly for us year-over-year, and we're expecting it to grow through the end of the -- through the second half and through next year. And one of the things that's really exciting is our RTX Studio line that we introduced recently. We observed, and through our discussions with the PC industry, that the creatives are really underexposed and underserved by the latest technologies. And they want notebooks and they want PCs that have powerful graphics. They use it for 3D content creation and high definition video editing and image optimization and things like that. And we introduced a brand-new line of computers that we call RTX Studio. Now the OEMs were so excited about it. And at SIGGRAPH, we now have 27 different laptops shipping and more coming. And so I think RTX is really geared for growth. We have great games coming. We got the SUPER line of GPUs. We have all of our notebooks that were designed into that we're ramping and, of course, the new RTX Studio line. And so I expect this to be a growth market for us. With the exception of a couple of hyperscalers, C.J., I would -- we're seeing broad-based growth in data centers. In the area of training, the thing that's really exciting everybody, and everybody is racing towards, is training these large gigantic natural language understanding models, language models. The transformer model that was introduced by Google, called BERT, has since been enhanced into XLned and RoBERTa and, gosh, so many different, GP2, and Microsoft's MASS. And there's so many different versions of these language models. And in the AI, NLU, natural language understanding, is one of the most important areas that everybody's racing to go to. And so these models are really, really large. It's over 1,000x larger than image models that we're training just a few years ago, and they're just gigantic models. It's one of the reasons why we built the DGX SuperPOD so that we could train these gigantic models in a reasonable amount of time. The second area -- so that's training in the hyperscalers. The second area where we're seeing enormous amounts of activity has to do with trying to put these conversational AI models into services so that they could be interactive and in real time. Whereas photo tagging and photo enhancement is something that you could put off-line and you could do that while you have excess capacity when it's off of the most busy time of the day. You can't do that with language and conversational AI. You better to respond to the person in real time. And so the performance that's required is significant. But more importantly, the number of models necessary for conversational AI from speech recognition to language understanding to recommendation systems to text-to-speech to wave synthesis, these 5, 6, 7 models have to be processed in real time -- in series and in real time so that you can have a reasonable conversation with the AI agent. And so these type of activities is really driving interest and activity at all of the hyperscalers. My expectation is that this is going to continue to be a big growth opportunity for us. But more importantly, in addition to that, we're seeing that AI is -- the wave of AI is going from the cloud to the enterprise to the edge and all the way out to the autonomous systems. The place where we're seeing a lot of excitement, and we talked about that in the past and we're seeing growth there, has to do with the vertical industry enterprises that are starting to adopt AI to create new products, whether it's a delivery robot or some kind of a chat bot or the ability to detect fraud in financial services, these applications in vertical industries are really spreading all over the place. There's some over 4,000 AI start-ups around the world. And the way that we engage them is they use our platform to start developing AI in the cloud. And as you know, we're the only AI platform that's available on-prem and in every single cloud. And so they can use our AI platforms for -- in all the clouds, which is driving our cloud computing, external cloud computing growth. And then they can also use it on-prem if their usage really grows significantly. And that's one of the reasons why our Tesla for OEMs and DGX is growing. And so we're seeing broad-based excitement around AI as they use it for their products and new services. And these 4,000, 4,500 start-ups around the world is really driving consumption of that. I actually had 2 as well, one quick one for Colette and one for Jensen. Colette, good to see the gross margin recovery getting into October. Is this 62% to 63% range a more sustainable level and perhaps a level you could grow off of as sales get more normalized levels? And then a bigger question is for Jensen. Again, on the data center side, Jensen, when I look back between -- 2015 to 2018, your data center business essentially grew 10x. And then the last year has been a tough one with the slowdown in cloud CapEx and so forth. When do you think your data center starts to grow back on a year-to-year -- on a year-on-year basis? Can that happen sometime -- later this year? And then just longer term, what is the right way to think about this business? Does it go back to prior levels? Does it go at a different phase? This is the one part of the business that I think is toughest for us to model, so any color would be very helpful. Great. So let me start first with your question, Vivek, regarding gross margins. Yes, thanks for recognizing that we are moving towards our expectations that, over time, we'll continue to see our overall volumes improve. Essentially, our business is normalized. We've reached normalized levels through the last couple of quarters. And this quarter, just very similar to what we will see going forward, is mix is the largest driver, what drives our overall gross margins and our gross margin improvements. Yes, Vivek, if you look at the last several years, there's no question our data center business has grown a lot. And my expectation is that it's going to grow a lot more, and let me explain to you why. Aside from a couple -- a few of uncontrollable circumstances and the exception of a couple of large customers, the overall trend, the broad-based trend, of our data center business is upward, to the right. And it is growing very nicely. There's a couple of different dynamics that's causing that on first principles to grow. And of course, one of them is as AI is well known now to require accelerated computing, our computing architecture is really ideal for it. AI is not just one network. It's thousands of different types of networks, and these networks are getting more and more complex over time, the amount of data you have to process is enormous. And so like all software programs, you can't predict exactly how the software is going to get programmed. And having a programmable architecture like CUDA and yet optimized for AI like Tensor Cores that we've created is really the ideal architecture. We know also that AI is the most powerful technology force of our time. The ability for machines to learn and write software by itself and write software that no humans can write is pretty extraordinary. And the applications of AI, as you guys are watching yourself, are just spreading in every single industry. And so the way we think about AI is in waves, if you will. The first wave of AI is developing the computer architecture, and that was the first part where -- that's when a lot of people discovered who we are, and we emerged into the world of high-performance computing in AI. The second wave is applying the AI for cloud service providers or hyperscalers. They have a large amount of data. They have a lot of consumer applications. Many of them are not life-critical and so, therefore, the application of an early technology -- early-adoption technology was really viable. And so you saw hyperscalers adopt AI. And the thing that's really exciting for us is beyond recommendations, beyond image enhancement, the area where we believe the most important application for AI is likely conversational AI. Most people talking and asking questions and talking to their mobile devices and looking for something or asking for directions. Instead of having a page of -- a list of options, it responds with an answer that is very likely a good one. The next phase of AI is what we call vertical industry enterprise AI. And this is where companies are using it not just to accelerate the business process internally, but they're using AI to create new products and services. They could be new medical instruments to IoT-based medical instruments to monitor your health. It could be something related to an application that -- used for financial services for forecasting or for fraud detection. It could be some kind of device that delivers pizza to you, delivery bots. And the combination of IoT and artificial intelligence, for the very first time, you actually have the software capabilities to make use of all of these sensors that you're putting all over the world. And that's the next phase of growth. And it affects companies from large industrials, transportation companies, retailers, you name it. Health care companies, you name it. And so that phase of growth of AI is the phase that we're about to enter into. And then the longer term is an industry that we all know to be extremely large, but it takes time because it's life-critical, and it has to do with transportation. It's a $100 trillion industry. We know it's going to be automated. We know that everything that moves in the future will be autonomous or have autonomous capabilities. And that's just a matter of time before we realize its full potential. And so the net of it all is that I believe that AI is the single most powerful technology force of our time, and that's why we're all in on it. And we know that acceleration and accelerated computing is the perfect model for that. And it started in the cloud, but it's going to keep moving out into the edge and through data centers and enterprises and hopefully -- well, eventually, all the way out into autonomous devices and machines in the real world. And so this is a big market, and I'm super enthusiastic about it. Yes, Toshiya, I got to tell you, I'm less good at normal pre -- near-term productions than I am good at thinking about long-term dynamics. But let me talk to you about inference. Our inference business is -- remains robust. It's double digits. It's a large part of our business. And -- but more importantly, the 2 dynamics that I think are near term and that's going to drive growth, number one is interactive conversational AI, interactive conversational AI inference. If you simply ask a chat bot a simple question, where is the closest pizza and you would -- pizza shop, and you would like to have a conversation with this bot, it would have to do speech recognition, it has to understand what it is that you asked about, it has to look it up in a recommender based on the locations you're at, maybe your preferences of styles of pizza and the price ranges that you're interested and how far you're willing to go, to go get it. It has to recommend a pizza shop for you to go to. It has to then translate that from text-to-speech and then into human -- a human understand a voice. And those models have to happen in just a few -- ideally, a few hundred milliseconds. Currently, it's not that. And it makes it really hard for these services to be deployed quite broadly and used for all kinds of different applications. And so that's the near-term opportunity, it's interactive conversational AI inference. And you could just imagine every single hyperscaler racing to go make this possible because recently, we had some important breakthroughs in machine learning language models. The BERT model that I mentioned earlier is really, really an important development, and it's caused a large number of derivatives that has improved upon it. And so near-term conversational AI inference. We're also seeing near term the inference at the edge. There are many types of applications where because of the laws of physics reasons, the speed of light reasons or the economics reasons or data sovereignty reasons, it's not possible to stream the data to the cloud and have the inference done at the cloud. You have to do that at the edge. You need the latency to be low, the amount of data that you're streaming is continuous. And so you don't want to be paying for that line rate the whole time, and maybe the data is of great confidentiality or privacy. And so we're seeing a lot of excitement and a lot of development for edge AI. Smart retail, smart warehouses, smart factories, smart cities, smart airports, you just make a list of those kind of things, basically locations where there is a lot of activity, where safety or cost or large amount of materials is passing through, you could just imagine the applications. All of those really want to be edge computing systems and edge inference systems. And so those are near term -- 2 near-term drivers, and I think it's fair to say that both of them are quite large opportunities. So to answer your question regarding gross margin in a little bit more detail, probably our largest area that we expect improvement in terms of our mix is our mix return regarding our overall gaming business. We expect to have a full quarter of our SUPER lineup within the next quarter, including our RTX as well as our notebook becoming a bigger mix as well as it grows. These drivers are one of the largest reasons why we see that growth in our gross margin. We always think about our component cost, our overall cost of manufacturing, so this is always baked in over time, but we'll continue to see improvements on that as well. Our hyperscale data center with a few customers don't give us very much -- we don't get very much visibility from a handful of customers in hyperscale. However, we're seeing broad-based growth and excitement in data centers. And the way to think about data center, our data center business consists of hyperscale training, internal training, hyperscale inference, cloud computing -- and that's hyperscale, and that cloud is a public cloud. And then we have vertical industry enterprise, what sometimes we call enterprise, vertical industry enterprise, it could be transportation companies, retailers, telcos, vertical industry adoption of AI either to accelerate their business or to develop new products and services. And then the -- so when you look at our data center from that perspective and these pieces, although we don't see as much -- we don't get as much visibility as we like in a couple of the large customers, the rest of the hyperscalers, we're seeing broad-based growth. And so we're experiencing the enthusiasm and the energy that maybe the others are. And so we'll keep reporting -- updating you guys as we go. We'll see how it goes. Well, Volta -- data center products can churn that fast. We -- gamers could churn products quickly because they're bought and sold one at a time. But data centers -- data center infrastructure really has to be planned properly, and the build-out takes time. And we expect Volta to be successful all the way through next year. And software still continues to be improved on it. We're still improving systems on it. And in fact, just 1 year -- in just 1 year, we improved our AI performance on Volta by almost 2x, 80%. And so you could just imagine the amount of software that's built on top of Volta and all the Tensor Cores and all the GPUs connected with NVLink and the large number of nodes that are connected to build supercomputers. The software of building these systems, large-scale systems, is really, really hard. And that's one of the reasons why you hear people talk about chips, but they never show up because building the software is just an enormous undertaking. The number of software engineers we have in the company is in the thousands, and we have the benefit of having built on top of this architecture for over 1.5 decades. And so when we're able to deploy into data centers as quickly as we do, I think we kind of lose sight of how hard it is to do that in the first place. The last time a new processor entered into a data center was an x86 Xeon, and you just don't bring processors in the data centers that frequently or that easily. And so I think the way to think about Volta is that it's surely in its prime, and it's going to keep -- continue to do well all the way through next year. In regard to our guidance on revenue, and we do guide in terms of the total. You have seen, in this last quarter, we executed a sequential increase really focusing on moving to a normalization of our gaming business. And we're now approaching the second half of the year getting ready for the back to school and the holidays. So you should expect also our gaming business to continue to grow to reach that full normalization by the end of Q3. We do expect the rest of our platforms to likely also grow. We have a couple different models on how that will come out. But yes, we do expect our data center business to grow, and then we'll see on the rest of our businesses as well. SUPER is off to a great start. Goodness, SUPER is off to a super start. And if you look at -- if you do channel checks all over, even though we've got a lot of products in the channel and we -- last quarter was a transitional quarter for us actually. And we didn't -- we shipped SUPER later in the quarter. But because the entire ecosystem and all of our execution engines are so primed, we were able to ship a fair number through the channel. And so -- and yet, if you do spot checks all around the world, they're sold out almost everywhere. And the pricing in the spot market is drifting higher than MSRP. That just tells you something about demand. And so that's really exciting. SUPER is off to a super start for -- and at this point, it's a foregone conclusion that we're going to buy a new graphics card, and it's going to the last 2, 3, 4 years to not have ray tracing is just crazy. Ray tracing content just keeps coming out. And between the performance of SUPER and the fact that it has ray tracing hardware, it's going to be super well positioned for -- throughout all of next year. Your question about APIs and software programmability. APIs is just one of the issues. The large issue about processors is how do you program it. The reason why x86s and CPUs are so popular is because they solve the great challenge of software developers: how to program a computer. And how to program a computer and how to compile for that computer is a paramount concern to computer science, and it's an area of tremendous research. Going from single CPU to multi-core CPUs was a great challenge. Going from multi-core CPUs to multi-node multi-core CPUs is an enormous challenge. And yet, when we created CUDA in our GPUs, we went from 1 CPU core or one processor core to a few to now, in the case of large-scale systems, millions of processor cores. And how do you program such a computer across multi-GPU, multi-node? It's a concept that's not easy to grasp. And so I don't really know how one programming approach or a simple API is going to make 7 different type of weird things work together. And I can't make it fit in my head. But programming isn't as simple as a PowerPoint slide, I guess. And I think it's just -- time will tell whether one programming approach could fit 7 different types of processors when no time in history has it ever happened. I wonder if you could talk about the strength in the automotive business. Looks like the services piece of that is getting to be bigger. What's the outlook for that part of the business? And can you give us a sense of the mix between services and components at this point? Sure. Thanks, Joe. Our approach to autonomous vehicles comes in basically 2 parts. The first part is a full stack, which is building the architected processor, the system, the system software and all of the driving applications on top, including the deep neural nets. The second part of it, we call that a full stack self-driving car computer. The second part of DRIVE includes an end-to-end AV development system. For those who would like to use our processors, use our system software but create their own applications, we created a system that allows -- basically shares with them our computing infrastructure that we built for ourselves that allows them to do end-to-end development from deep learning development to the application of AV to simulating that application to doing regression testing of that application before they deploy it into a car. And the 2 systems that we use there is called DGX for training and Constellation for simulation and what is called Replay. And then the third part of our business model is development agreements, otherwise known as NRE. These 3 elements, full stack computer, end-to-end development flow and NRE project development -- product development consists of the overall DRIVE business. And so although the cars will take several years to go into production, we're seeing a lot of interest in working with us to develop self-driving cars using our development systems and entering into development projects. And so we're -- the number of autonomous vehicle projects is quite large around the world as you can imagine. And so my sense is that we're going to continue to do well here. The additional part of autonomous vehicles and where the capability has been derived and is going to seal up more near-term opportunities has to do with things like delivery shuttles, self-driving shuttles and maybe cargo movers inside walled warehouses. Those kind of autonomous machines require basically the same technology, but it's sooner and easier to deploy. And so we are seeing a lot of excitement around that area. We launched -- well, first of all, the answer is that RTX adoption is faster than Pascal's adoption if you normalize to time 0 of launch. The reason for that is Pascal launched top to bottom on the same day. And as you guys know, we weren't able to do that for Turing. But if we did that for Turing, the adoption rate is actually faster. And to me, it's a rather sensible. And the reason for that is because Pascal was basically DX12. And Maxwell was DX12. And Turing is the world's first DXR, the first ray tracing GPU, brand-new functionality, brand-new API and a lot more performance. And so I think it's sensible that Turing's adoption is going to be rapid. The second element of Turing is something that we've never talked about before. We're mentioning it more and more because it's such an exciting book market for us is notebooks. The install base of Pascal has a very, very little notebook in it. And the reason for that is because, in the past, we were never able to put a high performance gaming GPU into a thin and light notebook until we invented Max-Q. And in combination with our energy efficiency, we were able to -- we're now able to put a 2080 into a laptop, and it's still beautiful. And so this is effectively a brand-new growth market for us. And with so few people and so few gamers in the world that are able to game on a laptop, I think this is going to be a nice growth market for us. And then the new market that we introduced and launched this last quarter is called RTX Studio. And this is an underserved segment of the market where consumers, enthusiasts, they could be artists that are working on small firms, they need powerful computers to do their work. They need powerful computers to do rendering and high-definition video editing. And yet it's underserved by workstations because workstations are really sold on a B2B basis into large enterprises. And so we aligned all of the OEMs and created a whole new line of notebooks called RTX Studio. And the enthusiasm has been great. We've launched 27 different laptops, and I'm looking forward to seeing the results of that. This is tens of millions of people who are creators. Some of them professionals, some of them hobbyists. And they use Adobe suites, they use Autodesk in their suites and some of them use SolidWorks and some of them use all kinds of renders, like blender. And these are 3D artists and video artists, and this digital content creation is the modern way of creativity. And so this is an underserved market that we're excited to go serve with RTX Studio. So to answer your question here, Stacy, on what we refer to when we're discussing the broad-based growth is the substantial expansion that we have on the types of customers and the industries that we are now approaching. As you know, even a year ago, we had a very, very small base in terms of industry-based hyper -- excuse me, industry-based AI workloads that they were using. Over this last quarter, we're continuing to see strong growth as we roll out all different types of AI solutions, both across the U.S. and worldwide, to these overall customers. Our hyperscalers, again, a couple of them, not necessarily growing. Some of them are flat and some of them are growing depending on whether or not that's for cloud instances or whether or not they're using it for internal use. So we believe that our continued growth with the industries is important for us for the long term to expand the use of AI, and we're just really pleased with what we're seeing in that growth this quarter. Thanks, everyone. We're happy with our results this quarter and our return to growth across our platforms. Gaming is doing great. It's great to see NVIDIA RTX reinvigorating the industry. GeForce has several growth drivers. Ray traced games continue to gain momentum. A large number of gaming laptops are rolling out, and our new Studio platform is reaching the large underserved community of creators. Outside a few hyperscalers, we're seeing broad-based growth in data centers. AI is the most powerful technology force of our time and a once-in-a-lifetime opportunity. More and more enterprises are using AI to create new products and services while leveraging AI to drive ultra-efficiency and speed in their business. And with hyperscalers racing to harness recent breakthroughs in conversational AI, we see growing engagements in training as well as interactive conversational inference. RTX, CUDA accelerated computing, AI, autonomous vehicles, the work we're doing is important, impactful and incredibly fun. We're just grateful there is so much of it. We look forward to updating you on our progress next quarter.","I guess first question on gaming, how should we think about your outlook into the October quarter vis--vis kind of normal seasonality? How are you thinking about Switch within that? And considering now that you have full Turing lineup as well as content truly coming to the forefront here, how do you think about trends beyond the October quarter? Very helpful. If I could follow up on the data center side, perhaps you can speak directly just to the hyperscale side, both internal and cloud, and whether you're seeing any green shoots, any signs of life there and how you're thinking about what that rate of recovery could look like over time. I had 2 as well, one for Jensen and the other for Colette. Jensen, you guys called out inference as a significant contributor to growth in data center last quarter. I think you guys talked about it being a double-digit percentage contributor, curious what you saw from inference in the quarter. And more importantly, if you can talk about the outlook, both near term and long term, as it relates to inference, that'll be helpful. And then secondly, for Colette, just want to double click on the gross margin question. The sequential improvement that you're guiding to is a pretty significant number. So I was just hoping if you can kind of break it down for us in terms of overall volume growth mix dynamics, both between segments and within segments and also to the extent DRAM pricing is impacting that, any color on that will be helpful as well. Again, your data center business, many of your peers on the compute and storage side are seeing spending recovery by cloud and hyperscalers in the second half of this year after a similar weak first half of the year. You guys saw some growth in Q2 driven primarily by enterprise. It seems like you had some broadening out of the customer spending this quarter. Inferencing continues to see strong momentum. Would you guys expect that this translates into a double-digit percentage sequential growth in data center in Q3 off of the low base in Q2? I had 2. I guess first for Jensen, Volta's been around now for about 2 years. Do you see signs of demand maybe building up ahead of the new set of nanometer products, whenever that comes out? I guess I'm just wondering whether there's some element of this is more around product cadence that gets resolved as you do roll out the product. That's the first question. And then I guess, the second question, Colette, is of the $300 million growth into October, it sounds like Switch is pretty flat, but I'm wondering if you can give us maybe some qualitative sense of where the growth is coming from, is it maybe like 2/3 gaming and 1/3 data centers, something like that? A couple of questions. I guess the first one is, Jensen, if you have any, I guess, high-level qualitative commentary on how the new SUPER upgrades of your Turing platform have been received in the market and how you might think about them progressing through the year. And then, I guess, the second question is a bigger one. Intel's talked quite openly about One API. The software stack at Xilinx is progressing with Versal ACAP. I mean you guys get a lot of credit for the decade of work that you've done on CUDA. But I wonder if you might comment on if you've seen any movement in the competitive landscape on the software side for the data center space. Congratulations on the improved performance. At your Analyst Day back a couple of months ago, you had highlighted the installed base opportunity for RTX. And I think at that point in time, you talked about 50% being Pascal base, 48% being pre-Pascal. You also alluded to the fact that you were seeing a positive mix shift higher in terms of the price points of this RTX cycle. So I'm curious, where do we stand on the current product cycle? And what are you seeing currently as we go through this product cycle on the Turing platforms? I have 2 for Colette. My first question is on data center. So I know you say that you have a broad-based growth except for a few hyperscalers. But you only grew at 3% sequentially, about $20 million. That doesn't sound like broad-based growth to me unless like -- did the hyperscalers get worse? Or are they just still so much bigger than like the rest of it? I guess, what's going on in data center? How do I wrap my head around like broad-based growth with relatively minimal growth observed? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q4 2019,2263,4776,1731,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2019. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2020. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, February 14, 2019, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. As you know, we lowered our fourth quarter guidance on January 28, and our results are in line with our pre-announcement. Q4 revenue was $2.21 billion, down 24% from a year earlier, driven primarily by a 45% year-on-year decline in gaming. Full year revenue was $11.72 billion, up 21% from our previous year. Starting with our gaming business. Revenue of $954 million was down 45% year-on-year and down 46% sequentially, weaker than our expectations heading into the quarter. Full year revenue was up 13% to $6.25 billion. Three factors contributed to the Q4 gaming revenue decline. First, post-crypto inventory of GPUs in the channel caused us to reduce shipments in order to allow excess channel inventory to sell through. We expect channel inventories to normalize in Q1, in line with 1- to 2-quarter time line we had outlined on our previous earnings call. Second, deteriorating macroeconomic conditions, particularly in China, impacted consumer demand for our GPUs. And third, sales of certain high-end GPUs using our new Turing architecture, including the GeForce RTX 2080 and 2070, were lower than we expected for the launch of a new architecture. These products deliver a revolutionary leap in performance and innovation with real-time ray tracing and AI, but some customers may have delayed their purchase while waiting for new -- for lower price points or further demonstration of the RTX technology in actual games. The significant volatility in our gaming business over the last few quarters has been challenging to model. Crypto mining demand and its after effects have distorted the quarter-to-quarter trends in the gaming business and obscured its underlying trend line. Let me try to give you some visibility into what we believe the normalized business looks like. As you know, our gaming business consists of desktop gaming, notebook gaming and gaming console products. To get a sense of the underlying run rate in our gaming business last year, it is helpful to look at desktop gaming revenue across a period that doesn't include crypto demand. Let's look at the 4 quarters starting from Q2 of last year to the current quarter or Q1 this year. In Q2 and Q3 of last year, with the benefit of hindsight, we shipped a higher amount of desktop gaming products relative to where end demand turned out to be. To allow the channel to work down that excess channel inventory, we shipped a lower amount relative to end demand in Q4, and will do so again in Q1. Therefore, exiting Q1, we expect channel inventories to be at normal levels. On average, our desktop gaming revenue across these 4 quarters is about $900 million. We believe this represents the normalized level of desktop gaming for this period. Notebook gaming and gaming consoles have averaged close to $500 million per quarter over the same 4 quarters. Thus, in total, we believe our normalized quarterly gaming business revenue run rate is approximately $1.4 billion. As we look past Q1, we expect the channel inventory correction to be behind us and our business to have bottomed. On a full year basis, we expect our gaming business to be down slightly, given the tough first half compares with growth in Turing and notebook gaming. At CES last month, we launched into the recovery of our gaming business. We announced the GeForce RTX 2060 at the midrange price point of $349. The 2060 delivers a 60% performance improvement over the GTX 1060 while also bringing Turing's real-time ray tracing and AI features to the mass market for the first time. The 2060 has received rave reviews and is off to a great start. In addition, we announced a record of 40-plus new Turing-based gaming laptops, which became available on January 29. This is more than double the number of GeForce-powered notebooks in the market last year, featuring the energy efficiency of the Turing architecture and the light laptops are able to deliver the performance of desktop gaming PCs. We expect GeForce laptops to continue to be the fastest-growing segment of gaming. We are also pleased to see growing momentum in the RTX ecosystem as more game developers are creating content to take advantage of the Turing architecture's amazing capabilities. Just this week, DLSS technology is becoming available in 2 blockbuster games, Battlefield V and Metro Exodus and Anthem coming soon. In addition, at CES, Justice and Atomic Heart showed demos featuring ray tracing and DLSS. And a large pipeline of games plan to integrate RTX technology. Pairing DLSS with ray tracing can provide comparable frame rates to traditional rasterization but also much more beautiful cinematic visuals, the best of both worlds. This is the next generation of gaming. While this was a challenging quarter in our gaming business, we look forward to putting the channel inventory correction behind us and building on a solid foundation of our Turing architecture. Moving to datacenter. Revenue was $679 million, up 12% year-on-year and down 14% sequentially. Full year datacenter revenue was $2.93 billion, up a strong 52%. The Q4 sales decline was broad-based across vertical end markets and geographies. As the quarter progressed, customers around the world became increasingly cautious, due to rising economic uncertainty and a number of deals did not close in January. In addition, hyperscale and cloud purchases declined both sequentially and year-on-year as several customers paused at the end of the year. We believe the pause is temporary. The strength of NVIDIA's accelerated computing platform remains intact. We continue to lead the industry in performance for scientific computing and deep learning. And with CUDA's programmability, we can continue to expand the value of our platform. For example, we recently announced RAPIDS or CUDA acceleration stack for data analytics and machine learning. In December, the first objective third-party AI benchmark called MLPerf became available, and NVIDIA captured the top spot in the 6 test categories for training deep learning models that we competed in. And in January, Google Cloud announced that NVIDIA T4 Tensor Core GPUs are now available in beta in its datacenters in the U.S., Europe, Brazil, India, Singapore and Tokyo. The T4 is a universal cloud GPU that accelerates a variety of workloads, including high-performance computing, deep learning training and inference, broader machine learning, data analytics and graphics. Our visibility remains low in the current cautious spending environment, and we don't forecast a meaningful recovery in the data center segment until later in the year. However, we are working closely with hyperscales around the world to integrate NVIDIA TensorRT software and Tensor Core GPUs into their inference production flow. Inference currently drives less than 10% of our data center business, but represents a significant expansion of our addressable market opportunity going forward. We have also strengthened our product portfolio and go-to-market capabilities to address vertical industries that have an enormous data and analytics requirements, such as automotive, financial services, retail, health care and consumer Internet services. With our RAPIDS software stack, NVIDIA can accelerate data analytics and machine learning, and we have -- as we have done in deep learning. And we made it easier for customers to adopt our technology by partnering with Cisco, IBM, NetApp and Pure Storage to create pre-integrated systems that can be sold through their global IT channels. Moving to pro visualization. Revenue reached $293 million, up 15% from the prior year and down 4% sequentially. Full year revenue was $1.13 billion, up 21% year-on-year. New applications like data science, AI and VR as well as the need for thin and light mobile workstations remain key growth drivers for the business. We had key wins in the quarter, including Boeing, Google, LinkedIn and Toyota for applications including AI and robotics. This past week, with our partners, HP, Dell, Lenovo, we announced the availability of Quadro RTX workstations. Quadro RTX is the most significant workstation GPU upgrade in 10 years. It will enable millions of designers and creative architects for the first time to work interactively with super high resolution media and photorealistic 3D rendering, enabling them to be creative with dramatically improved productivity. Finally, turning to automotive. Q4 revenue was $163 million, up 23% from a year ago and down 5% sequentially. Full year revenue reached $641 million, up 15%. The sequential decline was largely seasonal. The year-on-year growth was driven by the increasingly adoption of next-generation AI cockpit solutions and autonomous vehicle development deals, partially offset by declines in legacy infotainment. Last month at CES, we announced DRIVE AutoPilot, the world's first commercially available Level 2+ self-driving car computer. The system offers sophisticated automated driving features that far surpass today's ADAS offerings, increasing the vehicle's performance, functionality and road safety while the driver remains in control. We delivered these capabilities. DRIVE AutoPilot uses multiple deep neural networks; surround camera perception, both in and outside of the car; and significant AI processing capability. Systems from our Tier 1 partners, including Bosch, Continental, Veoneer and ZF were all on display on at our booths. Volvo, as announced back in October, was our first Level 2+ design win with cars slated for production in the early 2020s. Mercedes-Benz has also chosen NVIDIA for its next-generation autonomous vehicle and cockpit computer. This centralized AI computing system replaces dozens of smaller processors inside current cars. DRIVE AutoPilot is a major milestone for NVIDIA and takes our high-functioning, self-driving capabilities into the mass market. This will be an important year for robo-taxi pilots and initial Level 2 design wins. Moving to the rest of the P&L and balance sheet. Q4 GAAP gross margins was 54.7%. And non-GAAP was 56.0%, down sequentially and year-on-year, primarily due to a $128 million charge for DRAM, boards and other components associated with our lower-than-expected Q4 revenue and current market conditions. GAAP operating expenses were $913 million, and non-GAAP operating expenses were $755 million, up 25% and 24% year-on-year, respectively. GAAP EPS was $0.92, down 48% from a year earlier. Full year GAAP EPS was $6.63, up 38% from the prior year. Non-GAAP EPS was $0.80, down 53% from a year ago. Full year non-GAAP EPS was $6.64, up 35% from the prior year. We returned $1.95 billion to shareholders in the fiscal year through a combination of quarterly dividends and share repurchases. As we announced last quarter, we plan to return $3 billion to shareholders through the end of fiscal 2020 in the form of dividends and buybacks. We repurchased $700 million during the fourth quarter of fiscal 2019. With that, let me turn to the outlook for the first quarter of fiscal 2020. We expect revenue to be $2.2 billion plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 58.8% and 59%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $930 million and $755 million, respectively. GAAP and non-GAAP OI&E are both expected to be an income of $20 million. GAAP and non-GAAP tax rates are both expected to be 10% plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $150 million to $170 million. For fiscal 2020, we expect Q1 to mark the bottom as we pass the inventory correction in gaming. We expect total revenue for the year to be flat to down slightly with growth in our 4 end markets, compensating for the absence of crypto revenue and the excess selling from last year. We plan to grow OpEx in the high single digits this year, and we continue to invest in our focused growth areas of graphics, AI and self-driving cars. Further financial details are included in the CFO commentary, and other information is available on our IR website. In closing, I'd like to highlight upcoming events for the financial community. We'll be presenting at the Morgan Stanley Technology, Media and Telecom Conference on February 26. And our next earnings call to discuss our financial results for the quarter of fiscal 2020 will take place on May 15. We will now open the call for questions. Operator, would you please poll for questions? Thank you.","Yes, Toshiya. When we launched the 2070 and 2080, it was the first time we've ever launched a new generation where the only available SKUs were very high end. And in addition to that, the early boards that came out into the marketplace were the special edition and the overclock versions, and the MSRP versions didn't show up for some short time after, couple of months after. And so the conditions weren't ideal, if you will. Now we weren't able to launch into the mainstream segment with 2060 for all the reasons that I think everybody understands now. And so I think that the situation wasn't ideal. When you take a look at our situation now, every single graphics card had the best performance at its price point, and it remain so today. And I think that right out of the box, it delivered excellent performance. It is true that everybody was hoping to see more games with RTX on day 1. But it's such a new technology with ray tracing and AI for image processing that it's only really possible to make available with new games, which is tied to the schedules of new games. And now they're starting to come out. Battlefield V, Metro Exodus, I think the reviews from this week are just spectacular. People are finally realizing what it is that we were talking about. And that it's possible with RTX technology, the combination of applying ray tracing and AI for us to deliver much more beautiful images without sacrificing performance. And so I think people are starting to understand now the benefits of the RTX technology. And we just needed some time to demonstrate it. And I think the takeaway is simply this. RTX is the best graphics card at every single price point without using ray tracing technology. And for new games that are coming out, each one of the new games that come out in the future will apply ray tracing, and work with developers to apply ray tracing technology. I think everybody agrees that it is surely the next generation. And then probably one of the biggest stories that came out just last week is Unreal Engine and Unity, both of the game engines are going to incorporate RTX and ray tracing technology in the engine itself. And so all future games in the future will be able to take advantage of that. So that's a really big news, and I'm excited about that. So Toshiya, to answer your second question regarding our inventory balance. Our inventory balance at the end of Q4 rose just due to the weaker-than-expected finish to Q4. Inventory right now is primarily related to Turing, Volta and DGX. And we don't expect any further write-downs as we have incorporated approximately $128 million of write-downs within the current Q4. The slowdown is broad-based. We saw it across every vertical, every geography. There was just a level of cautiousness across all of the enterprise customers in the cloud service providers that we've not experienced in a while. And so I think that it has to be temporary. The computing needs of Earth has not certainly been satisfied with what we shipped last quarter. And so I think that the demand will return and customers will return. Our situation in datacenters is dramatically better year-over-year. And if you take a look at where we are, our deep learning solution is unquestionably the best in the world. We introduced T4 with inference capability. It's the world's first universal cloud GPU, and it does everything that NVIDIA does all in one GPU in 75 watts. And so it fits into every hyperscale datacenter. We're engaged with Internet service providers all around the world, optimizing, importing their high-production models, networks, so that we could deploy it into production. So we now have 4 different new growth drivers for our data center in addition to deep learning and scientific computing. We have inference that we're actively working on. We have data analytics that's called RAPIDS. Some people call it big data, but data analytics and machine learning. The third is rendering. And because of the partnerships that we've developed and the excitement that people see around enterprises around the world, we've developed partnerships with large IT companies to pre-configure systems that make it easier for enterprises to be able to adopt our technology. So we have 4 new ways for us to grow our enterprise business. And so we're looking forward to when the pause releases, and now we'll get back to growing. I just had a clarification and a question. On the clarification, gross margins, Colette, what is the normalized run rate for gross margin that as you get your sales back to normalized levels, how should we think about the trajectory of gross margins? And will there be any impact from the balance sheet inventory? And then on the question, Jensen, can you give us more reassurance that gaming is still a growth business? But I understand that over the last year, there's been a lot of confusion, there's been macro issues. But if you look at the number of gamers and the mix of product that they are buying, so essentially the sell-through to gamers, has that been on an upward trajectory? And as part of that, when do you think we could see Turing exceed the demand you saw for Pascal? So I'll go first. Thanks for the question. On gross margin, our gross margin, the largest contributor to our absolute gross margin is really just the mix of our products. The mix of our products based on our market platforms but also the mix of our products within data center as well as within gaming. We provided guidance for Q1, which has a good level of confidence from us, and we'll see how it goes from there. Vivek, the fundamentals of gaming has not changed. There are more gamers than ever. Games are better than ever. There's been a recent shift in the popularity of multiplayer competitive eSports-like games. That's good for hardware. It lowers the barrier to entry because it's free to play with the exception of downloadable content. And so the barriers to entry is lower. But you could see that the excitement around Fortnite and recently with Apex Legend, PUBG is still popular, League of Legends is still popular. And so this genre of games, these genre of games is both competitive, requires great hardware. It attracts a lot more players because it's social, and you want to play with your friends. And it's much stickier because it happens to be social. It happens to be a game where you had to play with a whole bunch of other people. So I think that gaming is vibrant as ever before. If you take the methodology that Colette described earlier and you averaged out our underlying gaming business and you compare that to a year before, surely it grew. If you compare that -- if you compare also the rate of which our gaming notebook is growing, I think that's pretty exciting. I think last year, we had mentioned it before that our gaming notebook business grew 50% year-over-year. And just at CES, the number of new notebook designs that came out with Turing because of an invention that we created called Max-Q and because of the energy efficiency of the Turing architecture, you can now make notebooks that are really wonderful and also high performing at the same time. And so I think the dynamics are the same, and gaming is going to continue to be a growth business. I'll start, and I'll let Jensen finish that question. So along the lines of Jensen's response in terms of what we do believe are the key drivers of gaming and everything still intact in terms of gaming, both with our Turing architecture, the growth expected with our Turing architecture as well as the growth from the notebook, we do believe will be great drivers as we head into the rest of the year. We'll have to wait and see in terms of how that plays out, but that is really the underlying reason why we think the growth will continue. Yes. I think your math isn't wrong. The part that you probably didn't consider is notebook. Our GeForce notebook business is quite large. We don't see them in high-performance computing. And so I haven't found where -- we haven't met them in high-performance computing and deep learning and in the areas that we serve. And so competitively, I can't -- I don't really -- we don't see it. But the bigger picture, I think is this, that the market segments that we serve, whether it's in deep learning, machine learning, data analytics, those segments are really quite large. And I think that it is, unquestionably, the future of high-performance computing is going to be highly data driven, both computational methods, algorithmic methods as well as data-driven methods. And so I think the fundamental trend has not changed. We have 4 new growth drivers, 4 new ways to grow in the datacenter. The first one, of course, is inference. We're making a lot of progress there. T4 is doing great. I think we're going to be quite successful with T4s. You just got to keep saying that. It has second-generation Tensor Core, 75 watts. And you can use that for training. You can use it for inference. You could use it for remote graphics. You could use it for high-performance computing. And it fits literally into any hyperscale data center. The second way is data analytics. This is a brand-new thing for us. You must know that big data and using data to predict dynamics in the marketplace is really important in retail, in e-tail, in health care, in financial services. And there's never been an accelerated approach to solve this problem for people. And because of the flexibility of CUDA and because of the performance of our architecture, over the course of last year, we reengineered the entire data analytics stack, so that we can accelerate it. It's called RAPIDS. That work is really, really important, and I hope to give you guys updates on that on a regular basis. Rendering is a brand-new market for us because of Turing. Finally, we can render photorealistic images in accelerated way. There are millions of servers in the world that are driving render farms, and they're getting upgraded on a regular basis. And then lastly, we've been successful with CSPs because they're easy for us to reach. But the world's enterprises are far and many and they're giant industries. And our companies' sales coverage doesn't allow us to reach every single health care company and every single insurance company and retail company. And that's where our network of partners really come in. We have great partnerships with HP. We have great partnerships with Dell and Cisco and IBM. And now we've developed relationships with the storage vendors, so that -- and the reason for that is because most of these big data problems require a great deal of storage. And they both -- they all see, they see the opportunities that we've created. And we came together to create pre-configured systems that are optimized and tuned, and these high-performance systems that you can just bring into the company, prop up and install. And we're seeing a lot of great success with that. And so we have 4 different ways to grow our datacenter business, and we're enthusiastic about it. I'm optimistic about it. Yes. Our guidance for the next quarter is a makeup of many different types of options across our market segment. We feel confident in terms of that roll-up, as we've provided the guidance today. And we'll just have to see how that ends out. With the expectation that we will be flat or slightly down for the full year, you are correct in some case that we're growing to have to build up to that over the course of the several next quarters. Likely, the second half of the year, we'll definitely be stronger than the first half of the year, and that is our expectation at this time. Yes, Tim, one of the things to keep in mind is that we have 4 growth drivers. We have 4 growth businesses. Our datacenter business is growing. It's unquestionably that our footprint is larger than ever. Our pro viz business is growing. Our workstation business now has 3 ways to grow. One is rendering. The second is data science -- data scientists are now a workstation customer. That has never happened before. And our software stack with Turing, turns a workstation into an ideal data science workstation. And the third is, finally, we're able to make workstations into notebooks. And they're delightfully thin, using all the same technologies that talked about for gaming notebooks. And so workstations is a growth business. And then lastly, our automotive is going to be a growth business. We've been investing, as you know, in self-driving cars. And this year, we announced entry into Level 2+, our first foot into the mainstream marketplace of autonomous vehicles. And the first design win is Volvo, and we have others to announce. And so I think this is going to be a good year for self-driving cars as well. So we have 4 growth businesses. Our 4 core businesses are all growing. Thanks, Stacy, we'll start with that first question on gross margin. You're correct, mix is still the primary driver of our gross margin every single quarter. You have correctly reduced -- or excuse me, changed our Q4 numbers to remove the overall inventory write-down. So when you look at Q1, there is a mix around our products that we plan on shipping by platforms but also within our gaming business and within our data center business. We also have different gross margins that would influence. This is our best estimate that we have at this time, and we'll see as we move through the year. I think it's more on the inter business. Now keep in mind, our Q4 has a very low percentage of gaming as a total in terms of there and then a different mix within there as we move to the next quarter as well. The mix of the intra, both within the segments as well as between the segments. Well, in the short term, in near term, we have relatively limited visibility. We don't think it's going to remain this way. And with a little bit of tailwind, I think we could have a fairly good year. And so we'll just see how it turns out. This is our year guidance for now, and we'll update you as we go. The fundamental dynamics doesn't change. The fact of the matter is the world needs more computing. And a lot of that computing is related to machine learning, data analytics, deep learning. It's related to the things that we're working on. And we have 4 new ways to grow our datacenter business. I think our deep learning position is as good as ever. Our scientific computing position is as good as ever. And we have 4 new ways to grow. We have inference, we have data analytics and machine learning. We have rendering. And now we're taking that entire stack to the enterprise. And so I think we have the right strategy. We have the best platform. And the utilization -- the utility of it is really fantastic. And so with a little bit of a tailwind, I think we could have a fairly good year, and we'll just report it as we go. I wanted to ask about again, competition in datacenter. AMD on their call had talked about graphics in their datacenter business being as big as server, which is sort of north of $100 million a quarter, which surprised me. My sense is they're doing quite a bit different applications than you guys are. But maybe if you can just give us some context around what they're doing and how you see the competition coming up from within other graphics vendors. Our data center business is really focused on computing, and we just don't -- we don't see anybody. Our primary competitor is CPUs. That's really the starting of it and the ending of it. And it's very clear. The vast majority of the world's data center only runs on CPUs today. But the advance of technology has slowed. And it's creeping along at a few percent a year, and unfortunately, that's just not good enough. And so either datacenters are going to continue to increase in CapEx or they're going to have to find a new approach. And I think people are fairly enthusiastic about University, about accelerated computing. And I think our position is really quite good. And so I would say that those are largely the positions. If you think about competitively comparing our GPU to a competitor's latest GPU, I think the expectation was really high, and didn't turn out quite that way. I think our -- we've established that the Turing energy efficiency is much better. I think we've established that NVIDIA's Tensor Core architecture as a result allows our Volta to be 4x the performance of the highest end of the alternative. And the T4 is 1/4 the power at the same performance. And so the benefit of having great architectural advantage is a really rich software stack, and engineering that resulted in the energy efficiency that we've achieved, generation in, generation out, I think those are really great advantages. And then lastly, because of the broad reach of our architecture and OEM or a cloud service provider can adopt our architecture, and the utility of it is going to be greater because there's just a lot more applications. And the best way to reduce cost for any utility is to increase its utility. And I think that that's -- that our position there is really strong as you could imagine. Yes, thanks a lot, Matt. First of all, the Turing architecture is the highest-performing architecture at every single price point. And it's a big jump from our last generation in every single way. Without ray tracing, the Turing architecture is the first GPU to do concurrent floating point and integer operations at the same time. The instruction per clock of the Turing processor is so much better than our last generation, so much better than what's available in the marketplace. The cache architecture is a big improvement, and you can just see it in all of the existing games. If you just measure the existing games without touching anything, Turing gives you a big boost. And that's before we talked about ray tracing. And we've already spoken about ray tracing earlier, Matt. And we know that every single game that are coming out, we're working with the developers to incorporate RTX technology. And a very, very big deal, both Epic with Unreal engine and Unity engine are going to incorporate Ray tracing. It is very, very clear that the next generation of computer graphics is ray tracing. And all of the work that we've done with RTX to move the industry forward is well worth it. But remember, that's just the graphics part of Turing. Turing comes with it several new opportunities for growth for us. The first is, of course, advancing games. Advancing games for notebooks, advancing computer graphics, photorealistic rendering for film. All the work that we've done with Tensor Core that we just talked about. It's our second-generation Tensor Core, making it great for training as well as inference, a big leap for us for inference. And then lastly, all of the work that we're doing for data analytics and machine learning, we'll take advantage of all the capabilities of Turing. And so Turing is a big deal for us. And that's one of the reasons why last year was so busy for us as we put Turing into workstations, into data centers, into clouds, into rendering, into video games. And so Turing is really a gigantic leap for us architecturally. We're really excited by it. It's -- I think that the turbulent Q4 kind of overshadowed all of it. But in the final analysis, I think Turing was a home run for us. And to also answer your question regarding DRAM prices. Yes, they have definitely been volatile over the historical period. It is great to see them coming down in price. Over the long term, yes, that is beneficial to us from a gross margin perspective. So if we look out to the horizon later, we will probably be able to incorporate that into our gross margins. First of all, on the pricing part, the biggest inhibitor was that we didn't have -- we couldn't launch our midrange segment. The inability to launch 2060 was a big inhibitor for us. But we did so at CES. The launch is a great success. The reviews are fantastic. People loved 2060. The price point is great. And so now we have a great stack from the midrange all the way to enthusiasts. The other part which could have turned out better is at the time of the launch, there were so many special editions and there were so many overclocked versions that the price point appeared high. But now while we have MSRP pricing for all of our segments. And so that's terrific. China is an important market. China is an important market, and it's an important gaming market. And I have every confidence it's going to rebound. Sure, I'll start off. To repeat what we indicated in our transcript, we plan to increase OpEx in the high single digits over where we finished in terms of fiscal year '19. That is related to our opportunities that we see in front of us, gaming, AI as well as self-driving cars. Our focus in terms of investment, we are a very R&D-heavy significant company. But there are investments across the board, both in R&D as well as what we need in terms of go-to-market strategies to obtain these higher markets in front of us. Yes. On your gross margin question, yes, we still have drivers within the mix of our products that allow us to grow our gross margin over the long term. Absolutely. And there is definitely a goal for us to continue doing that. So the focus in terms of what the cost components of what we do but also moving the entire portfolio into the higher value-added platforms that we sell. So over the long term, absolutely, all of those things are still in place and intact that we can do. And we'll look quarter to quarter to give the best guidance that we can to help you see that. Obviously, a pause in datacenter, particularly the hyperscale, is pretty well documented. I'm just kind of curious to your perspective, particularly being AI still a growth area versus -- or run rate being memory and CPUs. Can you give any perspective as to how widespread it is? Maybe number of customers or geographic perspective. And just kind of curious how AI is affected with this greater slowdown. Yes. Blayne, the hyperscalers, their pause is probably the most dramatic. We still see a lot of activity in enterprise. It's just a much smaller base for us, but we expect it to be a much larger base in the future. And the reason for that is because most of the enterprises today don't use deep learning, they use an approach called machine learning. They might use things like decision trees or graph analytics or regressions or clustering or things like that, algorithms like that. And they'll run data analytics applications for business intelligence on a large amount of data. And they might be running it on a -- on top of a Spark stack that was created out of Berkeley and open source from Databricks. And so there's a -- if you recognize some of these things, that's what health care companies do and financial services companies do and retail companies do. They use it for fraud detection, predicting inventory, trying to make the best matches between riders and drivers and trying to predict which route to take to deliver food, dinner to you. And so those kinds of applications, most of the developers today use machine learning in large -- and big data analytics. And so we invested in a stack called RAPIDS and built our architecture called T4. And we're in the process of partnering with large IT companies to take the stack and the solution out to the world's enterprise. And so I expect enterprise to be a fairly exciting growth opportunity for us. Meanwhile, the CSPs, their pause will end. The amount of computation they do is increasing. More and more of them are using deep learning. And we have inference opportunity with the work that we did with T4 and TensorRT. And so we've got a lot of exciting opportunities to go. Sure. Let me start on that piece. As we talked about earlier on the call, our hyperscale and our hyperscales, many of them are also cloud providers, did start to slow down in terms of their purchasing in the latter half of the year. But the overall growth rate, as you can see from our data center business, grew more than 50% for the full year. Now even with that strong growth, we are still a very small percentage of overall CapEx that we see in cloud providers or the overall hyperscales. We are likely one of their top priorities of areas where they need to grow in terms of in their data center as they focus on AI, as they focus on the cloud businesses and the importance of that compute is necessary. But we are still a very, very small percentage of it. So slow down in the second half of the year, full year growth, phenomenal growth of 50%. And we track with the (inaudible). Yes, depending on the quarter, we will have a mix of what is hyperscale growth or what is cloud. And again, in the core -- in the fourth quarter, that was not a growth opportunity for us. But earlier in the year, definitely it was. Well, we're enthusiastic about the second half. We're enthusiastic about our position, and we're enthusiastic about the solution we offer. And Pierre, as you know, we've also expanded our application and our market reach. We guided -- I think probably the biggest takeaway is we guided flight to slightly down for the whole year for the whole number. We do have 4 growth drivers. And maybe the best way to think about it is we should just wait and see how it goes. I think we've -- considering where we are, I feel pretty good about our guidance, and -- but I feel even better about our strategic position. And so I look forward to working through the year with you guys. I don't know that we could tear that apart, tease that apart, Harlan. We just know that China, the consumer market is relatively slow towards the end of the year. But the China economy is in the final analysis of growth economy, and so we're looking forward to it recovering. And gaming is one of the most important pastimes of that culture, and so I'm excited about our prospects there. All of the things that we're seeing in the near term, Colette, has done a really good job describing. And as we leave the bottom and leave this inventory issue behind us, we're super well positioned. We have a full stack of RTX, the Turing architecture is fantastic. It is unquestionably the best in the world. We have the best performance at every single price point. And we have great notebooks that the market can now buy. And so I'm looking forward to reporting our status with you guys as the year goes on. This should be a good year. 2018 was a record year, but it was a disappointing finish. This quarter, we expect to put the channel inventory issue behind us and get back on track. As the pioneer of accelerated computing, our position is unique and strong. And the opportunities ahead in graphics, high-performance computing, AI and autonomous machines remain enormous. We are as enthusiastic about these growth opportunities as ever. Thanks, everyone, for joining us today.","I had 2 questions. First, Colette, you talked about the weakness you saw in the 2070 and the 2080 in the quarter. I guess this question is more for Jensen. Are you concerned at all about your ability to convince and incentivize gamers to -- as Colette pointed out, it's more of a timing thing? And second question is inventory was up on the balance sheet. Colette, if you can just provide some color there and expectations going forward. I guess on the commentary regarding a pause in spending in datacenter and a handful of deals that got delayed, can you give a little bit more color in terms of, I guess what you're seeing across enterprise, cloud, high-performance compute? And I guess within that, how you're seeing the ramp of T4? And I guess if you can kind of then speak to I'm sure embedded in the fiscal '20 guidance is a pretty nice ramp into the second half. What are the key drivers, key milestones that you're looking for to see that business reaccelerate higher as we go through the year? Colette, I appreciate all the data you gave us on trying to size normalized demand for gaming. What I have to ask, though, is if you're still going through of the channel inventory worked out in fiscal first quarter, it seems like to hit your full year guide, the expectation is for gaming revenue to accelerate well above that normalized level you talked about. One, am I doing the math right? And if I am, kind of what gives you confidence throughout the year that you can see that kind of gaming growth off of these bottoms? That's helpful, Jensen. And then maybe just a follow-up, just on the data center side. Clearly, you've talked about new applications that should help grow your TAM inside of datacenter. I'm just kind of curious, the calendar fourth quarter of last year, I think marked the first time that a competitor had some meaningful volumes of GPU in the datacenter. There's always talk about the hyperscale guys are wanting to do their own ASICs. What kind of anecdotal evidence can you give us to help us get more comfortable of what's going on here is more macro and not share loss either to competition and/or architectural differences between GPUs and ASICs? First I had a clarification, Colette. I just wanted to clarify what the mix is assumed for the fiscal Q1 guidance? Are you kind of assuming that datacenter and gaming are both kind of flat sequentially? And then I guess my question was can you help us sort of shape the revenue through the year? To get to your full year guidance, you have to add roughly maybe $1.3 billion, $1.5 billion from where you are in fiscal Q1. How does that shape through the year? First, I wanted to get again at the mix. Colette, you -- this is a question for Colette. You have said that mix was going to be the primary driver of your gross margins. And I know that sequentially they're up, but if I correct for the inventory write-down in Q4, the normalized gross margin this quarter was 61.7%. And so you're guiding it to 59%, so it's down 270 basis points sequentially on flat revenues. So do I take from that guidance that that's an indicator of the mix between the businesses, it's the primary driver of that gross margin degradation? Is there something else going on that we should be aware of? So what do you think the bigger -- between those 2, whether it's intra-business mix or inter-business mix, between the businesses or within the businesses, which one of those is the biggest driver of the gross margin degradation sequentially into Q1? So you think it's the mix between the businesses within, like you said inter, is that mix between the businesses that you think is the bigger driver then? Okay. For my follow-up, Jensen, you and Colette, I guess you mentioned the datacenter was growing. But if I got full year revenue sort of flat to down slightly, and I've got gaming revenues down a bit like you said and I have pro viz and auto growing, it's kind of really hard for me within the envelope of that guidance to get datacenter growing much if at all. I mean, it could even be down within that. I mean, how are you thinking about the idea of datacenter growing within the context of the full year guidance that you've given? I have just a couple of questions. The first one, Jensen, it seemed like the Turing platform is delivering some amazing results. But as you talked about relying on some new software features to do it. I wonder if you might talk about any steps you're taking in the road map to really upgrade performance for the installed base of games, given the time that it might take for some of those software features to roll out. And then secondly, you noted in the pre-announcement something about the write-down having to do with DRAM. I mean, obviously, that commodity pricing has been volatile. Colette, is there anything you can talk about, about how big of an effect that might have on the business and on pricing overall? Kind of building on the discussion around the Turing platform and particularly to the gaming market, I'm curious, you mentioned in your prepared comments pricing of these new solutions was a bit of an inhibitor. Has the company invoked any changes in their pricing strategy around Turing? And then also, I'd be interested at can you help us frame how important China is to the gaming segment? And whether or not you're assuming that the China market rebounds in your annual assumption? Could you just -- a clarification, could you repeat what you said you thought the OpEx would grow in this fiscal year? And what is the -- where's the focus of that investment? To what extent is it R&D versus SG&A? And then maybe one layer deeper, where is the focus of the higher OpEx? So I don't want to poke too many holes on the memory side and the downturn in gaming due to the crypto inventory. But if I think about kind of the gross margin profile, you guys almost reached 65%. So if I look out, let's call it a year or even call it 18 months and make it a -- more of a kind of a long-term target, is there any reason why you guys can't get back up to kind of the mid-60s level? A question for Colette. I'm trying to figure out how your position with cloud data centers evolved last year in financial year 2019. So if I look at the cloud CapEx, they were up about 70%. If I look at the CPU going into the cloud, according to Intel, it was up about 60%. So I was wondering how much you guys have been increasing revenues within your cloud business? Did you grow faster than CPUs? Did you grow faster than the overall CapEx? And then I have a follow-up in 2020. But this 50% growth, is that for your overall datacenter business? Is that what like specifically your cloud hyperscale business grew as well? Or did that grow even faster with that? Great. And then quickly maybe, Jensen, on 2020, so you seem to be very cautious on the datacenter outlook. When we listen to players like Google and Facebook and others, they all seem to be seem like very keen to grow their overall spending this year. And when we listen to other providers around you, they all seem to be -- I mean they all demonstrate a bit of a confidence that in the second half, spending should resume. Is that just because they are being more optimistic than you? Do they see something you don't see? What do you hear from your clients about the second half? On the China gaming weakness, is it the slower economic environment? Or is it sort of government policy related? Because we know that the China government has had a freeze on new gaming approvals, although they've recently started to approve new games. This ban is in place, I think since the first half of last year. So given what you know of the business, how much of the China weakness is coming from the China gaming bans? Or is this just overall slower economic environment? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q1 2020,2085,5104,1599,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the First Quarter of Fiscal 2020. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2020. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 16, 2019, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. Q1 revenue was $2.2 billion, in line with our outlook and down 31% year-on-year and up 1% sequentially. Starting with our gaming business. Revenue of $1.05 billion was down 39% year-on-year and up 11% sequentially, consistent with our expectations. We are pleased with the initial ramp of Turing and the reduction of inventory in the channel. During the quarter, we filled out our Turing lineup with the launch of midrange GeForce products that enable us to delight gamers with the best performance at every price point starting at $149. New product launches this quarter included the GeForce GTX 1660 Ti, 1660 and 1650, which bring Turing to the high-volume PC gaming segment for both desktop and laptop. These GPUs deliver up to 50% performance improvement over their Pascal-based predecessors, leveraging new share innovations such as concurrent floating point and integer operations, a unified cache and Adaptive Shading, all with the incredibly power-efficient architecture. We expect continued growth in the gaming laptops this year. GeForce gaming laptops are one of the shining spots of the consumer PC market. This year, OEMs have built a record of nearly 100 GeForce gaming laptops. GeForce laptops start at $799 and all the way up to an amazing GeForce RTX 2080 4K laptops that are more powerful than even next-generation consoles. The content ecosystem for ray-traced games is gaining significant momentum. At the March Game Developers Conference, ray-tracing sessions were packed. Support for ray tracing was announced by the industry's most important game engines, Microsoft DXR, Epic's Unreal Engine and Unity. Ray tracing will be the standard for next-generation games. In March, at our GPU Technology Conference, we also announced more details on our cloud gaming strategies through our GeForce NOW service and the newly announced GFN Alliance. GeForce NOW is a GeForce gaming PC in the cloud for the 1 billion PCs that are not game-ready, expanding our reach well beyond today's 200 million GeForce gamers. It's an open platform that allows gamers to play the games they own instantly in the cloud on any PC or Mac anywhere they like. The service currently has 300,000 monthly active users with 1 million more on the waitlist. To scale out to millions of gamers worldwide, we announced the GeForce NOW Alliance, expanding GFN through partnerships with the global telecom providers. Softbank in Japan and LG Uplus in South Korea will be among the first to launch GFN later this year. NVIDIA will develop the software and manage the service and share the subscription revenue with alliance partners. GFN runs on NVIDIA's edge computing servers. As telcos race to offer the new services for their 5G network, GFN is an ideal new 5G application. Moving to data center. Revenue was $634 million, down 10% year-on-year and down 7% sequentially, reflecting the pause in hyperscale spending. While demand from some hyperscale customers bounced back nicely, others paused or cut back. Despite the uneven demand drop -- backdrop, the quarter had significant positives consistent with the growth drivers we outlined on our previous earnings call. First, inference revenue was up sharply both year-on-year and sequentially with broad-based adoption across a number of hyperscale and consumer Internet companies. As announced at GTC, Amazon and Alibaba joined other hyperscalers such as Google, Baidu and Tencent in adopting the T4 in their data centers. A growing list of consumer Internet companies is also adapting our GPUs for inference, including LinkedIn, Expedia, Microsoft, PayPal, Pinterest, Snap and Twitter. The contribution of inference to our data center revenue is now well into the double-digit percent. Second, we expanded our reach in enterprise, teaming up with major OEMs to introduce the T4 enterprise and edge computing servers. These are optimized to run the NVIDIA CUDA-X AI acceleration libraries for AI and data analytics. With an easy-to-deploy software stack from NVIDIA and our ecosystem partners, this wave of NVIDIA edge AI computing systems enables companies in the world's largest industries, transportation, manufacturing, industrial, retail, health care and agricultural, to bring intelligence to the edge where the customers operate. And third, we made significant progress in data center rendering and graphics. We unveiled a new RTX Server configuration packing 40 GPUs into an 8 use case and up to 32 servers in a pod, providing unparalleled density, efficiency and scalability. With a complete stack, this server design is optimized for 3 data center graphic workloads: rendering, remote workstations and cloud gaming. The rendering opportunity is starting to take shape with early RTX Server deployments at leading studios, including Disney, Pixar and Weta. In the quarter, we announced our pending acquisition of Mellanox for $125 per share in cash, representing a total enterprise value of approximately $6.9 billion, which we believe will strengthen our strategic position in data center. Once complete, the acquisition will unite 2 of the world's leading companies in high-performance computing. Together, NVIDIA's computing platform and Mellanox's interconnects power over 250 of the world's TOP500 supercomputer and have as customers every major cloud service provider and computer maker. Data centers in the future will be architected as giant compute engines with tens of thousands of compute nodes designed holistically with their interconnects for optimal performance. With Mellanox, NVIDIA will optimize data center scale workloads across the entire computing, networking and storage stack to achieve higher performance, greater utilization and lower operating cost for customers. Together, we can create better AI computing systems for the cloud to enterprise to the edge. As stated at the time of the announcement, we look forward to closing the acquisition by the end of this calendar year. Moving to pro visualization. Revenue reached $266 million, up 6% from the prior year and down 9% sequentially. Year-on-year growth was driven by both desktop and mobile workstations, while the sequential decline was largely seasonal. Areas of strength included the public sector, oil and gas and manufacturing. Emerging applications, such as AI, AR, VR, contributed an estimated 38% of pro visualization revenue. The real-time ray-tracing capabilities of RTX are a game changer for the visual effects industry, and we are seeing tremendous momentum in the ecosystem. At GTC, we announced that the world's top 3D application providers have adopted NVIDIA RTX in their product releases set for later this year, including Adobe, Autodesk, Chaos Group, Dassault and Pixar. With this rich software ecosystem, NVIDIA RTX is transforming the 3D market. For example, Pixar is using NVIDIA RTX ray tracing on its upcoming films. Weta Digital is using it for upcoming Disney projects and Siemens NX ray-traced studios users will be able to generate rendered images up to 4x faster in their product design workloads. We are excited to see the tremendous value that NVIDIA RTX is bringing to the millions of creators and designers served by ecosystem partners. Finally, turning to automotive. Q1 revenue was $166 million, up 14% from a year ago and up 2% sequentially. Year-on-year growth was driven by growing adoption of next-generation AI cockpit solutions and autonomous vehicle development deals. At GTC, we had major customer and product announcements. Toyota selected NVIDIA's end-to-end platform to develop, train and validate self-driving vehicles. This broad partnership includes advancements in AI computing, infrastructure using NVIDIA GPUs, simulation using NVIDIA DRIVE Constellation platform and in-car AV computers based on the DRIVE AGX Xavier or Pegasus. We also announced the public availability of DRIVE Constellation, which enables millions of miles to be driven in virtual worlds across the broad range of scenarios with greater efficiency, cost effectiveness and safety than what's possible to achieve in the real world. Constellation will be reported in our data center market platform. And we introduced NVIDIA Safety Force Field, a computational defensive driving framework that shields autonomous vehicles from collisions. Mathematically verified and validated in simulation, Safety Force Field will prevent a vehicle from creating, escalating or contributing to an unsafe driving situation. We continue to believe that every vehicle will have an autonomous capability one day, whether with a driver or driverless. To help make that vision a reality, NVIDIA has created an end-to-end platform for autonomous vehicles from AI computing infrastructure to simulation to in-car computing. And Toyota is our first major win that validates this strategy. We see this as a $30 billion addressable market by 2025. Moving to the rest of the P&L and balance sheet. Q1 GAAP gross margin was 58.4% and non-GAAP was 59%, down year-on-year to lower gaming margins and mix, up sequentially from Q4, which had $128 million charge from DRAM boards and other components. GAAP operating expenses were $938 million, and non-GAAP operating expenses were $753 million, up 21% and 16% year-on-year, respectively. We remain on track for high single-digit OpEx growth in fiscal 2020 while continuing to invest in the key platforms driving our long-term growth, namely graphics, AI and self-driving cars. GAAP EPS was $0.64, and non-GAAP EPS was $0.88. We did not make any stock repurchases in the quarter. Following the announcement of the pending Mellanox acquisition, we remain committed to returning $3 billion to shareholders through the end of fiscal 2020 in the form of dividends and repurchases. So far, we have returned $800 million through share repurchases and quarterly cash dividends. With that, let me turn to the outlook for the second quarter of fiscal 2020. While we anticipate substantial quarter-over-quarter growth, our Q2 outlook is somewhat lower than our expectations earlier in the quarter where our outlook for fiscal 2020 revenue was flat to down slightly from fiscal 2019. The data center spending pause around the world will likely persist in the second quarter, and visibility remains low. In gaming, the CPU shortages while improving will affect the initial ramp of our laptop business. For Q2, we expect revenue to be $2.55 billion, plus or minus 2%. We expect a stronger second half than the first half, and we are returning to our practice of providing revenue outlook one quarter at a time. Q2 GAAP and non-GAAP gross margins are expected to be 59.2% and 59.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $985 million, $765 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of approximately $27 million. GAAP and non-GAAP tax rates are both expected to be 10%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $120 million to $140 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We'll be presenting at the Bank of America Global Technology Conference on June 5, at the RBC Future of Mobility Conference on June 6 and at the NASDAQ investor conference on June 13. Our next earnings call to discuss financial results for the second quarter of fiscal 2020 will take place on August 15. We will now open the call for questions. Operator, will you please poll for questions? Thank you.","Sure. Thanks for the question as we start out here. I think when we had discussed our overall data center business 3 months ago, we did indicate that our visibility as we turned into the new calendar year was low. We had a challenge in terms of completing some of the deals at the end of that quarter. As we moved into Q1, I think we felt solid in terms of how we completed. We saw probably a combination of those moving forward continuing with their CapEx expenditures and building out in terms of what they need for the data centers. Some others are still in terms of the pause. So as we look in terms of with Q2, I think we see a continuation of what we have in terms of the visibility, not the best visibility going forward but still rock-solid to what we think are benefits of what we provide in terms of a platform. Our overall priorities are aligned to what we see with the hyperscalers as well as the enterprises as they think about using AI in so many of their different workloads. But we'll just have to see as we go forward how this turns out. But right now, visibility probably just remains the same about as where we were when we started 3 months ago. So at this time, we don't plan on giving a full year overall guidance. I think our look in terms of gaming, all of the drivers that we thought about earlier in the quarter and we talked about on our Investor Day and we have continued to talk about are still definitely in line. The drivers of our gaming business and Turing RTX for the future are still on track, but we're not providing guidance at this time for the full year. Sure. China looks fine. I think China has stabilized. The gaming market in China is really vibrant, and it continues to be vibrant. Tencent's releasing new games. I think you might have heard that Epic stores now are open in Asia, and games are available from the West. So there are all kinds of positive signs in China. There are some 300 million PC gamers in China, and people are expecting it to grow. We're expecting the total number of gamers to continue to grow from the 1-plus billion PC gamers around the world to something more than that. And so things look fine. Yes. Well, it's seasonal. Second half of the year, we expect to see some great games. We won't preannounce anybody else's games for them, but this is a great PC cycle because it's the end of the console cycle, and PCs is just where the action's at these days with battle royale and eSports and so much social going on, the PC gaming ecosystem is just really vibrant. Our strategy with RTX was to take a lead and move the world to ray tracing. And at this point, it's fairly, fairly safe to say that the leadership position that we've taken has turned into a movement that has turned next-generation gaming ray tracing into a standard. Almost every single game platform will have to have ray tracing, and some of them already announced it. And the partnerships that we've developed are fantastic. Microsoft DXR is supporting ray tracing. Unity is supporting ray tracing. Epic is supporting ray tracing. Leading publishers like EA has adopted RTX and supporting ray tracing. And movie studios, Pixar has adopted -- announced that they're using RTX and will use RTX to accelerate their rendering of films. And so Adobe and Autodesk jumped on top -- jumped onto RTX and will bring ray tracing to their content and their tools. And so I think at this point, it's fair to say that ray tracing is the next generation and is going to be adopted all over the world. Yes. I'll start off here and kind of go back to where our thoughts were in Q1 and why we provided full year guidance when we were in Q1. When we looked at Q1 and what we were guiding, we understood that it was certainly an extraordinary quarter, something that we didn't feel was a true representative of our business, and we wanted to get a better view of our trajectory of our business in terms of going forward. We are still experiencing, I think, the uncertainty as a result of the pause in terms of with the overall hyperscale data centers. And we do believe that's going to extend into Q2. However, we do know and expect that our Q2 -- or excuse me, our H2 will likely be sizably larger than our overall H1, and the core dynamics of our business at every level is exactly what we expected. Just that said though, we're going to return to just quarterly guidance at this time. Yes. So when you think about our growth between Q1 and Q2, yes, we do expect in terms of our gaming to increase. We do expect our Nintendo Switch to start again in sizable amounts once we move into Q2, and we do at this time expect probably our data center business to grow. Hyperscalers are digesting the capacity they have. They -- at this point, I think it's fairly clear that in the second half of last year, they took on a little bit too much capacity. And so everybody has paused to give themselves a chance to digest. However, our business on inference is doing great, and we're working with CSPs all over the world to accelerate their inference models. Now the reason why recently the inference activity has gotten just off the charts is because of breakthroughs in what we call conversational AI. In fact, today, I think that -- I just saw it today, but I've known about this work for some time, Harry Shum's group, Microsoft AI research group, today announced their multitask DNN general language understanding model. And it just -- it broke benchmark records all over the place. And basically, what this means is the 3 fundamental components of conversational AI, which is speech recognition; natural language understanding, which does multitask DNN is a breakthrough, and it's based on a piece of work that Google did recently called BERT; and then text-to-speech. All of the major pieces of a conversational AI are now put together. Of course, it's going to continue to evolve, but these models are gigantic to train. And in the case of Microsoft's network, it was trained Volta GPUs, And these systems require large amounts of memory. The models are enormous. It takes an enormous amount of time to train these systems. And so we're seeing a breakthrough in conversational AI. And across the board, internet companies would like to make their AI much more conversational, so that you can access your phones and smart speakers and be able to engage AI practically everywhere. The work that we're doing in the industries makes a ton of sense. We're seeing AI adoption in just about all the industries from transportation to health care to retail to logistics, industrials, agriculture. And the reason for that is because they have a vast amount of data that they're collecting. And I heard a statistics just the other day from a talk that Satya gave that some 90% of today's data was created just 2 years ago and is being created by and gathered by these industrial systems all over the world. And so if you want to put that data to work -- and you could create the models using our systems, our GPUs for training, and then you can extend that all the way out to the edge. This last quarter, we started to talk about our enterprise server based on T4. This inference engine that has been really successful for us at the CSPs is now going out into the edge, and we call them edge servers and enterprise servers. And these edge systems are going to do AI basically instantaneously. It's too much data to move all the way to the cloud. You might have data sovereignty concerns. You want to have very, very low latency. Maybe it needs to have multi-sensor fusion capabilities so it understands the context better. For example, what it sees and what it hears has to be harmonious. And so you need that kind of AI, those kind of sensor computing at the edge. And so we're seeing a ton of excitement around this area. Some people call it intelligent edge, some people call it edge computing. And now with 5G networks coming, we're seeing a lot of interest around the edge computing servers that we're making. And so those are the activities that we're seeing. Yes, when you look at our sequential gross margin increase, that will be influenced by our larger revenue -- our larger revenue and better mix, which you are correct, is our largest driver of our overall gross margin. However, we will be beginning the Nintendo Switch backup, and that does have lower gross margins than the company average, influencing therefore Q2 gross margin guidance that we had provided. As we look forward towards the rest of the year, we think mix and the higher revenue again will influence and likely rise our overall gross margins for the full year. You talked quite a bit about GeForce NOW in the prepared remarks and at the Analyst Day. It seems like cloud gaming is going to be a big topic at E3. Is that going to be your preferred way to go to market with cloud gaming? And do you expect to sell GPUs to sort of traditional cloud vendors in non-GeForce NOW fashion? Yes. Our strategy for cloud gaming is to extend our PC position for GeForce gamers into the cloud, and our strategy for building out our network is partnerships with telcos around the world. And so we'll build out some of it. And on top of the service, we have our entire PC gaming stack. And when we host the service, we'll move to a subscription model. And with our telcos around the world who would like to provision the service at their edge servers, and many of them would like to do so in conjunction with our 5G telco services to offer cloud gaming as a differentiator. And all of these different countries where PC exposure has been relatively low, we have an opportunity to extend our platform out to that billion PC gamers. And so that's our basic strategy. We also offer our edge server platform to all of the cloud service providers. Google has NVIDIA GPU graphics in the cloud, amazon has NVIDIA GPU graphics in the cloud and Microsoft has NVIDIA GPU graphics in the cloud. And these GPUs will be fantastic also for cloud gaming and workstation graphics and also ray tracing. And so the platform is capable of running all of the things that NVIDIA runs, and we try to put it in every data center, in every cloud from every region that's possible. I actually had a clarification for Colette and a question for Jensen. Colette, are you now satisfied that the PC gaming business is operating at a normal level when you look at the Q2 guidance? Like are all the issues regarding inventory and other issues, are they over? Or do you think that the second half of the year is more the normalized run rate for your PC gaming business? And then Jensen, on the data center, NVIDIA has dominated the training market. Inference sounds a lot more fragmented and competitive. There's a lot of talk of software being like more at the framework level. How should we get the confidence that your lead in training will help you maintain a good lead in inference also? Thanks for the question. So let's start with your first part of the question regarding how we reach overall normalized gaming levels. When we look at our overall inventory in the channel, we believe that this is relatively behind us and moving forward that it will not be an issue. Going forward, we will probably reach normalized level for gaming somewhere between Q2 and Q3 similar to our discussion that we had back at Analyst Day, at the beginning of the quarter. NVIDIA's strategy is accelerated computing. It is very different than an accelerator strategy. For example, if you were building a smart microphone, you need an accelerator for speech recognition, ASR. Our company is focused on accelerated computing, and the reason for that is because the world's body of software is really gigantic, and the world's body of software continues to evolve. And AI is nowhere near done. We're probably at the first or a couple of innings of AI at that. And so the amount of the software and the size of the models are going to have to continue to evolve. Our accelerated computing platform is designed to enable the computer industry to bring forward into the future all the software that exists today, whether it's TensorFlow or Caffe or PyTorch or it could be classical machine learning algorithms like XGBoost, which is actually right now the most popular framework and machine learning overall. And there's so many different types of classical algorithms and not to mention all of the handwritten engineered algorithms by programmers. And those algorithms and those hand-engineered algorithms also would like to be mixed in with all of the deep learning or otherwise classical machine learning algorithms. This whole body of software doesn't run on a single function accelerator. If you would like that body of software to run on something, it would have to be sufficiently general purpose. And so the balance that we made was we invented this thing called a Tensor Core that allows us to accelerate deep learning to the speed of light. Meanwhile, it has the flexibility of CUDA so that we can bring forward everything in classical machine learning as people have started to see with RAPIDS and is being announced, being integrated into machine learning pipelines, in the cloud and elsewhere, and then also all of the high-performance computing applications or computer vision algorithms, image processing algorithms that don't have deep learning or machine learning alternatives. And so our company is focused on accelerated computing. And speaking of inference, that's one of the reasons why we're so successful in inference right now. We're seeing really great pickup. And the reason for that is because the type of models that people want to run on one application, and let's just use one application, one very, very exciting one, conversational AI, you would have to do speech recognition. You would have to then do natural language understanding to understand what the speech is. You might have to convert -- you have to translate to another language. Then you have to do something related to maybe making a recommendation or making a search. And then after that, you have to convert that recommendation and search and the intent into speech. While some of it could be 8-bit integers, some of it really wants to be 16-bit floating point. And some of it because of development state of it may want to be in 32-bit floating point. And so the mixed precision nature and the computational algorithm nature or flexibility nature of our approach make it possible for cloud providers and people who are developing AI applications to not have to worry about exactly what model it runs or not. We run every single model. And if it doesn't currently run, we'll help you make it run. And so the flexibility of our architecture and the incredible performance in deep learning is really a great balance and allows customers to deploy it easily. So our strategy is very different than an accelerator. I think the only accelerators that I really see successful at the moment are the ones that go into smart speakers. And surely, there are a whole bunch being talked about, but I think the real challenge is how to make it run real, real workloads. And we're going to keep cranking along in our current strategy and keep raising the bar as we have in the past. Sure. As we discussed, Stacy, we are seeing many of the hyperscalers definitely purchasing in terms of the inferencing into the installment that it continues, also in terms of the training instances that they will need for their cloud or for internal use absolutely. But we have some that have paused and are going through those periods. So that -- we do believe this will come back. We do believe as we look out into the future that they will need that overall deep learning for much of their research as well as many of their workloads. So no concern on that, but right now, we do see a pause. I'll turn it over to Jensen to see if he has any additional comments. Let's see. I think that when it comes down to training, if your infrastructure team tells you not to buy anything, the thing that suffers is time to market and some amount of experimentation that allows you to get better comps and waiting longer. And I think that for computer vision type of algorithms and recommendation type of algorithms, those -- that posture may not be impossible. However, the type of work that everybody is now jumping on top of, which is natural language understanding and conversational AI and the breakthrough that Microsoft just announced, if you want to keep up with that, you're going to have to buy much, much larger machines. And I'm looking forward to that, and I expect that that's going to happen. But in the latter part of last year, Q4 and Q1 of this year, we did see pause from the hyperscalers and -- but I don't expect it to last. Well, on first principles, the acquisition is going to enable data centers around the world, whether it's U.S. or elsewhere in China, to be able to advance much, much more quickly. We're going to invest and building infrastructure technology. And as a combined company, we'll be able to do that much better. And so this is good for customers, and it's great for customers in China. The 2 matters, whether it's -- the 2 matters that we're talking about are just -- are different. And one is related to competition in a -- with respect to our acquisition, the competition in the market, and the other one is related to trade. And so the 2 matters are just different. And in our particular case, we bring so much value to the marketplace in China, and I'm confident that the market will see that. We expected to grow in 2019. The -- a lot of our T4 inference work is related to what people call edge computing. And it has to be done at the edge because of the amount of data that otherwise would be transferred to the cloud is just too much. It has to be done at the edge because of data sovereignty issues and data privacy issues. And it has to be done at the edge because the latency requirement is really, really high. It has to respond basically like a reflex and to make a prediction or make a suggestion or stop a piece of machinery instantaneously. And so a lot of that work, a lot of the work that we're doing in T4 inference is partly in the cloud, a lot of it is at the edge. T4 servers for enterprise were announced, I guess, about halfway through the quarter. And the OEMs are super excited about that because the number of companies in the world who want to do data analytics, predictive analytics is quite large, and the size of the data is growing so significantly. And with Moore's Law ending, it's really hard to power through terabytes of data at a time. And so we've been working on building the software stack from the new memory architectures and storage architectures all the way to the computational middleware, and it's called RAPIDS. And I appreciate you saying that, and that's being put together. And the activity in GitHub is just fantastic. And so you can see all kinds of companies jumping in to make contributions because they would like to be able to take that open-source software and run it in their own data center on our GPUs. And so I expect the enterprise side of our business, both for enterprise big data analytics or for edge computing, to be a really good growth driver for us this year. Our automotive strategy has several components. There's the engineering component of it where we -- our engineers and their engineers have to codevelop the autonomous vehicles. And then there's 3 other components. There's the component of AI computing infrastructure we call DGX and/or any of the OEM service that include our GPUs that are used for developing the AIs. The cars are collecting a couple of terabytes per day per test car, and all of that data has to be powered through and crunched through in the data center. And so we have an infrastructure of what we call DGX that people could use, and we're seeing a lot of success there. We just announced this last quarter, a new infrastructure called Constellation that lets you essentially drive thousands and thousands of test cars in your data center, and they're all going through pseudo-directed random or directed scenarios that allows you to either test untestable scenarios or regress against previous scenarios, and we call that Constellation. And then lastly, after working on a car for several years, we would install the computer inside the car, and we call that DRIVE. And so these are the 4 components of opportunities that we have in the automotive industry. We're doing great in China. There's a whole bunch of electric vehicles being created. The robotaxis' developments around the world largely use NVIDIA's technology. We recently announced the partnership with Toyota. There's a whole bunch of stuff that we're working on, and I'm anxious to announce them to you. But this is an area that is the tip of the iceberg of a larger space we call robotics and computing at the edge. But if you think about the basic computational pipeline of a self-driving car, it's no different essentially than a smart retail or the future of computational medical instruments, agriculture, industrial inspection, delivery drones, all basically use essentially the same technique. And so this is the foundational work that we're going to do for a larger space that people call the intelligent edge or computing at the edge. In Q2, we were -- we had to deal with some CPU shortage issues at the OEMs. It's improving, but the initial ramp will be affected. And so the CPU shortage situation has been described fairly broadly, and it affected our initial ramp. We don't expect it to affect our ramp going forward. And the new category of gaming notebooks that we created called Max-Q has made it possible for really amazing gaming performance to fit into a thin and light. And these new generations of notebooks with our Max-Q design and the Turing GPU, which is super energy efficient, in combination, made it possible for OEMs to create notebooks that are both affordable all the way down to $799, thin and really delightful, all the way up to something incredible with RTX 2080 and a 4K display. And these are thin notebooks that are really beautiful that people would love to use. And this -- the invention of the Max-Q design method and all the software that went into it that we announced last year, we had -- I think last year, we had some 40 notebooks or so, maybe a little bit less than that. And this year, we have some 100 notebooks that are being designed at different price segments by different OEMs across different regions. And so I think this year is going to be quite a successful year for notebooks. And it's also the most successful segment of consumer PCs. It's the fastest-growing segment. It is very largely underpenetrated because until Max-Q came along, it wasn't really possible to design a notebook that is both great in performance and experience and also something that a gamer would like to own. And so finally, we've been able to solve that difficult puzzle and created powerful gaming machines that are inside a notebook that's really wonderful to own and carry around. And so this is going to be a really -- this is a fast-growing segment, and all the OEMs know it. And that's why they put so much energy into creating all these different types of designs and styles and sizes and shapes. And we have 100 Turing GPU notebooks, gaming PCs ramping right now. Yes. Level 2+, call it 2020, late 2021 or like 2022-ish. So that's Level 2+. I would say 2019, very, very early for robotaxis; next year, substantially more volume for robotaxis; 2021, bigger volumes for robotaxis. The ASP differences, the amount of computation you put into a robotaxi because of sensor resolutions, sensor diversity and redundancy, the computational redundancy and the richness of the algorithm, all of it put together, it's probably an order of magnitude plus in computation. And so the economics would reflect that. And so that robotaxi is kind of like next year, the year after, ramp. And then think of Level 2+ as 2021, 2022. Overall, remember that our economics come from 4 different parts. And so there's the NRE components of it, there's the AI development infrastructure, computing infrastructure part of it, the simulation part of it called Constellation and then the economics of the car. And so we just announced Constellation. The enthusiasm around it is really great. Nobody should ever ship anything they don't simulate. And my expectation is that billions of miles will get simulated inside a simulator long before they'll ship it. And so that's a great opportunity for Constellation. The entire reason for Q4 and Q1 is attributed to oversupply in the channel as a result of cryptocurrency. It has nothing to do with Turing. In fact, Turing is off to a faster start than Pascal was, and it continues to be on a faster pace than Pascal was. And so the pause in gaming is now behind us. We're in a growth trajectory with gaming. RTX has -- took the lead on ray tracing and has now become the standard for next-generation gaming, support from basically every major platform and software provider on the planet. And our notebook growth is going to be really great because of the Max-Q design that we invented. And the last couple of quarters would also intersect it with -- overlap with the seasonal slowdown that -- not sell but build -- the seasonal builds of the Nintendo Switch, and we're going to go back to the normal build cycle. And as Colette said earlier, somewhere between Q2 and Q3, we'll get back to normal levels for gaming. And so we're off to a great start in Turing, and I'm super excited about that. And in the second half of the year, we would have fully ramped up from top to bottom our Turing architecture, spanning everything from 179 to as high performance as you like. And we have the best price, best performance and best GPU at every single price point. And so I think we're in a pretty good shape. In terms of process nodes, we tend to design our own process with TSMC. If you look at our process and you measure its energy efficiency, it's off the charts. And in fact, if you take our Turing and you compare it against a 7-nanometer GPU on energy efficiency, it's incomparable. In fact, the world's 7-nanometer GPU already exists. And it's easy to go and pull that and compare the performance and energy efficiency against one of our Turing GPUs. And so the real focus for our engineering team is to engineer a process that makes sense for us and to create an architecture that is energy efficient. And the combination of those 2 things allows us to sustain our leadership position. Otherwise, buying off the shelf process is something that we can surely do, but we want to do much more than that. Okay. And to discuss your question regarding OpEx trajectory for the rest of the year, we're still on track to our thoughts on leaving the fiscal year with a year-over-year growth and overall OpEx on a non-GAAP basis in the high single digits. We'll see probably an increase sequentially quarter-to-quarter along there, but our year-over-year growth will start to decline as we will not be growing at the speed that we did in this last year. But I do believe we're on track to meet that goal. Thanks, everyone. We're glad to be returning to growth. We are focused on driving 3 growth strategies. First, RTX ray tracing. It's now clear that ray tracing is the future of gaming and digital design, and video RTX is leading the way. With the support of Microsoft DXR, Epic, Unity, Adobe and Autodesk, game publishers like EA, movie studios like Pixar, industry support has been fantastic. Second, accelerated computing and AI computing. The pause in hyperscale spending will pass. Accelerated computing in AI are the greatest forces in computing today, and NVIDIA is leading these movements. Whether cloud or enterprise or AI at the edge for 5G or industries, NVIDIA's one scalable architecture from cloud to edge is the focal point platform for the industry to build AI upon. Third, robotics. Some call it an embedded AI, some edge AI or autonomous machines. The same computing architecture is used for self-driving cars, pick-and-place robotics arms, delivery drones and smart retail stores. Everything machine -- every machine that moves or machines that watch other things that move, whether with driver or driverless, will have robotics and AI capabilities. Our strategy is to create an end-to-end platform that spans NVIDIA's DGX AI computing infrastructure to NVIDIA Constellation simulation to NVIDIA AGX embedded AI computing. And finally, we are super excited about the pending acquisition of Mellanox. Together, we can advance cloud and edge architectures for HPC and AI computing. See you next quarter.","Colette, I was wondering if you could give a little bit more color or discussion around what exactly you've seen in the data center segment and whether or not or what you're looking for in terms of signs that we can kind of return to growth or maybe this pause is behind us. I guess what I'm really asking is kind of what's changed over the last, let's call it, 3 months relative to your prior commentary from a visibility perspective and just demand perspective within that segment. Okay. And then as a quick follow-up on the gaming side, last quarter, you talked about that being down -- I think it was termed as being down slightly for the full year. Is that still the expectation? Or how has that changed? On the last earnings call, you'd mentioned China gaming demand as a headwind. At the Analyst Day in mid-March, I think Jensen had mentioned that the team was already starting to see better demand trends out of China maybe given the relaxed stance on gaming bans. Do you anticipate continued China gaming demand on a go-forward basis? And maybe talk about some of the dynamics driving that demand profile in the China geography. And then as a follow-up, a big part of the demand profile in the second half of the year for the gaming business is always the lineup of AAA-rated games. Obviously, you guys have a very close partnership with all of the game developers. How does the pipeline of new games look? Kind of they get launched October, November time frame, either total number of blockbuster games and also games supporting real-time ray tracing as well as some of your DLSS capabilities. I guess the first question is for Colette. So what went into the decision to pull full year guidance versus just cutting it? Is it really to see around how long it could take for data center to come back? Okay. And then just as a follow-up, can you give us some even qualitative, if not quantitative, sense of the $320 million incremental revenue for July, how that breaks out? Is the thinking sort of that data center is going to be flat to maybe up a little bit and pretty much the remainder of the growth comes from gaming? Jensen, I had a follow-up on the data center business. I was hoping you could provide some color in terms of what you're seeing not only from your hyperscale customers, which you've talked extensively on, but more on the enterprise and HP side of your business and specifically on the hyperscale side. You guys talked about this pause that you're seeing from your customer base. When you're having conversations with your customers, do they give you a reason as to why they're pausing? Is it too much inventory of GPUs and CPUs and so on and so forth? Or is it optimization giving them extra capacity? Is it caution on their own business going forward? Or is it a combination of all the above? Any color on that would be helpful, too. As a quick follow-up on the gaming site, Colette, can you characterize product mix within gaming? You saw in the current quarter -- you cited mix is one of the key reasons why gross margins were down year-over-year, albeit off a high base. Going into Q2 in the back half, would you expect SKU mix within gaming to improve or stay the same? I asked because it's important for gross margins obviously. This is a question for Colette. Colette, so you said inference and rendering within data center were both up very strongly. But I guess that has to imply that like the training/acceleration pieces is quite weak, even weaker than the overall. And given those should be adding to efficiency, I'm just surprised it's down that much. I mean is this truly just digestion? I mean is it share? I mean like your competitor is now shipping some parts here. I mean I guess how do we get confidence that just we haven't seen the ceiling on this? I mean do you think given the trajectory, you can exit the year above the prior peaks? I guess you kind of have to given at least the qualitative outlook in the second -- I guess maybe just any color you can give us on any of those trends would be super helpful. Got it. Just as a quick follow-up, I just wanted to ask about the regulatory around Mellanox in the context of what we're seeing out of China now. How do we sort of gauge the risk of, I guess, potential for the deterioration in relationships sort of spilling over on the regulatory front around that deal? We've seen that obviously with some of the other large deals in the space. What are your thoughts on that? I guess a question on the noncloud part of your data center business. So if you think about the trends you're seeing in enterprise virtualization, in HPC and all the work you're doing around RAPIDS, rendering, et cetera, can you kind of talk through the visibility you have today for that part of your business? I think that's roughly 50% of the mix. So is that a piece that you feel confident you can grow in 2019? And any color around that would be appreciated. And a follow-up real quickly on auto. It's a business that you talked about, more R&D focused, but clearly I think has surprised positively. What's the visibility like there? And how should we think about growth trajectory into the second half of the year? First question is on notebooks and just to clarify what's been different from your expectations this year. Is it simply that the OEMs didn't launch the new models you'd expected given the shortage? Or is it more just about unit volume? And then just following up on that, what's your level of confidence in that coming back to be a driver as you go into the second half of the year? That's very helpful. As a follow-up, I just want to follow up on some of the previous questions on the automotive market. And we've been talking about it for a while. Obviously, the design cycles are very long, so you do have some visibility, and I guess the question is when can we expect an acceleration of auto revenue. Is next year the year? And then what will be the driver of that in terms of dollar contribution? I presume some of the Level 2+ things you've been talking about would probably mean most likely they're given the amount of volume there. If you can confirm that and just give some color on expectations for drivers. I have a few questions, one for Jensen and one for Colette. I guess, Jensen, you've done -- you said in many forums that the move down to the new process node at 7-nanometer across the business was not really sufficient to have a platform approach, and I agree with that. But maybe you could talk a little bit about your product plans at least in general terms around a 7-nanometer franchise in the gaming business and also in your training accelerator program. And I wonder if that might be waiting for some of those products or at least anticipation of those might be the cause of a little bit of a pause here. And secondly, Colette, maybe you could talk us through your expectations. I understand there's a lack of visibility in certain parts of the business on revenue, but maybe you could talk about OpEx trends through the rest of the year where you might have a little more visibility. PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q3 2020,1919,4926,1157,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2020. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2020. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 14, 2019, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. Q3 revenue was $3.01 billion, down 5% year-on-year and up 17% sequentially. Starting with our gaming business. Revenue of $1.66 billion was down 6% year-on-year and up 26% sequentially. Results exceeded our expectations driven by strength in both desktop and notebook gaming. Our GeForce RTX lineup features the most advanced GPU for every price point and uniquely offers hardware-based ray tracing for cinematic graphics. While ray tracing launched a little more than a year ago, 2 dozen top titles have shipped with it or are on the way. Ray tracing is supported by all the major publishers, including all-star titles and franchise such as Minecraft, Call of Duty, Battlefield, Watch Dogs, Tomb Raider, Doom, Wolfenstein and Cyberpunk. Of note, Call of Duty: Modern Warfare, had a record-breaking launch in late October that came on heels of CONTROL, an action-adventure game with multiple ray trace features. Reviews have praised both for their ray tracing implementation and game-play performance. With last week's PC release of Red Dead Redemption 2 as a strong gaming lineup for the holiday season, our business reflects this growing excitement. RTX GPUs now drive more than 2/3 of our desktop gaming GPU revenue. Gaming laptops were a standout, driving strong sequential and year-on-year growth. This holiday season, our partners are addressing the growing demand for high-performance laptops for gamers, students and prosumers by bringing more than 130 NVIDIA-powered gaming and studio laptop models to market. This includes many thin and light form factors enabled by our Max-Q technology, triple the number of Max-Q laptops last year. In late October, we announced the GeForce GTX 1660 Super and the 1650 Super, which refresh our mainstream desktop GPUs with more performance, faster memory and new features. The 1660 Super delivers 50% more performance than our prior-generation Pascal-based 1060, the best-selling gaming GPU of all time. It began shipping on October 29, priced at just $229. PC World called it the best GPU you can buy for 1080p gaming. We also announced the next generation of our streaming media player with 2 new models, Shield TV and Shield TV Pro, which launched on October 28. These bring AI to the streaming market for the first time with the ability to upscale video real time from high definition to 4K using NVIDIA-trained deep neural networks. Shield TV has been widely recognized as the best streamer on the market. Finally, we made progress in building out our cloud gaming business. Two global service providers, Taiwan Mobile and Russia's Rostelecom with GFN.ru joined SoftBank and Korea's LG as partners for our GeForce NOW game-streaming service. Additionally, Telefnica will kick off a cloud gaming proof-of-concept in Spain. Moving to data center. Revenue was $726 million, down 8% year-on-year and up 11% sequentially. Our hyperscale revenue grew both sequentially and year-on-year, and we believe our visibility is improving. Hyperscale activity is being driven by conversational AI, the ability for computers to engage in human-like dialogue, capturing context and providing intelligent responses. Google's breakthrough, introduction of the BERT model, with its superhuman levels of natural language understanding, is driving a way of neural networks for the language understanding. That, in turn, is driving demand for our GPUs on 2 fronts. First, these models are massive and highly complex. They have 10 to 20x, in some cases 100x, more parameters than image-based models. As a result, training these models requires V100-based compute infrastructure that, in orders of magnitude, beyond what is needed in the past. Model complexity is expected to grow significantly from here. Second, real-time conversational AI requires very low latency and multiple neural networks running in quick succession from de-noising to speech recognition, language understanding, text-to-speech and voice encoding. While conventional approaches fail at these tasks, NVIDIA's GPUs can handle the entire inference chain, in less than 30 milliseconds. This is the first AI application where inference requires acceleration. Conversational AI is a major driver for GPU-accelerated inference. In addition to this type of internal hyperscale activity, our T4 GPU continue to gain adoption in public clouds. In September, Amazon AWS announced general availability of the T4 globally, following the T4 rollout on Google Cloud platform earlier in the year. We shipped a higher volume of T4 inference GPU this quarter with V100 training GPUs, and both were records. Inference revenue more than doubled from last year and continued a solid double-digit percentage of total data center revenue. Last week, the results of the first industry benchmark for AI inference, MLPerf inference, were announced. We won. In addition to demonstrating the best performance among commercially available solutions for both data center and edge applications, NVIDIA accelerators were the only ones that completed in all 5 MLPerf benchmarks. This demonstrates the programmability and performance of our computing platform across diverse AI workloads, which is critical for wide-scale data center deployment and is a key differentiator for us. Several product announcements this quarter helped extend our AI computing platform into new markets, the enterprise edge. At Mobile World Congress, Los Angeles, we announced a software-defined 5G wireless RAN solution accelerated by GPUs in collaboration with Ericsson. This opens up the wireless brand market to NVIDIA GPUs. It enables new AI applications as well as AR, VR and gaming to be more accessible to the telco edge. We announced the NVIDIA EGX Intelligent Edge Computing Platform. With an ecosystem of more than 100 technology companies worldwide, early adopters include Walmart, BMW, Procter & Gamble, Samsung Electronics, NTT East and the cities of San Francisco and Las Vegas. Additionally, we announced a collaboration with Microsoft on intelligent edge computing. This will help industries better manage and gain insights from the growing flood of data created by retail stores, warehouses, manufacturing facilities and urban infrastructure. Finally, last week, we held our GPU Technology Conference in Washington, D.C., which was sold out with more than 3,500 registered developers, CIOs and federal employees. At the event, we announced that the U.S. Postal Service, the world's largest delivery service with almost 150 billion pieces of mail delivered annually, is adopting AI technology from NVIDIA, enabling 10x faster processing of package data and with higher accuracy. Moving to ProVis. Revenue reached a record $324 million, up 6% from the prior year and up 11% sequentially driven primarily by mobile workstations. NVIDIA RTX graphic and Max-Q technology have enabled a new wave of mobile workstations that are powerful enough for design applications yet thin and light enough to carry. We expect this to become a major new category with exciting growth opportunities. Over 40 top creative design applications are being accelerated with RTX GPUs. Just last week, at the Adobe Max Conference, RTX accelerated capabilities were added to 3 Adobe Creative apps. RTX-accelerated apps are now available to tens of millions of artists and designers, driving demand for our RTX GPUs. We also continue to see growing customer deployment of data science, AI and VR applications. Strong demand this quarter came from manufacturing, public sector, higher education and health care customers. Finally, turning to automotive. Revenue was $162 million, down 6% from a year ago and down 22% sequentially. The sequential decline was driven by a onetime nonreoccurring development services contract recognized in Q2. Additionally, we saw a roll-off of legacy infotainment revenue and general industry weakness. Our AI cockpit business grew driven by the continued ramp of the Daimler as they deploy their AI-based infotainment systems across their fleet of Mercedes-Benz vehicles. In August, Optimus Ride launched New York City's first autonomous driving pilot program powered by NVIDIA DRIVE. Urban settings pose unique challenges for autonomous vehicles given the number of density of objects that need to be perceived and comprehended in real time. Our DRIVE computer and software stack allows these shuttles to safely and effectively provide first- and last-mile transit services. We remain excited about the long-term opportunity in auto. Our offering is -- consists of in-car AV computing platforms as well as GPU servers for all AI development and simulation. We believe we are well positioned in the industry with leading end-to-end platform that enables customers to develop, test and safely operate autonomous vehicles, ranging from cars and trucks to shuttles and robo-taxis. Moving to the rest of the P&L. Q3 GAAP gross margins was 63.6%, and non-GAAP was 64.1%, up sequentially, reflecting a benefit from sales of previously written-off inventory, higher GeForce GPUs average selling prices and lower component costs. GAAP operating expenses were $989 million, and non-GAAP operating expenses were $774 million, up 15% and 6% year-on-year, respectively. GAAP EPS was $1.45, down 26% from a year earlier. Non-GAAP EPS was $1.78, down 3% from a year ago. Cash flow from operations was a record $1.6 billion. With that, let me turn to the outlook for the fourth quarter of fiscal 2020, which does not include any contribution from the pending acquisition of Mellanox. We expect revenue to be $2.95 billion, plus or minus 2%. This reflects expectations for strong sequential growth in data center, offset by a seasonal decline in notebook GPUs for gaming and Switch-related revenue. GAAP and non-GAAP gross margins are expected to be 64.1% and 64.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $1.02 billion and $805 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of approximately $25 million. GAAP and non-GAAP tax rates are both expected to be 9%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $130 million to $150 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight the upcoming events for the financial community. We will be at the Crdit Suisse Annual Technology Conference on December 3, Deutsche Bank's Auto Tech Conference on December 10 and Barclays Global Technology, Media and Telecommunications Conference on December 11. We will now open the call for questions. Operator, would you please poll for questions?","For my first one, you mentioned that you were seeing strong sequential growth in the data center going into Q4. Jensen, I was wondering if you could give us some color on what's driving that, and just how you think about the sustainability of data center growth going into next year and what markets do you think will drive that. Is it more enterprise, more hyperscale, more HPC? Just some color on near and longer term on data center. And then I have a follow-up for Colette. Yes. Thanks a lot, Vivek. We had a strong Q3 in hyperscale data centers. As Colette mentioned earlier, we shipped a record number of V100s and T4s. And for the very first time, we shipped more T4s than V100. And most of the T4s are driven by inference. In fact, our inference business is now a solid double-digit, and it doubled year-over-year. And all -- most -- that is really driven by several factors. The -- as you know, we've been working on deep learning for some time, and people have been developing deep learning models. It started with computer vision. But image recognition doesn't really take that much of the data center capacity. Over the last couple of years, a couple of very important developments have happened. One development is a breakthrough in using deep learning for recommendation systems. As you know, recommendation systems is the backbone of the Internet. Whenever you do shopping, whenever you're watching movies, looking at news, doing search, all of the personalized web pages, all of just about your entire experience on the Internet is made possible by recommendation systems because there is just so much data out there putting the right data in front of you based on your social profile or your personal use patterns or your interest or your connections. All of that is vitally important. For the very first time, we're seeing recommendation system based on deep learning throughout the world. And so increasingly, you're going to see people roll this out. And the backbone of the Internet is now going to be based on deep learning. The second part is conversational AI. Conversational AI has been coming together in pieces; at first, speech recognition, which requires some amount of noise processing or beam forming. Then you go into speech recognition. Then it goes to natural language understanding, which then gets connected to a recommendation system, which then gets connected to text-to-speech and a speech encoder. And then that has to be done very, very quickly. Whereas images could be done off-line, conversation has to be done in real time. And without acceleration and without NVIDIA's accelerators, it's really not possible to do it in real time. It takes seconds to process all of the handful of deep learning models, and now we're able to do that all on an accelerator and do it in real time. And so the combination of these various breakthroughs from deep learning-based recommenders, the speech stack as well as natural language understanding breakthrough in what is called a bidirectional encoded transformer, that breakthrough is really quite significant. And since then, derivative works have come from that approach. And natural language understanding is really, really working incredibly well. And so what we're seeing people do is -- the hyperscalers across the world, we work with just about everybody, this area of work is really complicated. The models are very, very large. There's a whole bunch of models that has to work together, and  they're getting larger. And so that's one large category, which is the hyperscalers. The second, which we introduced this quarter, is really about taking AI out to the edge. And the reason for that is because there are many applications, whether it's based on video or other types of sensors of all kinds where there's a vibration sensor, temperature sensors, barometric sensor. There's all kinds of sensors that are used in industries to monitor the health of equipment, monitor the conditions of various situations. And you want to do the processing at the point of action. This way, you don't have to screen the data, which is continuous back into the cloud, which costs a lot of money. You want to take the action at the point of action because latency matters. Maybe you're controlling gates or vehicles or robots or drones or whatnot. And then lastly, one major issue is data sovereignty. Maybe your company doesn't own all of the data that you are processing and, therefore, you have to do that processing at the edge, and you can't afford to put that into the cloud. And so these various industries: retail, warehouse, logistics, smart cities, we're just seeing so much enthusiasm there around that. And this -- so we built a platform called the EGX, which basically is a cloud native, completely secure, takes advantage of NVIDIA's full stack of every single model. And it's managed with Kubernetes remotely, and you could deploy these services at the edge in faraway places because IT departments can't afford to go out there to manage them. And we've seen some really great adoption. We announced this last quarter. Walmart is using our platform. BMW is using it for logistics, Procter & Gamble for manufacturing, Samsung Electronics for manufacturing, visual inspection. And then last week, we announced probably the largest logistics operation in the world, the United States Postal Service. And so those are -- I would say that intelligent edge will likely be the largest AI industry in the world for rather clear reasons. If you just kind of estimated the size of retail, it's nearly $30 trillion. And if retail stores could be made a little bit more convenient, it could save the industry a lot of money: warehouses, logistics, transportation, farming. I think there's like 0.5 million farms in the world, covers 1/3 of the world's land mass. And so there's a lot of places where AI could be put at the edge and could make a big difference. And I think this is going to be the grand adventure that we started this last quarter with the announcement of NVIDIA EGX. Right. And Jensen, as quick follow-up, on PC gaming, how are you looking at growth going forward in that you had a very good quarter in October? I think in January, you're probably guiding to some seasonal declines, but I imagine a lot more of that is due to console decline. Just how are you looking at PC gaming growth going into October -- into January and then next year as you get competition from 2 new consoles that are also supposed to come out? Yes. The -- during Q3 -- during Q4 and Q1, we see normal seasonal declines of console builds, and we also see a normal seasonal decline of notebook builds. And the reason for that is because the notebook vendors have to line up all their manufacturing in Q3 so that they could meet the hot selling season in Q4. And so we're seeing -- what we see in the Q4 and Q1 time frame are just normal seasonal declines of these systems. Overall, for PC gaming -- and RTX is doing fantastic. Let me tell you why it's so important. I would say that at this point, I think it's fairly clear that ray tracing is the future and that RTX is a home run. Just about every major game developer has signed on to ray tracing. Even the next-generation consoles had to stutter step and include ray tracing in their next-generation consoles. The effects -- the photorealistic look is just so compelling, it's not possible to really go back anymore. And so I think that it's fairly clear now that RTX ray tracing is the future. And there are several hundred million PC gamers in the world that don't have the benefits of it, and I'm looking forward to upgrading them. Second, and this is a combination of RTX and Max-Q, we really created a brand-new game platform, notebook PC gaming. Notebook PC gaming really didn't exist until Max-Q came along. And our second-generation Max-Q, this last season, really turbocharged this segment. Over 100 laptops now are available for PC gaming. And my sense is that this is likely going to be the largest gaming platform, new gaming platform that emerges. And we're just in the beginning innings of that. And so the combination of upgrading the entire installed base of PC gamers to RTX and ray tracing and this new gaming segment called notebook PC gaming is really quite exciting, and it's going to drive our continued growth for some time. And so I'm excited about that. Sure. Thanks for the question. In the current quarter, the net benefit, as we refer to as the net release of our inventory provisions primarily associated with our components, was about 1 percentage point to our overall gross margin. As you know, going forward, mix is still the largest driver of our gross margin over time. Over the long term, we do expect gross margins to improve, and we'll continue to see, outside of the benefit that we received, gross margin improvement for the long term. Yes. As you know, just to add to that, as you know, NVIDIA's really become a software company. If you take a look at almost all of our products, the GPU -- having the world's best GPU, of course, is the starting point. But almost everything that we do, whether it's in artificial intelligence or data analytics or health care or robotics or self-driving cars, almost all of these platforms: gaming, rendering, cloud graphics, all of these platforms start from a really rich stack of software. And you can't just put a chip in these scenarios and they work. And so most of our businesses are now highly software-rich, and they address verticals that we focus on. And then secondarily, we're a platform company. And so our platform is available from all the OEMs and cloud providers. And as a platform company that has a great deal of software intensity, it's natural that the margins would be higher over time. Yes. We had a strong Q3. We're going to see a much stronger Q4. And the foundation of that is AI, it's deep learning inference. That is -- this deep learning inference is understandably going to be one of the largest computer industry opportunities. And the reason for that is because the computation intensity is so high. And for the very first time, aside from computer graphics, this mode of software is not really practical without accelerators. And so I mentioned earlier about the large-scale movement to deep learning recommendation systems. Those models are really, really hard to train. I mentioned earlier about conversational AI. Because conversation requires real-time processing, several seconds is really not practical. And so you have to do it in milliseconds, tens of milliseconds. And our accelerator makes that possible. What makes it really complicated and the reason why -- although so many people talk about it, only we demonstrated -- we submitted all 5 results -- all 5 tests for the MLPerf inference benchmark, and we won them. And the reason for that is because it's far more than just a chip. The software stack that sits on top of the chip and the compilers that sits on top of the chip are so complicated. And it's understandably complicated because a supercomputer wrote the software, and this body of software is really, really large. And if you have to make it both accurate as well as performant, it's really quite a great challenge. And it's one of the great computer science challenges. This is one of those problems that hasn't been solved, and we've been working hard at it for the last 6, 7 years now. And so this is really the great opportunity. We've been talking about inference for some time now. Finally, the workloads and a very large diverse set of workloads are now moving into production. And so I'm hoping -- I'm enthusiastic about the progress and seeing the trends and the visibility that inference should be a large market opportunity for us. Yes. C.J., that's really good. Let me break it down. So when we think about hyperscale, there are 3 parts: training, inference and public cloud. Training, you might have seen the work that was done at open AI recently where they've been measuring and monitoring the amount of computation necessary to train these large models. These large models are now only getting larger. The amount of data necessary, therefore, has to scale as well. The computation is now growing and doubling every 3 months. And the reason for that is because of recent breakthroughs in natural language understanding. And all of a sudden, a whole wave of problems are now able to be solved. And just as AlexNet 7 years ago kind of was the watershed event for a lot of computer vision-oriented AI work, now the transformer-based natural language understanding model and the work that Google did with BERT really is a watershed event also for natural language understanding. This is, of course, a much, much harder problem. And so the scale of the training has grown tremendously. I think what we're going to see this year is a fair number of very sizable installations of GPU systems to do this very thing, training. The second part is an untapped market for us, and this untapped market is really inference. The reason why I haven't really spoken about it until now is because we've never really been able to validate our intuition that inference is going to be a large market opportunity for us, that it's going to be very complicated. The models are very large. They're very diverse. They require large amount of computation, large amount of memory bandwidth and large amounts of memory and large and significant capabilities of programmability. And so I've talked about this before, but I've never been able to validate it. And of course, with MLPerf and sweeping the benchmarks and, frankly, only -- the only one although so many have attempted, they submitted results and some of them resented it, that this benchmark is just really, really hard. Inference is hard. And then finally, our business results also validated the -- our intuition. And so our engagement now with CSPs are now global. We're working across natural language understanding, recommendation systems, conversational AI, just a whole bunch of really, really interesting problems. Now the cloud is the third piece. And the reason why cloud is growing so well and represents about half almost of many of our CSPs, particularly the ones with the public cloud, the reason for that is because the number of AI start-ups in the world is still growing so incredibly. I think we're tracking something close to 10,000 and more AI start-ups around the world. In health care, in transportation, in retail, in consumer Internet, in Fintech, the number of AI companies out there is just extraordinary. I think over the last 3 or 4, 5 years, some $20 billion, $30 billion have been invested into start-ups. And these start-ups, of course, use cloud service providers so that they don't have to invest in their own infrastructure because it's fairly complicated. And so we're seeing a lot of growth there. And so that's just the hyperscalers. The hyperscalers give us 3 points of growth -- 3 areas of growth: training, inference and public cloud. And the public cloud is primarily AI start-ups. Then there's the intelligent edge, which we recently ventured into, and we've been building this platform called EGX for some time. And it's cloud native. It's incredibly secure. You can manage it from afar. It's -- the stack is complicated. It's performant. And we saw some -- we've been working with some early adopters. And this last quarter, we announced some of them: Walmart and BMW and Procter & Gamble and the largest logistics company in the world, USPS. And so this new platform, I think, long term, will likely be the largest opportunity. And the reason for that is because of the industries that it serves. Gaming is solid in China, and it is also the fastest adopter of our gaming notebooks. This gaming RTX notebooks or GeForce notebooks is really a brand new category. This category never existed before because we couldn't get the technology in there so that it's both delightful to own as well as powerful to enjoy. And so we saw really great success with RTX notebooks and GeForce notebooks in China, and RTX adoption has been fast. Your comments make sense because most of the games are free-to-play these days. The primary games that people play are esports, which you want the best gear, but you could -- after you buy the gear, you pretty much enjoy it forever; and mobile, which is largely free-to-play. You invest in some of your own personal outfits. And after that, I think you can enjoy it for quite a long time. And so the gear is really important. One of the areas where we've done really great work, particularly in China, has to do with social. We have this platform called GeForCe Experience. And as an extension of that, there's a new feature called RTX Broadcast Engine. And it basically applies AI to broadcasting your content to share it. You could make movies. You could capture your favorite scenes and turn it into art, applying AI. And one of the coolest features is that you could overlay yourself on top of the game and share it with all the social networks without a green screen behind you. We use AI to stitch you out, basically, to cut you out of the background and irrespective of what noisy background you've got. And so as you know, China has really a super hyper social community -- communities back there and they have all kinds of really cool social platforms to share games and user-generated content and short videos and all kinds of things like that. And so GeForce has that one additional feature that really makes it successful. Yes. Toshiya, let me address the first question regarding our legacy infotainment systems for our automotive business. It is still representing maybe about half or more of our overall revenue in the automotive business. We have our AI cockpit continuing to grow and grow quite well, both sequentially as well as year-over-year, as well as our autonomous vehicle solutions that we may be doing, including development services. Let's see. The -- we're the first -- probably the first AV car that's going to be passenger-owned on the road, and I think we've talked about it before, is Volvo. And we're expecting them to be in the late 2020, early 2021 time frame. And I'm still expecting so. And then there's the 2022, 2023 generations. Most -- I would say most of the passenger-owned vehicle developments are going quite well. The industry, as you know, is under some amount of pressure, and so a lot of them have slipped it out a couple of years or so. And this is something that I think we've already spoken about in the past. Our focus, our strategy consists of several areas. One area, of course, is passenger-owned vehicles. The second part is robot taxis. We have developments going with just about every major robot taxi company that we know of. And they're here in the states. They're in Europe. They're in China. And when you hear news of them, we're delighted to see their progress. And then the third part has to do with trucks, shuttles and increasingly a large number of vehicles that don't carry people, they carry goods. And so we have a major development with Volvo. That was Volvo Trucks. Volvo Cars and Volvo Trucks, as you know, are 2 different companies. One of them belongs to Geely, Volvo Cars. Volvo Trucks is the heritage Volvo. And we have a major program going with them to automate the delivery of goods. You also see us during various GTCs, I'll mention companies that we're working with on grocery delivery or goods delivery or within a warehouse product delivery. You're going to see a whole bunch of things like that because the technology is very similar, and it's starting to -- the development -- the technology we develop for passenger-owned vehicles has started to propagate down into logistics vehicles. I continue to believe that everything that moves eventually will have autonomous capability or be fully autonomous. And that, I think, is, at this point, fairly certain. Now our strategy is both in developing the in-car AV computing system, and it's software-defined, it's scalable, as well as the AI development and simulation systems. And so when somebody's working on AV and they're using AI, and most of them are, there's a great opportunity for us. And when they start ramping up and they're collecting miles of data, it becomes a very large market opportunity for us. And so I'm anxious to see every single car company be as progressive and aggressive in developing AV. And they will be. They will be. This is a foregone conclusion. Sure, Stacy. When we had provided guidance in Q3 and how we finished the quarter in Q3, we had indicated that our growth would stem from both gaming and data center. We completed that. And we also had stronger than expected from guidance from both gaming and data center in our Q3 results. Moving to Q4, Q4 is a sequential decrease in totality versus Q3. We have reminded the teams about our overall seasonality that we sometimes have in gaming associated with our consoles as well as also with our notebooks that seem to be primarily in Q2 and Q3 being our strongest quarters and likely, therefore, a seasonal downtick as it move to Q4. What we wanted to do was, if we have in totality overall decline associated with that, we did want to emphasize what we are expecting in terms of data center with the overall strong growth sequentially. I would believe our growth of 17% was higher than we expected to Q3. Again, when we get into Q4, we'll see how the quarter ends in terms of data center, but we are expecting strong growth. Thanks, Stacy. Sure. Our enterprise business has been beginning to ramp from over a year ago at a very, very, very small base. We've continued to see great traction in there with a lot of the things that we've announced throughout. But keep in mind in our year ago quarter, we also had very strong systems and a very large deal associated with our DGX. So when we look from a quarter-over-quarter period or just looking at 1 quarter, we can have a little bit of lumpiness. So that year-over-year impact is really just due to an extremely large deal in the prior year Q3. Let's see. I would say 2018, it was nearly all related to training. And this year, we started to see the growth of inference to the point where we now -- we have now sold more -- this last quarter, we sold more T4 GPUs for inference than we sold V100s that's used for training, and both of them were record highs. And so the comment that Colette just made, comparing to year-over-year, we had a large DGX system sale a year ago that we didn't have this year. But if you excluded that, the V100 and the T4 is doing great. They're at record levels. And T4 didn't hardly existed a year ago, now it's selling more than V100s, and both of them are record highs. And so that kind of gives you a feeling for it. I think that's really the major difference that inference is really kicking into gear, and my sense is that it's going to continue to grow quite nicely. I wonder if you could talk a little bit more about the 5G opportunity that you announced at Mobile World. And I guess you talked a lot about AI and IoT services in a C-RAN environment. But is there -- how big is that opportunity? And can you address kind of the core compute aspect to C-RAN with the GPU? Yes. If you look at the world of mobile today, there are players that are building DRAMs and their radio heads in the BBU, basically the baseband units. In the data center where people would like to move the software for radio networks, it's really an untapped market. And the reason for that is because the CPU is just not able to support the level of performance that's necessary for 5G. And ASICs are too rigid to be able to put into a data center. And so the data center needs a programmable solution that is data center-ready that can support all of the software richness that goes along with the data center, whether it's a VM environment like VMware. And we -- recently, during the quarter, we announced another partnership with VMware. They recognize that increasingly, our GPUs are becoming a core part of data centers and cloud. We had a partner -- we announced a partnership with Red Hat. They realize the momentum that they're seeing us in, in telcos, and they would like to adapt their entire stack from open stack to OpenShift on top of our GPUs. And so now with VMware, with Red Hat, we're going to have a world-class telco enterprise stack that ranges all the way from hypervisors and virtual machines all the way to Kubernetes. And so our strategy is to -- our goal is to really create this new world of C-RAN, vRAN centralized data centers and software-defined networking. And the software-defined networking will, of course, include things like in the data center networking as well as  firewalls. But the computationally-intensive stuff is really the 5G radio. And so we're going to create a software stack for 5G and basically exactly the same way that we've done for creating a -- excuse me, a software stack for deep learning. And we call it Aerial. Aerial is to 5G essentially what Cuda-NN is for deep learning and essentially what optics is for ray tracing. And this software stack is going to allow us to run the whole software -- run the whole 5G stack in software and deliver the highest performance, the incredible flexibility and scale to as many layers of MIMO as customers need and to be able to put all of it in the data center. The power of putting it into data center, as you know, is flexibility and fungibility. With the low latency capability of 5G, you could put a data center somewhere in the regional hub. And depending on where the traffic is going, you could shift the traffic computation from 1 data center to another data center, something that you can't do in basebands, in baseband units in the cell towers, but you can do that in the data center. And that helps them reduce the cost. The second benefit is that the telcos would love to be a service provider for a data center's computation at the edge. And the edge applications are things like smart cities and whether it's warehouses or retail stores or whatever it is because they're geographically located and is distributed all over the world. And so to be able to use their data center to also be able to use AI in combination with IoT is really exciting to them. And so I think that that's really -- this is really the future that we're going to see a lot more service providers at the edge. And these edge data centers will have to run the data center, the networking, including the mobile network and software as well as run 5G and IoT -- AI and IoT applications I would say, for our Q4, both of them are expected to be seasonally down. In the case of the consoles, we do wait for Nintendo to assist in terms of what they need. So we will have to see how the quarter ends on that. But in both cases, in totality, these businesses have ranged, maybe in totality of the 2, of about $500 million a quarter. And we'll see both of them sequentially decline. Thank you. Thanks, everyone. We had a good quarter driven by strong gaming growth and hyperscale demand. We're making great strides in 3 big impact initiatives. The world of computer graphics is moving to ray tracing, and our business reflects that. Some of the biggest blockbuster games this holiday season and beyond are RTX-enabled, including Call of Duty: Modern Warfare; and the best-selling game of all-time, Minecraft. Design applications used by millions of artists and creators are rapidly adopting RTX ray tracing. We're reinventing computer graphics and look forward to upgrading the hundreds of millions of PC gamers to RTX. Hyperscale demand was strong this quarter, and our visibility continues to improve. The race is on for conversational AI, which will be a powerful catalyst for us in both training and inference. And lastly, we have extended our computing platform beyond the cloud to the edge, where GPU-accelerated 5G, AI and IoT, will revolutionize the world's largest industries. We look forward to updating you on our progress in February.","I have a follow-up if I can as well. Just thinking about the trajectory of gross margin here, solid gross margin upside in the quarter, you also noted that you had the benefit of selling through some written-off components. So I guess first question is what was that impact in this most recent reported quarter. And how do we think about the trajectory of gross margin here even beyond the January quarter? What should we be thinking about in terms of that gross margin trend? And again, I have a quick follow-up. Yes. Very helpful. And then you mentioned in your prepared remarks that you've seen hyperscale -- your hyperscale business within data center grow both on a quarter-over-quarter as well as year-over-year basis in this last print. You also mentioned that your visibility is improving. Can you just help us understand what exactly you're seeing in the hyperscale guys because it feels like there's some mixed data points out there? What underpins your improved visibility? Or what are you seeing in that piece of your business? I guess I'd love to follow on, on that last question. So clearly, your commentary, Jensen, here is much more bullish than I've heard you, I think, before on inference, particularly as it relates to this first benchmark. And so I guess can you talk a bit about how you see mix within data center looking out over the next 12, 24 months as you see kind of training versus inference as well as cloud versus enterprise, considering, I would think, inference over time could be -- could grow into a large opportunity there as well? There are a lot of concerns around China trade tensions, economic slowdown. But history has shown that gamers tend to be less sensitive to these macro trends and, in fact, also somewhat insensitive to price changes, at least at the enthusiast level. So given that China is such a big part of the gaming segment, can you just discuss the gaming demand trends out of this geography? I wanted to ask on automotive. Colette, in your prepared remarks, you talked about your legacy infotainment business being down in the quarter. Just curious, what percentage of automotive revenue at this point is legacy infotainment versus the newer AI/ADAS solutions? And more importantly, Jensen, if you can speak to the growth trajectory in automotive over the next 1.5 years, maybe 2, that would be appreciated. And I do ask the question because it feels like we've heard many, many announcements, customer announcements, collaborative work that you're doing with your customers, yet we haven't quite seen sort of a hockey-stick inflection that some of us were expecting a couple of years ago. So just kind of curious when we should -- how we should set our expectations going forward. I have 2 data center questions for Colette. The first question, I want to return to your kind of outlook for strong sequential data center growth in Q4. Now this business grew 11% sequentially in Q3. And you didn't actually call out strong growth as we were going into the quarter. You are calling it out for Q4. Does that suggest to me that you expect sequential growth in Q4 to be stronger than Q3 given you're calling it out in Q4 and you didn't call it out in Q3? Or would you define like what you saw in Q3 as well as already being strong sequential growth? Like how do we think about the wording of that in relation to what we've seen in Q3 and what you expect for Q4? So I guess to ask the question again, would you define what you saw in Q3 as being strong growth as well? Okay. And for my second question, hyperscale you said was up year-over-year. Now -- and that's after, off of last year, where it was the peak. Inference doubled year-over-year. And this suggests to me -- I know you said enterprise was down year-over-year. But this suggests to me that it wasn't just down year-over-year, it was down a lot year-over-year. How do we think about that in the context of like the growth that we've seen very strongly over the last few quarters in enterprise. And going back to your commentary at the Analyst Day, which was almost entirely about the opportunity coming from enterprise growth, what's going on there? What drove that? And what should we expect going forward? I apologize for any background noise, but I just have one question, just for Jensen. So in 2018, can you give us a rough update on what the GPU utilization was for deep learning application? What it is today? I'm just wondering how the -- how that's advanced over the last couple of year or 2. I apologize for the background noise. But Colette, maybe you could give us an idea of gaming. In the guidance, it's down. And I was wondering, could you maybe give us the impact of the console business versus the laptop and give us an idea of what might be the bigger driver there? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q2 2021,2652,4528,967,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the second quarter of fiscal 2021. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2021. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, August 19, 2020, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. Q2 was another extraordinary quarter. The world continued to battle the COVID-19 pandemic, and most of our employees continued to work from home. But through the team's agility and dedication, we successfully combined Mellanox into NVIDIA while also delivering a very strong quarter. revenue was $3.87 billion, up 50% year-on-year, up 26% sequentially and well ahead of our outlook. Starting with gaming. Revenue was $1.65 billion, was up 26% year-on-year and up 24% sequentially, significantly ahead of our expectations. The upside is broad-based across geographic regions, products and channels. Gaming's growth amid the pandemic highlights the emergence of a leading form of entertainment worldwide. For example, the number of daily gamers on Steam, a leading PC game online distributor is up 25% from pre-pandemic levels. And NPD reported that U.S. consumer spending on video games grew 30% in the second calendar quarter to a record $11 billion. NVIDIA's PCs and laptops are ideal for the millions of people who are now working, learning and gaming at home. At the outset of the pandemic, many retail outlets were closed, and demand shifted to online channels. As the quarter progressed and the stores reopened, retail demand picked up, iCafes largely reopened and online sales continued to thrive. Gaming laptop demand is very strong as students and professionals turn to GeForce-based systems to improve how they work, learn and game from home. We ramped over 100 new models with our OEM partners, focused on both premium and mainstream price points. In the premium laptop segment, we delivered unparalleled performance with the GeForce RTX 2080 and the 2070 SUPER GPUs InfiniBand form factors. We also brought ray tracing to gaming laptops for the first time at price points as low as $999 with the GeForce RTX 2060. In the mainstream segment, we brought the GeForce GTX to laptop price points as low as $699. Momentum continues for our Turing architecture, which enables stunning new visual effects in games and is driving powerful upgrade cycle among gamers. Its RTX technology adds ray tracing and AI to programmable shading and has quickly redefined the new standard for computer graphics. DLSS used the AI capabilities of Turing to boost frame rates by almost 2x while generating crisp image quality. RTX support in blockbuster games continues to grow, including megahit Death Stranding, the high anticipated Cyberpunk 2077 and the upcoming release of Watch Dogs. These games join Minecraft and other major titles that support NVIDIA RTX ray tracing and DLSS. We're in the midst of a 21-day countdown campaign, promoting a GeForce special event on September 1, with each day highlighting a year in the history of GeForce. We don't want to spoil the surprise, but we encourage you to tune in. We are very pleased with the traction of our GeForce NOW cloud gaming service, now in its second quarter of commercially availability. GFN offers the richest content to any game streaming service through partnerships with leading digital game stores, including Valve Steam, Epic Games and Ubisoft Uplay. GeForce NOW enables users with underpowered PC, Macs or Android devices to access powerful GPUs to play their libraries of PC games in the cloud, expanding the universe of gamers that we can reach with GeForce. Just yesterday, we announced that GFN is now supported on Chromebooks, further expanding our reach in to tens of millions of users. In addition to NVIDIA's own service, GFN is available or coming soon to a number of telecom partners around the world, including SoftBank and KDDI DION in Japan, Rostelecom and Beeline in Russia, LG U+ in South Korea and Taiwan Mobile. Moving to Pro Viz. In Q2 was $203 million in revenue, down 30% year-on-year and down 34% sequentially, with declines in both mobile and desktop workstations. Sales were hurt by lower enterprise demand amid the closure of many offices around the world. Industries negatively impact during the quarter include automotive, architectural, engineering and construction, manufacturing, media and entertainment and oil and gas. Initiatives by enterprises to enable remote workers drove demand for virtual and cloud-based graphic solutions. Accordingly, our Q2 vGPU bookings accelerated, increasing 60% year-on-year. Despite near-term challenges, we are winning new business in areas such as health care, including Siemens, Philips and General Electric, and the public sector. We continue to expand our market opportunity with over 50 leading design and creative applications that are NVIDIA RTX-enabled, including the latest release from Foundry, Chaos Group and Maxon. These applications provide faster ray tracing and accelerated performance, improving creators design workflows. The pandemic will have a lasting impact on how we work. Our revenue mix going forward will likely reflect this evolution in enterprise workforce trends with a greater focus on technologies, such as NVIDIA laptops and virtual workstations, that enable remote work and virtual collaboration. Moving to automotive. Automotive revenue was $111 million, down 47% year-on-year and down 28% sequentially. This was slightly better than our outlook of a 40% sequential decline as the impact of the pandemic was less pronounced than expected, with auto production volumes starting to recover after bottoming in April. Some of the decline is also due to the roll-off of legacy infotainment revenue, which remained a headwind in future quarters. In June, we announced a landmark partnership with Mercedes-Benz which, starting in 2024, will launch software-defined intelligent vehicles across an entire fleet in using end-to-end NVIDIA technology. Mercedes will utilize NVIDIA's full technology stack, including the DRIVE AGX computer, DRIVE AV autonomous driving software and NVIDIA's AI infrastructure, spanning from the core to the cloud. Centralizing and unifying computing in the car will make it easier to integrate and upgrade advanced software features as they are developed. With over-the-air updates, vehicles can receive the latest autonomous driving and intelligent cockpit features, increasing value and extending majority of ownership with each software upgrade. This is a transformative announcement for the automotive industry, making the turning point of traditional vehicles becoming high-performance, updatable data centers on wheels. It's also a transformative announcement for NVIDIA's to evolving business model as the software content of our platforms grows, positioning us to build a recurring revenue stream. Moving to data center. Data center is diverse, consist of cloud service providers, public cloud providers, supercomputing centers, enterprises, telecom and industrial edge. Q2 revenue was a record $1.75 billion, up 167% year-on-year and up 54% sequentially. In Q2, we incorporated a full quarter of contribution from the Mellanox acquisition, which closed on April 27, the first day of our quarter. Non-ops contributed approximately 14% of company revenue and just over 30% of data center revenue. Both compute and networking within data center set a record with accelerating year-on-year growth. The biggest news in data center this quarter was the launch of our Ampere architecture. We are very proud of the team's execution in launching and ramping this technological marvel, especially amid the pandemic. The A100 is the largest chip ever made with 54 billion transistors. It runs our full software stack for accelerating the most compute-intensive workloads. Our software leases include CUDA 11, the new versions of over 50 CUDA-X libraries and a new application frameworks for major AI workloads, such as Jarvis for conversational AI and Merlin for deep recommender systems. The A100 delivers NVIDIA's greatest generational leap ever, boosting AI performance by 20x over its predecessor. It is also our first universal accelerator, unifying AI training and inference and powering workloads, such as data analytics, scientific computing, genomics, edge video analytics, 5G services and graphics. The first Ampere GPU, A100, has been widely adopted by all major server vendors and cloud service providers. Google Cloud Platform was the first cloud customer to bring it to market, making it the fastest GPU to come to the cloud in our history. And just this morning, Microsoft Azure announced the availability of massively scalable AI clusters, which are based on the A100, and interconnected with 200 gigabyte per second Mellanox InfiniBand networking. A100 is also getting incorporated into offerings from AWS, Alibaba Cloud, Baidu Cloud and Tencent Cloud. And we announced that the A100 is going to market with more than 50 servers from leading vendors around the world, including Cisco, Dell, Hewlett Packard Enterprise and Lenovo. Adoption of the A100 into leading server makers offerings is faster than any prior launch, with 30 systems expected this summer and over 20 more by the end of the year. The A100 is already winning industry recognition. In the latest A100 training benchmark, MLPerf 0.7, NVIDIA set 16 records, sweeping all categories for commercially available solutions in both per chip and outscale performance based on the A100. MLPerf offers the industry's first and only objective AI benchmark. Since the benchmark was introduced 2 years ago, NVIDIA has consistently delivered leading results and record performance for both training and inference. NVIDIA also topped the chart in the latest top 500 list of the fastest supercomputers. The ranking, released in June, showed that 8 of the world's top 10 supercomputers use NVIDIA GPUs, NVIDIA networking or both. They include the most powerful systems in the U.S. and Europe. NVIDIA, now combined with Mellanox, powers 2/3 of the top 500 systems on the list compared with just less than a half for the 2 companies in total 2 years ago. In energy efficiencies, systems using NVIDIA GPUs are pulling away from the pack. On average, they are nearly 2.8x more powerful and efficient than systems without NVIDIA GPUs, measured by gigaflops per watt. The incredible performance and efficiency of the A100 GPU is best amplified by NVIDIA's own new Selene supercomputer, which debuted as #7 on the top 500 list and is the only top 100 systems to cross the 20 gigaflops per watt barrier. Our engineers were able to assemble Selene in less than 4 weeks using NVIDIA's open modular DGX SuperPOD reference architecture instead of the typical build time of months or even years. This is the system that we will use to win the MLPerf benchmarks, and it is a reference design. It's available for our customers to quickly build a world-class supercomputer. We also brought GPU acceleration to data analytics, one of the largest and fastest-growing enterprise workload. We enabled an acceleration of the entire data analytics workload pipeline for the first time with NVIDIA's GPUs and software stack in the latest version of Apache Spark released in June. Spark is the world's leading data analytics platform used by more than 500,000 data scientists and 16,000 enterprises worldwide. And we have 2 major milestones to share. We have now shipped a cumulative total of 1 billion CUDA GPUs, and the total number of developers in the NVIDIA ecosystem just reached 2 million. It took over a decade to reach the first million and less than 2 years to reach the second million. Mellanox has fantastic results across the board in its first quarter as part of NVIDIA. Mellanox revenue growth accelerated with strength across Ethernet and InfiniBand products. Our Ethernet shipments reached a new record. Major hyperscale build drove the upside in the quarter as growth in cloud computing and AI is fueling increased demand for high-performance networking. Mellanox networking was a critical part of several of our major new product introductions this quarter. These include the DGX AI system, the DGX SuperPOD clusters for our Selene supercomputer and the EGX Edge AI platform. We also launched the Mellanox ConnectX-6 Ethernet NIC, the 11th generation product of the ConnectX family, and it's designed to meet the needs of modern cloud and hyperscale data centers, where 25, 50 and 100 gigabyte per second is becoming the standard. We expanded our switch networking capabilities with the addition of Cumulus Networks, a privately held network software company that we purchased in June. Cumulus augments our Mellanox acquisition in building out open modern data center. The combination of NVIDIA accelerated computing, Mellanox networking and Cumulus software enables data centers that are accelerated, disaggregated and software-defined to meet the exponential growth in AI, cloud and high-performance computing. Moving to the rest of the P&L. Q2 GAAP gross margin was 58.8% and non-GAAP gross margin was 66%. GAAP gross margin declined year-on-year and sequentially due to costs associated with the Mellanox acquisition. Non-GAAP gross margins increased by almost 6 points year-on-year, reflecting a shift in product mix with higher data center sales and lower automotive sales. Q2 GAAP operating expenses were $1.62 billion, and non-GAAP operating expenses were $1.04 billion, up 67% and 38% from a year ago, respectively. Q2 GAAP EPS was $0.99, up 10% from a year earlier. Non-GAAP EPS was $2.18, up 76% from a year ago. Q2 cash flow from operations was $1.57 billion. With that, let me turn to the outlook for the third quarter of fiscal 2021. We expect revenue to be $4.4 billion, plus or minus 2%. With that, we expect gaming to be up just over 25% sequentially with data center to be up in the low to mid-single digits sequentially. We expect both pro viz and the auto to be at similar levels out in Q2. GAAP and non-GAAP gross margins are expected to be 62.5% and 65.5%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $1.54 billion and $1.09 billion, respectively. Full year GAAP and non-GAAP OpEx is tracking in line with our outlook of $5.7 billion and $4.1 billion, respectively. GAAP and non-GAAP OI&E are both expected to be expense of approximately $55 million. GAAP and non-GAAP tax rates are both expected to be 8%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $225 million to $250 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We will be at the BMO Virtual Technology Summit on August 25, Citi's 2020 Global Technology Conference on September 9, Deutsche Bank's Technology Conference on September 14 and the Evercore's Virtual Memo Forum, The Future of Mobility, on September 21. We will also host a financial analyst Q&A with Jensen on October 5 as part of our next virtual GTC. Our earnings call to discuss our third quarter's results is scheduled for Wednesday, November 18. We will now open the call for questions. Operator, would you please poll for questions? Thank you.","Congratulations on the strong growth and execution. Jensen, I'm wondering how much of the strength that you're seeing in gaming and data center is maybe more temporary because COVID or some customer pull-ins in the data center or so forth? And how much of it is more structural and more secular that can continue even once we get into, hopefully, sooner rather than later, into a more normalized period for the industry? Yes. Vivek, thank you. So first of all, we didn't see pull-ins, and we're in the beginning of our brand-new product cycle with Ampere. And so the vast majority of the data center growth came from that. The gaming industry, they -- with all that's happening around the world, and it's really unfortunate, but it's made gaming the largest entertainment medium in the world. More than ever, people are spending time digitally, spending on time -- spending their time in video games. The thing that people haven't realized about video games is that it's not just the game itself anymore. The variety of different ways that you can play, whether you can hang out with your friends in Fortnite, go to a concert in Fortnite, building virtual worlds in Minecraft, you're spending time with your friends, you're using it to create -- to realize your imaginations. People are using it for broadcast, for sharing ideas and techniques with other people, and so -- and then of course, it's just an incredibly fun way to spend time even by consumption of the video -- of a video game. And so there's just so much that you could do with video games now. And I think that this way of enjoying entertainment digitally has been accelerated as a result of the pandemic, but I don't think it's going to return. Video game adoption has been going up over time and pretty steadily. PC is now the single largest entertainment platform -- is the largest gaming platform. And GeForce is now the largest gaming platform in the world. And as I mentioned, it's not just about gaming. There's just so many different ways that you could enjoy games. With data center, the thing that -- the structural change that's happening in data center are a couple of different dynamics that are happening at the same time. The first dynamic, of course, is the movement to the cloud. The way that a cloud data center is built and the way that an enterprise data center or a cluster is built is fundamentally different. And it's really, really beneficial to have the ability to accelerate applications that cloud service providers would like to offer, which is basically everything. And we know that one of the most important applications of today is artificial intelligence. It's a type of software that really wants acceleration, and NVIDIA's GPU acceleration is the perfect medium, perfect platform for it. And then the last reason about the data center is the architectural change from hosting applications to hosting services that's driving this new type of architecture called disaggregation versus hyper converged. And the original name of hyperscalers is a large data center of a whole bunch of hyperconverged computers. But the computers of today are really disaggregated. A single application service could be running on multiple servers at the same time, which generates a ton of east-west traffic, and a lot of it is artificial intelligence neuro network models. And so because of this type of architecture, 2 components, 2 types of technologies are really important to the future of cloud. One of them, as I mentioned, was -- is acceleration, and our GPU is ideal for it. And then the other one is high-speed networking. And the reason for that is because the server is now disaggregated, the application is fractionalized and broken up into a bunch of small pieces that are running across the data center. And whenever an application needs to send parts of the answer to another server for the microservice to run, that transmission is called east-west traffic, and the most important thing you could possibly do for yourself is to buy really high-speed, low-latency networking. And that's what Mellanox is fantastic at. And so we find ourselves really in this perfect condition where the future is going to be more virtual, more digital, and that's one -- that's the reason why GeForce is so successful. And then we find ourselves in a world where the future is going to be more autonomous and more AI-driven, and that's the benefit of our GPUs. And then lastly, cloud microservice transactions really benefit high-speed networking, and that's where Mellanox comes in. And so I think that this is -- the dynamics that I'm describing are permanent, and it's just been accelerated to the present because of everything that's happening to us. But this is the future, and it's not -- there's no going back, and we just found everything accelerated. Yes. We push architecture really hard. And the way we push architecture is we start from the system, but we believe that the future computer company is a data center-scale company. The computing unit is no longer a microprocessor or even a server or even a cluster. The computing unit is an entire data center now. And as I was explaining to Vivek just now that a microservice that we're enjoying hundreds of billions of transactions a day, those are broken up into a whole bunch of microservices that are running across the entire data center. And so the data center is running -- the entire data center is running an application. I mean that's pretty remarkable thing, and that's happened in the last several years. We were ahead of this trend, and we recognized that as a computing company, we have to be a data center-scale company, and we architect from that starting. If you look at our architecture, we were the first in the world to create the concept of an NVLink with 8 processors being fully synchronized across a computing node, and we created the DGX. We recognize the importance of high-speed networking and low-latency networking, and that's why we bought Mellanox. And the amount of software that we invented along the way to make it possible for low-latency communications, whether it's GPUDirect or recently the invention of GPUDirect Storage, all of that technology was inspired by the idea that you have to think about the data center all-in-one holistic way. And then in the last -- in this current generation with Ampere, we invented the world's first multi-instance GPU. We invented the world's first multi-instance GPU, which means that our Ampere GPU could simultaneously be 1 GPU or, with NVlink, 8 GPUs combined, working together, so you could think of them as being tiled. So those 8 GPUs are working harmoniously together. Or each one of the GPUs could fractionalize itself, if you don't need that much GPU working on your workload, fractionalize into a multi-GPU instance, we call the MIG, a little tiny instance. And each one of those tiny instances are more powerful and more performant than our entire Volta GPU lap time. And so whether you like to fractionalize the GPU, which happens oftentimes; create a larger GPU using NVLink; or create an even larger GPU, the size of DGX POD, connected together with high-speed, low-latency networking with Mellanox, we could architect it any way you'd like. You made a comment about -- you asked a question about ARM. We've been a long-term partner of ARM, and we use ARM in a whole bunch of applications. And whether it's autonomous driving or a robotics application, the Nintendo Switch, console business that we're in. And then recently, we brought CUDA to ARM and to bring accelerated computing to ARM. And so we worked with the ARM team very closely. They're really great guys. And one of the specials about the ARM architecture that you know very well is that it's incredibly energy-efficient. And because it's energy-efficient, it has the headroom to scale into very high-performance levels over time. And so anyways, we love working with the ARM guys. Yes. Thanks a lot, Aaron. We are -- we're still in the ramping of the RTX generation. Turing, Turing the current generation that we're in, is the world's first ray tracing GPU. And it fuses -- the RTX technology fuses 3 fundamental technologies: the programmable shader that we introduced a long time ago that revolutionized computer graphics; and we added 2 new technologies. One technology is a ray tracing acceleration core that makes the tracing of rays and looking for intersections between the ray and the scene -- objects in the scene super, super fast. And that -- it's a complicated problem. It's a super-complicated problem. We want it to be running concurrently to shading so that the ray traversal and the shading of the pixels could be done independently and concurrently. The second thing is we invented this technology to bring AI, artificial intelligence, using this new type of algorithm called deep learning to computer graphics. And one example of its capability is the algorithm we introduced called DLSS, Deep Learning Super Sampling, which allows us to essentially synthesize by learning from previous examples, essentially learning from previous examples of images and remembering it, remembering what beautiful images look like so that when you take a low-resolution image, and you run it through the deep neural network, it synthesizes a high-resolution image that's really, really beautiful. And people have commented that it's even more beautiful than native rendered images at the native resolution. And the benefit is not only is it beautiful, it's also super fast. We essentially nearly doubled the performance of RTX as a result of doing that. So you can have the benefit of ray tracing as well as very high resolution and very high speed. And so that's called RTX. And Turing is probably not even close, not even 1/3 of the total installed base of all of our GeForce GPUS, which is, as you know, the single-largest installed base of gaming platforms in the world. And so we support this large installed base, and we're in the process of bringing them to the future with RTX. And now with the new console generation coming, every single game developer on the planet is going to be doing ray tracing, and they're going to be creating much, much richer content. And because of multi-platform, cross-platform play and because of the size of the gaming platform, PC gaming platform, it's really important that these game developers bring the latest generation content to PCs, which is great for us. Yes. Let me see if I can answer this one for you. Yes, we have been talking about our visibility of data center. And as you've seen in our Q2 results, you can see that our overall adoption of the NVIDIA computing portfolio has accelerated quite nicely. But keep in mind, we're still really early in the product cycle. A100 is ramping. It's ramping very strong into our existing installed bases but also into new markets. Right now, A100 probably represents less than 1/4 of our data center revenues. So we still have a lot to grow. We have good visibility looking into Q3 with our hyperscales. We have a little bit more of a mixed outlook in terms of our vertical industries, given a lot of the uncertainty in the market and in terms of the overall economy. On-premises are challenged because of the overall COVID. But remember, industries are quickly and continuing to adopt and move to the overall cloud. But overall, we do expect a very strong Q3. Yes. Thanks so much, C.J. We're expecting a really strong second half for gaming. I think this may very well be one of the best gaming seasons ever. And the reason for that is because PC gaming has become such a large format. The combination of amazing games like Fortnite and Minecraft and because of the way people game now, their gaming and their e-sporting, even F1 is an e-sport now. They're hanging out with friends. They're using it to create other content. They're using game captures, create art. They're sharing it with the community. It's a broadcast medium. The number of different ways you could game has just really, really exploded. And it works on PCs because all the things that I described require cameras or keyboards or streaming systems or -- and -- but it requires an open system that is multitasking. And so the PC has just become such a large platform for gaming. And the second thing is that RTX, it's a home run. We really raised the bar with computer graphics, and the games are so beautiful, and it's really, really the next level. It's not been this amazing since we introduced programmable shaders about 15 years ago. And so for the last 15 years, we've been making programmable shaders better and better and better, and it has been getting better. But there's never been a giant leap like this, and RTX brought both artificial intelligence as well as ray tracing to PC gaming. And then the third factor is the console launch. There's -- people are really -- the game developers are really gearing up for a big leap. And because of the gaming -- because how vibrant the gaming market is right now and how many people around the world is depending on gaming at home. I think it's going to be the most amazing season ever. We're already seeing amazing numbers from our console partner, Nintendo. Switch has -- about to sell more than Super Nintendo, more than all the Famicom, which was one of the best gaming consoles of all time. I mean they're underway to make Switch the most successful gaming platform of all time. And so I'm super excited for them. And so I think it's going to be quite a huge second half of the year. Colette, I felt like I didn't -- I missed C.J.'s second question. Can we jump on and answer it? I think the question was regarding our inventory purchases on that piece. Is that the part that you're referring to? Yes. That's the one. Yes. Yes. Keep in mind, C.J., that when you think about the complexity of the products that we are building, we have extremely long lead times, both in terms of what we produce for the data center, our full systems that we need to do as well as what you are seeing now between the sequential growth between Q2 and Q3 for overall gaming. So all of that is in preparation for the second half. Nothing unusual about it other than, yes, we've got to hit those revenue numbers that are in our Q3 guidance. Yes. Thank you. So thanks for the question. The -- our data center trend is really tied to a few factors. One is the proliferation of using deep learning and artificial intelligence and all the services that are in -- by the cloud service providers. And I think it's fair to say that over the last several years, the number of breakthroughs in artificial intelligence has been really terrific. And we're seeing anywhere from 10x, 10x more computational requirement each year to more than that. And so in the last 3 years, we've seen somewhere between 1,000 to 3,000x increase in the size of models, the computational requirement necessary to create these AI models and to deploy these AI models. And so the #1 trend that we're probably indexed to is the breakthroughs of AI and the usefulness of AI and how people are using it. And one of the -- and I remember C.J.'s question now, and I'll answer this along with that. One of the things that we look for and you should look for is how -- what kind of breakthroughs are based on deep learning and based on AI that these services all demand. And there are 3 big ones, just gigantic one. Of course, one of them is natural language understanding, the ability to take a very complicated text and use deep learning to create essentially a dimension reduction, it's called deep embedding, dimension reduction on that body of text so that you could use that vector as a way to teach a recommender system, which is the second major breakthrough, the recommender system, how to predict and make a recommendation to somebody. Recommendation on ads and videos, and there are trillions of videos on the web. You need ways to recommend them, both the news and just the amount of information that is going to -- that is in true dynamic form require these recommenders to be instantaneous. And so the first one is natural language understanding. The second one is the recommender system, gigantic breakthroughs in the last several years. And the third is conversational AI. I mean we're going to have conversational engines that are just super clever, and they can predict what you're about to ask. They're going to predict the right answer for you, make recommendations to you based on the 3 pillars that I just described. And I haven't even started talking about robotics, the breakthroughs that are happening there with all the factories that need to automate and breakthroughs that we're seeing in self-driving cars, the models there are really improving fast. And so the answer to you, Toshiya, and C.J. are kind of similar, that on the first one, we're indexed to AI. The second, we're indexed to breakthroughs of AI. So that it can continue to consume more and more capability and more technology. And then the third thing that we're indexed to is the movement of workloads to the cloud. It is now possible to do rendering in the cloud, remote graphics workstations in the cloud. And NVIDIA virtual workstations is in every single cloud. You could do big data analytics in the cloud. And these applications, I've just given you a few applications where you can do scientific computing in the cloud. These applications all have fundamentally different computing architectures. NVIDIA is the only accelerated architecture that allows you to do microservices for conversational AI and other types of AI applications to scale up applications like high-performance computing, training, big data analytics to virtualize applications like workstations. Our platform is universal, and these 3 facets that I just described are supremely complex, virtualized, microservices-based and scale-up-based. And so these -- bare metal scale-up. And these are complicated, and it's one of the reasons why we bought Mellanox because they're at the core and at the intersection of all of that. The storage, the networking, the security, the virtualization, they're at the intersection of all of that. And I just described 3 dynamics that are very, very powerful and are at the early stages yet. And so those are the things that we're really indexed to. And then lastly, when somebody adopts -- when we introduce a new platform like Ampere, we're in the beginning of a multiyear product cycle, Ampere is such a gigantic breakthrough. It's the first universal GPU we ever created. It is both able to scale up as well as scale out, scale up as in multi GPUs, scale out is fractionalization, multi-instance GPUs. And it's -- it reduced -- it saves money, tremendous amount of money for people who use it. It speeds up their application. It reduces their TCO. Their TCO value just goes through the roof. And so we're in the beginning of this multiyear cycle and the enthusiasm has been fantastic. This is the fastest ramp we've ever had. And so we're going to keep on racing through the second half. Okay. And Toshiya, you asked a question regarding our guidance going forward regarding gross margin. And within our Q3 guidance, we have just a small decline in our gross margin from Q2. Most of that is really associated with mix but also a little bit in terms of the ramping of our new Ampere architecture products that we have. So keep in mind, our data center will likely be a lower percentage of total revenue, given the strong overall gaming growth that we expect between Q2 and Q3. Within that gaming growth, keep in mind, consoles are also included, which will continue to be below our company totals average gross margin, and that is expected to be up strongly quarter-over-quarter for our overall console shipments. We're going to be ramping those new architectures over time when we have the ability to expand our gross margin as Ampere GPUs mature, too. Yes. Stacy, so thanks for the question. Let me first start on our Q3 outlook and what we're seeing. And when we think about our demand and our supply, we're very comfortable with the supply that we have. Keep in mind, our products are quite complex, and a lot of our time is spent in terms of procuring every aspect of that supply over multiple quarters previously. So that's how we work. But we are very confident with the overall supply that we have across the board in data center. Keep in mind that it's not just A100. We are continuing to sell V100 or T4. And we're also bringing new versions of the A100 coming to overall market. So I hope that helps you understand our statements on where are we at in terms of the Q3 guidance. We'll see if Jensen wants to add a little bit more to that. Well, when we're ramping, we sure love to have more and sooner. And -- but this is our plan, and we're executing to the plan. It is a very complicated product, as Colette mentioned. It is the most complicated. Yes. So in terms of moving from Q2 to Q3, we believe that most of the actual growth that we will receive in that single -- low single-digit to mid-single-digit growth will likely stem from NVIDIA compute, will be the largest driver of that. I wonder if I could ask a longer-term question about the -- how you guys see the importance of process technology. There's been a lot of discussion around that in the CPU domain. But you guys haven't really felt the need to be first on 7 nanometer, and you've done very well. Just how important do you think it is to be early in the new process node? And how does that factor into the cycle of innovation at NVIDIA? Yes. First of all, thanks, Joe. The process technology is a lot more complex than a number. I think people have simplified it down to almost a ridiculous level, right? And so process technology, we have a really awesome process engineering team, world-class. Everybody will recognize that it's absolutely world-class. And we work with the foundries, we work with TSMC really closely, to make sure that we engineer transistors that are ideal for us and we engineer metallization systems that's ideal for us. It's a complicated thing, and we do it at high part. Then the second part of it is where architecture, where the process technology and the rest of the design process, the architecture of the chip, and the final analysis, what NVIDIA paid for, is architecture, not procurement of transistors. We're paid for architecture. And there's a vast difference between our architecture and the second best architecture and the rest of the architectures. The difference is incredible. We are easily twice the energy efficiency all the time, irrespective of the number of the -- in the transistor side. And so it must be more complicated than that. And so we put a lot of energy into that, and then the last thing I would say is that going forward, it's really about data center-scale computing. Going forward, you optimize at the data center scale. And the reason why I know this for a fact is because if you're a software engineer, you would be sitting at home right now, and you will write a piece of software that runs on the entire data center in the cloud. You have no idea what's underneath it, nor do you care. And so what you really want is to make sure that, that data center is as high throughput as possible. There are a lot of code in there. And so what NVIDIA has decided to do over the years is to take our game to a new level. Of course, we start with building the world's best processors, and we use the world's best foundries, and we partnered them very closely to engineer the best process for us. We partner with the best packaging companies to create the world's best packaging. We're the world's first user of cobots. And whether it's -- I think we're -- I'm pretty sure we're still the highest volume by far of 2.5D and 3D packaging. And so we start from a great chip. We start from a great chip, but we don't end there. That's just the beginning for us. Now we take this thing all the way through systems, the system software, algorithms, networking, all the way up to the entire data center. And the difference is absolutely shocking. We built our data center, Selene, and it took us 4 weeks. We put up Selene in 4 weeks' time. It is the seventh fastest supercomputer in the world, one of the fastest AI supercomputers in the world. It's the most energy-efficient supercomputer in the world, and it broke every single record in MLPerf, and that kind of shows you something about the scale that we work and the complexity of the work that we do. This is the future. It's for -- the future is about data centers. Thank you. The accelerated computing model we pioneered has clearly passed the tipping point. Adopting of NVIDIA computing is accelerating. On this foundation and leveraging one architecture, we have transformed our company in 3 dimensions. First, NVIDIA is a full stack computing platform company, offering the world's most dynamic industries, the chips systems, software and libraries like NVIDIA AI to tackle their most pressing challenges. NVIDIA -- second, NVIDIA is a data center-scale company with capabilities to architect, build and operate the most advanced data centers. The data center is the new computing unit. With this capability, we can create modern data center architectures that are computer maker partners, and then scale out to the world's industry. Third, NVIDIA is a software-defined company today, with rich software content like GeForce NOW, NVIDIA virtual workstation in the cloud, NVIDIA AI and NVIDIA Drive that will add recurring software revenue to our business model. In the coming years, AI will revolutionize software. Robotics will automate machines, and the virtual and physical worlds will become increasingly integrated through VR and AR. Industry advancements will accelerate, and NVIDIA accelerated computing will play an important role. Our next GTC will be coming on October 5, again from my kitchen. Join me. I have some exciting developments to share with you. Thanks, everyone.","Jensen, I guess I had a question on both architecture and also manufacturing. And I think on the manufacturing side, you've been radical now for some time. And when you've been asked in the past about moving to more of a tiled or chiplet approach, you sort of made light of that. But the CPU guys are clearly taking that approach. So I guess the question is, why do you think you won't have to make a similar move? And then on the side of architecture, the theme of Hot Chips this week was really how compute demand is far outstripping computing power? And then we see this talk about you and ARM. So I guess can you talk about whether GPU is the future and maybe the broader opportunity to integrate CPU and GPU? Congratulations on the quarter. Just building on some prior questions. The first one, I was just curious if you could help us appreciate kind of the installed base of the gaming GPU business relative to where we're at the Turing upgrade cycle. What do we see still on prior generations, be it Pascal or before? And then secondly, I was wondering, Colette, could you just give me a kind of updated commentary or views on visibility in the data center business? How has that changed over the last 3 months? What does that look like as you look through the back half of the calendar year? And then on the data center visibility? I guess 2 questions. If I look at your outstanding inventory purchase obligations, grew, I think, 17% sequentially. Is that as you prepare for the September 1 launch? And can you kind of comment on gaming visibility into the back half of the year? And then the second question, Jensen, I know you're very focused on platforms and driving recurring revenues. Would love to hear if there's any particular platforms over the last 3 months where you've made real headway or get you excited, whether Jarvis, Merlin, Spark or whatever. I had one for Jensen and another one for Colette. Jensen, just following up on the data center business. As you probably know, quite a few of your peers have been talking about potential digestion of capacity on the part of your hyperscale customers over the next, call it, 6 to 12 months. Curious, is that something that you think about, worry about in your data center business? Or do you have enough idiosyncratic growth drivers like the A100 ramp? And I guess the breadth that you've built within your data center business across compute and networking, are those enough for you to buck the trend within data center over the next 6 to 12 months? And then the second one for Colette, just on gross margins. You're guiding October quarter gross margins down 50 basis points sequentially. Based on the color that you provided for the individual segments, it looks like mix remains pretty positive. So just curious what's driving the marginal decline in gross margins in the October quarter? I wanted to dig into data center a little bit. This is a question for Colette. So in the quarter, ex Mellanox, data center was up, core data center, maybe 6%, 7%. The guide looks to be roughly similar to that into Q3. Can you talk to us a little bit about what's driving the trajectory? Are you more demand or more supply limited at this point? What does your supply situation look like? And what are the lead times especially on the A100 products for data center look like at this point? Like if you have more capacity available, do you think you'd have like a stronger trajectory than you have right now? Got it. Got it. And just a quick follow-up. Within the data center guidance, how do you think about like the core data center sequential growth versus Mellanox? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. Refinitiv reserves the right to make changes to documents, content, or other information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES REFINITIV OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q4 2020,2312,4763,1231,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's Conference Call for the Fourth Quarter of Fiscal 2020. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2021. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, February 13, 2020, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Thanks, Simona. Q4 revenue was $3.11 billion, up 41% year-on-year and up 3% sequentially, well above our outlook, reflecting upside in our data center and gaming businesses. Full year revenue was $10.9 billion, down 7%. We recovered from the excess channel inventory in gaming and an earlier pause in hyperscale spending and exited the year with great momentum. Starting with gaming. Revenue of $1.49 billion was up 56% year-on-year and down 10% sequentially. Full year gaming revenue was $5.52 billion, down 12% from our prior year. We enjoyed strong end demand for our desktop and notebook GPUs. Let me give you some more details. Our gaming lineup was exceptionally well positioned for the holidays with the unique ray tracing capabilities of our RTX GPUs and incredible performance at every price point. From the Singles Day shopping event in China through the Christmas season in the West, channel demand was strong for our entire stack. Fueling this were new blockbuster games like Call of Duty: Modern Warfare, continued eSports momentum and new RTX Super products. With RTX price points as low as $299, ray tracing is now the sweet spot for PC gamers. Gaming is thriving and gamers prefer GeForce. The global phenomenon of eSports keeps gaming momentum with an audience now exceeding 440 million, up over 30% in just 2 years according to Newzoo. The League of Legends World Championship brought more than 100 million viewers, on par with this month's Super Bowl. Ray tracing titles continue to come to market, and GeForce RTX GPUs are the only ones that support this important technology. This quarter, Wolfenstein: Young blood and Deliver Us The Moon were the latest titles to support ray tracing as well as NVIDIA's Deep Learning Super Sampling technique, which also uses AI to boost performance. With the proliferation of RTX-enabled games and our best ever top-to-bottom performance, we are solidly into the Turing architecture upgrade cycle. Gamers continue to move to higher-end GPUs, seeking better performance and support for ray tracing. Gaming laptops posted double-digit year-on-year growth for the eighth consecutive quarter. The category continues to expand, driven by appealing thin and light form factors with fantastic graphics performance. This holiday season, retailers stocked a record 125 gaming laptops based on NVIDIA GPUs, up from 94 last year, with our Max-Q designs up 2x. At CES, we launched the world's first 14-inch GeForce RTX laptop with ASUS. We also continue to expand our Studio lineup of laptops for the fast-growing population of freelance creators, designers and YouTubers with 13 new RTX Studio systems introduced at CES. Powered by Turing GPUs, these systems are optimized for over 55 creative and design applications with RTX accelerated ray tracing and/or AI. Last week, we launched our GeForce NOW cloud gaming service. Powered by GeForce, GeForce NOW is the first cloud gaming service to deliver ray trace games. It's also the only open platform so gamers can enjoy the games they already have and use their existing store accounts without having to repurchase games. GeForce NOW enables PC games on Macs, Windows, PCs, TVs, Mobile devices and soon, Chromebooks. GFN has a freemium business model that includes 2 membership plans: a free membership with standard access; and a Founders tier with a starting price of $4.99 per month, which gives priority access and RTX ray tracing support. Our goal with GeForce NOW is to expand GeForce gaming to more gamers. About 80% of GeForce NOW gamers are playing on underpowered PCs or devices with Mac OS or Android. With GeForce NOW, they are able to enjoy PC gaming on a GeForce GPU in the cloud. GeForce now can expand GeForce well beyond the roughly 200 million gamers we reach today. Separately, we entered into a collaboration with Tencent, the world's largest gaming platform, to bring PC gaming in the cloud to China, the world's largest gaming market. NVIDIA GPU technology will power Tencent's Start cloud gaming service, which is in early testing stages. Moving to data center. Revenue was a record $968 million, up 43% year-on-year and up 33% sequentially, our strongest ever sequential growth in dollar terms. Full year fiscal year '20 data center revenue was a record $2.98 billion, up 2% from the prior year. Strong growth was fueled by hyperscale and vertical industry end customers. Hyperscale demand was driven by purchases of both our training and inference products in support of key AI workloads, such as natural language understanding, conversational AI and deep recommendators. Hyperscale demand was also driven by cloud computing. AWS now makes the T4 available in every region. This underscores the versatility of the T4, which excels at a wide array of high-performance computing workloads, including AI inference, cloud gaming, rendering and virtual desktop. Vertical industry growth was driven primarily by consumer Internet companies. Other verticals such as retail, health care and logistics continue to grow from early-stage build-outs with a strong foundation of deep learning engagements, and we see an expanding set of opportunities across high-performance computing, data science and edge computing applications. T4, our inference platform, had another strong quarter, with shipments up 4x year-on-year, driven by public cloud deployments as well as edge AI video analytics applications. T4 and V100, reflecting strong demand for inference and training, respectfully, set records this quarter for both shipments and revenue. Even as NVIDIA remains the leading platform for AI model training, NVIDIA's inference platform is getting wide use by some of the world's leading enterprise and consumer Internet companies, including American Express, Microsoft, PayPal, Pinterest, Snap and Twitter. The industry continues to do groundbreaking AI work for NVIDIA. For example, Microsoft's biggest quality improvements made over the past year in its Bing search engine stem from its use of NVIDIA GPUs and software for training and inference of its natural language understanding models. These DNN transformer models popularized by BERT have computational requirements for training that are in the order of magnitude higher than earlier image-based models. Conversational AI is a major new workload, requiring GPUs for inference to achieve high throughput within the desired low latency. Indeed, Microsoft cited an inference throughput increase of up to 800x on NVIDIA GPUs compared with CPUs, enabling it to serve over 1 million BERT inferences per second worldwide. And just this week, Microsoft researchers announced a new breakthrough in natural language processing with the largest ever publicized model trained on NVIDIA DGX-2. This advances the state of the art for AI assistance in tasks, such as answering questions, summarization and natural language generation. Recommendators are also an important machine learning model for the Internet, powering billions of queries per second. The industry is moving to deep recommendators such as wide and deep model, which leverage deep learning to enable automatic feature learning and to support unstructured content. Running these models on GPUs can dramatically increase inference throughput and reduce latency compared with CPUs. For example, Alibaba's and Baidu's recommendation engines run on NVIDIA AI, boosting their inference throughput by orders of magnitudes beyond CPUs. Deep recommendators enabled Alibaba to achieve 10% increase in click-through rates. We also announced the availability of a new GPU-accelerated supercomputer on Microsoft Azure. It enables customers for the first time to rent an entire AI supercomputer on demand from their desk, matching the capabilities of large on-premise supercomputers that can take months to deploy. And in Europe, energy company Eni announced the world's fastest industrial supercomputer based on NVIDIA GPUs. AI has even come to pizza delivery. At the National Retail Federation's Annual Conference last month, we announced Domino's as a customer deploying our platform for deep learning and data science applications, helping with customer engagement and order accuracy prediction. More broadly in retail, we have seen a significant increase in the adoption of NVIDIA's edge computing offerings by large retailers for powering AI applications that reduce shrinkage, optimize logistics and create operational efficiencies. At the SC19 Supercomputing conference, we introduced a reference design platform for GPU-accelerated ARM-based servers, along with ecosystem partners, ARM, Ampere Computing, Fujitsu and Marvell. We made available our ARM-compatible software development kit consisting of NVIDIA CUDA-X libraries and development tools for accelerating computing. This opens the floodgates of innovation to support growing new applications from hyperscale cloud to Exascale supercomputing. We also introduced NVIDIA Magnum IO, a suite of software optimized to eliminate storage and input/output bottlenecks. Magnum IO delivers up to 20x faster data processing for multi-server, multi-GPU computing nodes when working with massive data sets to carry out complex financial analysis, climate modeling and other workloads for data scientists, high-performance computing and AI researchers. Finally, we introduced TensorRT 7, the seventh generation of our inference software development kit, which speeds up components of conversational AI by 10x comparing to running on CPUs. This helps drive latency below the 300 millisecond threshold considered necessary for real-time interactions supporting our growth in conversational AI. Moving to ProVis. Revenue reached a record $331 million, up 13% year-on-year and up 2% sequentially. Full year revenue was a record $1.21 billion, an increase of 7% from the prior year. ProVis accelerated in Q4 as the rollout of more RTX-enabled applications is driving strong upgrade cycle for our Turing GPUs. RTX is also opening up new market segment opportunities, such as rendering and studio for freelance creatives. In November, V-ray, Arnold and Blender software renderers began shipping with RTX technology. These joined our leading creative and design applications, including Premier Pro, Dimension, SOLIDWORKS, CATIA and Maya. With RTX, these applications enable enhanced creativity and notable productivity gains. In Blender Cycles, for example, real-time rendering performance is boosted 4x versus a CPU. RTX is now supported by more than 40 leading creative and design applications, reaching a combined user base of over 40 million. Finally, turning to automotive. Revenue was $163 million, flat from a year ago and up 1% sequentially. Full year revenue reached a record $700 million, up 9% year-on-year. During the quarter, we announced DRIVE AGX Orin, the next-generation platform for autonomous vehicles and robots, powered by our new Orin SoC and delivering nearly 7x the performance of the previous generation Xavier SoC. The platform scales from level 2 plus AI-assisted driving up to level 5 fully driverless operation. Orin is software-defined and compatible with Xavier, allowing developers to leverage their investment across multiple product generations. Moving to the rest of the P&L. Q4 GAAP gross margins was 64.9% and non-GAAP was 65.4%, up sequentially, largely reflecting a higher contribution of data center products. Q4 GAAP operating expenses were $1.02 billion and non-GAAP operating expenses were $810 million, up 12% and 7% year-on-year, respectively. Q4 GAAP EPS was $1.53, up 66% from a year earlier. Non-GAAP EPS was $1.89, up 136% from a year ago. Q4 cash from operations was $1.46 billion. Fiscal year '20 cash flow from operations was a record $4.76 billion. With that, let me turn the outlook for the first quarter of fiscal 2021. The outlook does not include any contribution from the pending acquisition of Mellanox. We are engaged and progressing with China on the regulatory approval and believe the acquisition will likely close in the first part of calendar 2020. Before we get to the new -- the numbers, let me comment on the impact of the coronavirus. While it is still early and the ultimate effect is difficult to estimate, we have reduced our Q1 revenue outlook by $100 million to account for the potential impact. We expect revenue to be $3 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be 65% and 65.4%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $1.05 billion and $835 million, respectively. GAAP and non-GAAP OI&E are both expected to be income of approximately $25 million. GAAP and non-GAAP tax rates are both expected to be 9%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $150 million to $170 million. Further financial details are included in the CFO commentary and other information available on the IR website. In closing, let me highlight an upcoming event for the financial community. We will be at the Morgan Stanley Technology, Media and Telecom Conference on March 2 in San Francisco. With that, we will now open the call for questions. Operator, will you please poll for questions.","Yes. Toshiya, thanks a lot for your question. The primary driver for our growth is AI. There are 4 fundamental dynamics. The first is that the AI models that are being created are achieving breakthroughs and quite amazing breakthroughs, in fact, in natural language understanding, in conversational AI, in recommendation systems. And you know this, but for the others in the audience, recommendation systems are essentially the engine of the Internet today. And the reason for that is because there are so many items in the world, whether it's a store or whether it's content or websites or information you are querying, there are hundreds of billions, trillions, and depending on how you count it, hundreds of trillions of items in the world. And there are billions of people, each with their own characteristics and their countless contexts. And between the items, the people, the users and the various contexts that we're in, location and what you're looking for and weather or what's happening in the environment, those kind of contexts affects the search query that -- the answer they provide you. The recommendation system is just foundational now to search. And some people have said this is the end of search and the beginning  -- and the era of recommendation systems. Work is being done everywhere around the world in advancing recommendation systems. And very first time over the last year, it's been able to be done in deep learning. And so the first thing is just the breakthroughs in AI. The second is production AI, which means that whereas we had significant and we continue to have significant opportunities in training because the models are getting larger, and there are more of them, we're seeing a lot of these models going into production, and that business is called inference. Inference, as Colette mentioned, grew 4x year-over-year. It's a substantial part of our business now. But one of the interesting statistics is TensorRT 7, the entire TensorRT download this year was about 500,000, a doubling over a year ago. What most people don't understand about inference is it's an incredibly complex computational problem, but it's an enormously complex software problem. And so the second dynamic is moving from training or growing from training and models going into production called inference. The third is the growth, not just in hyperscale anymore, but in public cloud and in vertical industries. Public cloud because of thousands of AI start-ups that are now developing AI software in the cloud. And the OpEx model works much better for them as they're younger. When they become larger, they could decide to build their own data center infrastructure on-prem, but the thousands of start-ups start their lives in the cloud. We're also seeing really great success in verticals. One of the most exciting vertical is logistics. Logistics, retail, warehousing. We announced, I think, this quarter or last -- end of last quarter, USPS, American Express, Walmart, just large companies who have enormous amounts of data that they're trying to do data analytics on and do predictive analytics on. And so the third dynamic is the growth in -- beyond hyperscale and public cloud as well as vertical industries. And then the last dynamic is being talked about a lot, and this is really, really exciting, and it's called edge AI. We used to call it industries and AI where the action is. But the industry now calls edge AI. We're seeing a lot of excitement there. And the reason for that is you need to have low latency inference. You might not be able to stream the data all the way to the cloud for cost reasons or data sovereignty reasons, and you need the response time. And so those 4 dynamics around AI really drove our growth. Great. Just following up on that. As you look back at the last 12 months and the deceleration that you saw in your HPC cloud business, now that you have the perspective of seeing what's driving the rebound, any thoughts on what drove it to slow down in the first place? Was it just digestion? Was it sort of a handoff from image recognition to these newer applications that you just talked about? Just help us -- what happened there? And I guess as it pertains to the future, do we think of this as a business that will have that kind of lumpiness to it? Yes. That's a really good question. In fact, if you look backwards, now we only have the benefit of history. The deep recommendation systems, the natural language understanding breakthroughs, the conversational AI breakthroughs, all happened in this last year. And the velocity by which the industry captured the benefits here and continue to evolve and advance from these what so-called transformer models was really quite incredible. And so all of a sudden, the number of breakthroughs in AI has just grown tremendously, and these models have grown tremendously. Just this last week, Microsoft announced that they've trained a neural net model in collaboration with work that we did, we call Megatron, increased the size of a model from 7.5 billion parameters to 17.5 billion parameters. And the accuracy of their natural language understanding has just -- has really been boosted. And so the models are -- AI is finding really fantastic breakthroughs, and models are getting bigger and there are more of them. And when you look back and look at when these breakthroughs happened, it essentially happened this last year. The second, we've been working on inference for some time. And until this last year, very few of those inference models went into production. And now we have deep learning models across all of the hyperscalers in production. And this last year, we saw really great growth in inference. The third dynamic is public clouds. All these AI startups that are being started all over the world, there's about 6,000 of them, they're starting to develop and be able to put their models into production. And with the scale out of AWS, we now have T4s in every single geography. So the combination of the availability of our GPUs in the cloud, and the startups and vertical industries deploying their AI models into production, the combination of all that just kind of came together. And all of that happened this last year. And as a result, we had record sales of V100s and T4s. And so we're quite excited with the developments, and it's all really powered by AI. Congratulations on returning the business back to the strong growth. Jensen, I wanted to ask about how you are positioned from a supply perspective for this coming year? Your main foundry is running pretty tight. How will you be able to support the 20% or so growth here that many investors are looking for? If you could just give us some commentary on how you're positioned from a supply perspective, that will be very helpful. Well, I think we're in pretty good shape on supply. We surely won't have ample supply. It is true that the industry is tight and the combination of supporting multiple processes, multiple fabs across our partner, TSMC. We've got a lot of different factories and a lot of different -- several different nodes of process qualified. I think we're in good shape. And so we just have to watch it closely. And we're working very closely with all of our customers in forecasting. And of course, that gives us better visibility as well and -- but all of us have to do a better job forecasting, and we're working very closely between our customers and our foundry partners, TSMC. Yes. Tim, thanks for the question. Similar to what we had seen last quarter, with all things growing as we moved into this quarter, growth in terms of the hyperscales, continued expansion in terms of those vertical industries and even in the cloud instances. We're still looking at around the same split of 50-50 between our hyperscales and our vertical industries and maybe a little bit tad below 50 in terms of our total overall hyperscales. Thanks, Aaron. Colette, do you want to go first? Sure. When we think about going into Q1 and our data center overall growth, we do expect to see continued growth, both going into Q1. We believe our visibility still remains positive quite well, and we're expecting that as we move into it and go forward. Yes. Aaron, I believe that every query on the Internet will be accelerated someday. And at the very core of it, most -- almost all queries will have some natural language understanding component to it. Almost all queries will have to sort through and make a recommendation from the trillions of possibilities, filter it down and recommend a handful of recommended answers to your queries. Whether it's shopping or movies or just asking locations or even asking a question, the number of the possibilities of all the answers versus what is best answer is -- needs to be filtered down. And that filtering process is called recommendation. That recommendation system is really complex, and deep learning is going to be involved in all that. That's the first thing. I believe that every query will be accelerated. The second is, as you know, CPU scaling has really slowed, and there's just no two ways about it. It's not a marketing thing. It's a physics thing. And the ability for CPUs to continue to scale without increasing cost or increasing power has ended. And it's called the end of Dennard scaling. And so there has to be another approach. The combination of the emergence of deep learning and the use of artificial intelligence and the amount of computation that's necessary to -- for every single query but the benefit that comes along with that, and the end of Dennard scaling, suggests that there needs to be another approach, and we believe that approach is acceleration. Now our approach for acceleration is fundamentally different than an accelerator. Notice, we never say accelerator, we say accelerated computing. And the reason for that is because we believe that a software-defined data center will have all kinds of different AIs. The AIs will continue to evolve, the models will continue to evolve and get larger, and a software-defined data center needs to be programmable. It is one of the reasons why we've been so successful. And if you go back and think about all the questions that have been asked of me over the last 3 or 4 years around this area, the consistency of the answer has to do with the programmability of architecture, the richness of the software, the difficulties of the compilers, the ever-growing size of the models, the diversity of the models and the advances that these models are creating. And so we're seeing the beginning of a new computing era. And a fixed function accelerator is simply not the right answer. And so we believe that the future is going to be accelerated. It's going to require an accelerated computing platform, and software richness is really vital, so that these data centers could be software defined. And so I think that we're in the early innings, the early innings, very, very early innings of this new future. And I think that accelerated computing is going to become more and more important. Sure. Thanks for the question, Matt. So it's really still quite early in terms of trying to figure out what the impact from the overall coronavirus may be. So we're not necessarily precise in terms of our estimate. Yes, our estimates are split between an impact possibly on gaming and data center and split pretty much equally. The $100 million also reflects what may be supply challenges or may be overall demand. But we're still looking at those to get a better understanding where we think that might be. In terms of our business and our business makeup, yes, our overall China business for gaming is an important piece. We have about 30% of our overall China gaming as a percentage of our overall gaming business. For data center, it's -- it moves quite a bit. They are a very important market for us, but it moves from quarter-to-quarter just based on the overall end customer mix as well as the system builds that they may choose. So it's a little harder to determine. Let's see. Tencent is the world's largest publisher. China represents about 1/3 of the world's gaming, and transitioning to the cloud is going to be a long-term journey. And the reason for that is because Internet connection is not consistent throughout the entire market. And a lot of application still needs to be onboarded, and we're working very closely with them. We're super enthusiastic about it. If we're successful long term, and we're talking about an extra 1 billion gamers that we might be able to reach. And so I think that this is an exciting opportunity, just a long-term journey. Now here in the West, we've had a lot more opportunity to refine the connections around the world and working through the data centers, the local hubs as well as people's WiFi routers at home. And so we've been in beta for quite some time, as you know. And here in the West, our platform is open. And we have several hundred games now and we're in the process of onboarding another 1,500 games. We're the only cloud platform that's based on Windows and allows us to be able to bring PC games to the cloud. And so the reach is -- we've had more experience here in the West with reach, and we've had -- we obviously have a lot more games that we can onboard. But I'm super enthusiastic about the partnership we have with Tencent. Overall, our GeForce NOW -- you guys saw the launch, it's -- the reception has been fantastic, the reviews have been fantastic. Our strategy has 3 components. There's the GeForce NOW service that we provide ourselves. We also have GeForce NOW alliances with telcos around the world to reach the regions around the world that we don't have a presence in. And that is going super well, and I'm excited about that. And then lastly, partnerships with large publishers, for example, like Tencent. And we offer them our platform, of course, and a great deal of software and just a lot of engineering that has to be done in collaboration to refine the service. Yes. So C.J., I'm going to go first, and then Colette is going take it home here. So the first part of it is this, our gaming business has at the end -- I'm sorry. Okay. Our gaming business, the end market demand is really terrific. It's really healthy. It's been healthy throughout the whole year. And it's pretty clear that RTX is doing fantastic. And it's very -- it's super clear now that ray tracing is the most important new feature of next-generation graphics. We have 30 -- over 30 games that have been announced, 11 games or so that have been shipped. The pipeline of ray tracing games that are going to be coming out is just really, really exciting. The second factor -- and one more thing about RTX, we finally have taken RTX down to $299. So it's now at the sweet spot of gaming. And so RTX is doing fantastic. The sell-through is fantastic all over the world. The second part of our business that is changing in gaming is this -- the amount of notebook sales and the success of Nintendo Switch has really changed the profile of our overall gaming business. Our notebook business, as Colette mentioned earlier, has seen double-digit growth for 8 consecutive quarters, and this is unquestionably a new gaming category. Like it's a new game console. This is going to be the largest game console in the world, I believe. And the reason for that is because there are more people with laptops than there are of any other device. And so the fact that we've been able to get RTX into a thin and light notebook, a thin and light notebook, is really a breakthrough. And it's one of the reasons why we're seeing such great success in notebook. Between the notebook business and our Nintendo Switch business, the profile of gaming overall has changed and has become more seasonal. It's more seasonal because devices, systems, like notebooks and Switch, are built largely in 2 quarters, Q2 and Q3. And they build it largely in Q2 and Q3 because it takes a while to build them and ship them and put them into the hubs around the world. And they tend to build it ahead of the holiday season. And so that's one of the reasons why Q3 will tend to be larger and Q4 will tend to be more seasonal and Q1 will tend to be more seasonal than the past. But the end demand is fantastic. RTX is doing great. And part of it is just a result of the success of our notebooks. I'm going to hand it over to Colette. Yes. So with that from a background and you think about all those different components that are within gaming, the notebook, the overall Switch, and of course, all of the ray tracing that we have in terms of desktop, our normal seasonality, as we look at Q1 for gaming with all those 3 pieces, is usually sequentially down from Q4, sequentially down Q4 to Q1. This year, the outlook assumes it will probably be a little bit more pronounced due to the coronavirus. So in total, we're probably looking at Q1 to be in the low double-digit sequential decline in gaming. So Atif, thanks for the question. Again, it's still very early regarding the coronavirus. Our thoughts are out with both the employees, the families and others that are in China. So our discussions, both with our supply chain that is very prominent in the overall Asia region as well as our overall AIC makers as well as our customers, is as about as timely as we can be. And that went into our discussion and our thoughts on the overall guidance that we gave into our $100 million. We'll just have to see how the quarter comes through, and we'll discuss more when we get to it. But at this time, that was our best estimate at this time. Yes. Thanks a lot, Will. Let's see, I think the -- historically, inference has been a small part of our business because AI was still being developed. Deep learning, AI is not -- historical AI, classical machine learning weren't particularly suited for GPUs and weren't particularly suited for acceleration. It wasn't until deep learning came along that the amount of computation necessary is just extraordinary. And the second factor is the type of AI models that were developed. Eventually, if -- the type of models related to natural language understanding and conversational AI and recommendation systems, these require instantaneous response. The faster the answer, the more likely someone is going to click on the answer. And so you know that latency matters a great deal, and it's measurable. The effect on the business is directly measurable. And so for conversational AI, for example, we've been able to reduce the latency of the entire pipeline from speech recognition to the language processing to, for example, fix the errors and such, come up with a recommendation to text to speech to the voice synthesis. That entire pipeline could take several seconds. We run it so fast that it's possible now for us to process the entire pipeline within a couple of hundred, 200, 300 milliseconds. That is in the realm of interactive conversation. Beyond that, it's just simply too slow. And so the combination of AI models that are large and complex that are moving to inference, moving to production. And then secondarily, conversational AI and latency-sensitive models and applications where our GPUs are essential, now moving forward, I think you're going to see a lot more opportunities for us in inference. The way to think about that long-term is acceleration is essential because of end of Dennard scaling. Process technology is going to demand that we compute in a different way. And the way that AI has evolved and deep learning, it suggests that acceleration on GPUs is just a really phenomenal approach. Data centers are going to have to be software-defined. And I think as I mentioned, I think I mentioned earlier to another question, I believe that in the future, the data center will all be accelerated. It will be all running AI models, and it will be software-defined and it will be programmable and having an accelerated computing platform is essential. As you move out to the edge, it really depends on whether your platform is software-defined, whether it has to be programmable or whether it's fix functioned. There are many, many devices where the inference work is very specific. It could be something as simple as detecting changes in temperature or changes in sound or detecting motion. Those type of inference models are -- could still be based on deep learning. It's function-specific. You don't have to change it very often and you're running 1 or 2 models at any given point in time. And so those devices are going to be incredibly cost-effective. I believe, those AI chips, you're going to have AI chips that are $0.50, $1, and you're just going to put it into something and it's going to be doing magical detections. The type of platforms that we're in, such as self-driving cars and robotics, the software is so complicated and there's so much evolution to come yet, and it's going to constantly get better. Those software-defined platforms are really the ideal targets for us. And so we call it AI at the edge, edge computing devices. One of the edge computing devices I'm very excited about is what people call mobile edge or basically 5G telco edge. That data center will be programmable. We recently announced that we partnered with Ericsson and we're going to be accelerating the 5G stack. And so that needs to be a software-defined data center. It runs all kinds of applications, including 5G. And those applications are going to be -- those opportunities are fantastic for us. Yes. Thanks, Mark. Our company has to live 10 years ahead of the market. And so we have to imagine where the world is going to be in 10 years' time, in 5 years' time and work our way backwards. Now our company is focused on one singular thing. The simplicity of it is incredible. And that one singular thing is accelerated computing, accelerated computing. And accelerated computing is all about the architecture, of course. It's about the complicated systems that we're in because throughput is high. When our acceleration, we can -- when we can compute 10x, 20x, 50x, 100x faster than the CPU, all of a sudden, everything becomes a bottleneck. Memory's a bottleneck, networking's a bottleneck, storage is a bottleneck, everything is a bottleneck. And so we have to be -- NVIDIA has to be a supremely good system designer. But the complexity of our stack, which is the software stack above it, is really where the investments over the course of the last -- some 29 years now, has really paid off. NVIDIA, frankly, has been an accelerated computing company since the day it was born. And so we -- our company is constantly trying to expand the number of applications that we can accelerate. Of course, computer graphics was an original one, and we're reinventing it with real-time ray tracing. We have rendering, which is a brand-new application that we're making great progress in. We just talked -- I just mentioned 5G acceleration. Recently, we announced genomics computing. And so those are new applications that are really important to the future of computing. In the area of artificial intelligence, from image recognition to natural language understanding, to conversation, to recommendation systems, to robotics and animation, the number of applications that we're going to accelerate in the field of AI is really, really broad. And each one of them are making tremendous progress and getting more and more complex. And so the question about the sustainability of our company really comes down to 2 dimensions. Let's assume for the fact -- let's assume for now that accelerated computing is the path forward, and we surely believe so. And there's a lot of evidence from the laws of physics to the laws of computer science that would suggest that accelerated computing is the right path forward. But this really basically comes down to 2 dimensions. One dimension is are we continuing to expand? Are we continuing to expand the number of applications that we can accelerate? Whether it's AI or computer graphics or genomics or 5G, for example. And then the number -- and then the second is those applications, are they getting more impactful and adopted by the ecosystem, the industry? And are they continuing to be more complex? Those dimensions, the number of applications and the rich -- and the impact of those applications and the evolution, the growth of complexity of those applications, if those dynamics continue to grow, then I think we're going to do a good job. We're going to sustain. And so -- and I think when I spelled it out that way, it's basically the equation of growth of our company. I think it's fairly clear that the opportunities are fairly exciting ahead. Jensen, I just wanted to ask you on the auto side. I think at least one of your customers might have slowed out their program. Just kind of curious as you look out the next couple of years, the challenges, if the OEM is moving slower? And then just any perspective on the regulatory side, has anything changed there, would be helpful. I think that the automotive industry is struggling, but -- for all of the reasons that everybody knows. However, the enthusiasm to redefine and reinvent their business model has never been greater. Every single one of them, every single one of them would know now and they surely -- they've known for some time, and autonomous capabilities is really the vehicle to do that. They need to be tech companies. Every car company wants to be a tech company. They need to be a tech company. Every car company needs to be software-defined. And the platform by which to do so is an electric vehicle with autonomous autopilot capability. That car has to be software-defined. And this is their future and they're racing to get there. And so although the automotive industry is struggling in near term, their opportunity has never been better in my opinion. The future of AV is more important than ever. The opportunity is very real. The benefits of autonomous is for whether it's safety, whether it's utility, whether it's cost reduction and productivity, has never been more clear. And so I think that I'm as enthusiastic as ever about the autonomous vehicles and the projects that we're working on are moving ahead. And so the near-term challenges of the automotive industry or whatever sales slowdown in China that they're experiencing, I feel badly about that. But the industry is as clearheaded about the importance of AV as ever. We had an excellent quarter with strong demand for NVIDIA RTX graphics and NVIDIA AI platforms and record data center revenue. NVIDIA RTX is reinventing computer graphics, and the market's response is excellent, driving a powerful upgrade cycle in both gaming and professional graphics, while opening whole new opportunities for us to serve the huge community of independent creative workers and social content creators and new markets in rendering and cloud gaming. Our data center business is enjoying a new wave of growth, powered by 3 key trends in AI, natural language understanding, conversational AI, deep recommenders, are changing the way people interact with the Internet. The public cloud demand for AI is growing rapidly. And as AI shifts from development to production, our inference business is gaining momentum. We'll be talking a lot more about these key trends and much more at next month's GTC Conference in San Jose. Come join me. You won't be disappointed. Thanks, everyone.","I guess on data center, Colette or Jensen, can you speak to some of the areas that drove the upside in the quarter? You talked about inference and -- both the T4 and the V100 having record quarters but relative to your internal expectations, what were some of the businesses that drove the upside? And if you can also speak to the breadth of your customer profile today relative to a couple of years ago, how that's expanded, that would be helpful as well. Colette, I'm wondering if you can give us -- in data center, if you can give us a little idea of what the mix was between industries and hyperscale. I think last quarter, hyperscale was a little bit less than 50%. Can you give us maybe the mix or how much it was up, something like that? Congratulations on the results. When I look at the numbers, the growth on an absolute basis sequentially in data center was almost 2x or north of 2x, what we've seen in the past as far as the absolute sequential change. Through the course of this quarter, you were pretty clear that you would expect to see an acceleration of growth in the December quarter. I'm just curious of how you think about that going into the April quarter? And how we should think about that growth rate through the course of this year? If you can give us any kind of framework. And Jensen, just curious, I mean, as you think about the bigger picture, where do you think we stand from an industry perspective today in terms of the amount or the attach rate of GPUs, is it for acceleration in the server market? And where do you think that might be looking out over the next 3 years or so? Obviously, congratulations on the data center success. I wanted to ask a little bit, Colette, about the -- you took $100 million out for coronavirus, and I wanted to ask a little bit about how you got to that number. Really 2 pieces. One, if you could remind us maybe in terms of units or revenue, how -- what percentage of your gaming business is within China? And as you look at that $100 million that you pulled out of the guidance, are you thinking about that from a demand disruption perspective? Or are you thinking about it from something in the supply chain that might limit your sales? Congratulations on the strong results and guidance. On gaming -- yes, no problem. Good to see the recent launch of your GeForce NOW service. But on the partnership with Tencent on cloud gaming, seems like Tencent should have a smoother transition to the cloud model. They are the largest gaming company in the world, so they own many of the games. They also have their own data center infrastructure already in place. But how is the NVIDIA team going to be supporting this partnership? Is it going to be [deal your] GeForce NOW hardware framework? Or will you just be supporting them with your standalone GPU products? And when do you expect the service to go mainstream? I guess a question on the gaming side. If I look at your overall revenue guide, it would seem to suggest that you're looking for typically, I guess, better seasonal trends into April. And I guess can you speak to that? And then how are you seeing desktop gaming demand with ray tracing content becoming more available? How should we think about the growth trajectory through 2020? And then just really as a modeling question as part of gaming, with notebook now 1/3 of the revenues, how should we think about kind of the seasonality going into April and July for that part of your business? Good job on results and guide. On the same topic, coronavirus. Colette, I'm a bit surprised that the guidance -- the range on the guidance is not wider versus historic. Can you just talk about why not widen the range? And what went into that $100 million hit from the coronavirus? Jensen, I'd love to hear your thoughts as to how you anticipate the inference market playing out. Historically, NVIDIA's had essentially all of the training market and little of the inference market in the last 1.5 years or so. I think that's changed where you've done much better in inference. Now you have the T4 in the cloud, you have EGX at the edge. And you have Jetson, I think, is what it's called at the sort of endpoint device. How do you anticipate that market for inference developing across those various positions? And how are you aligning your portfolio for that growth? Jensen, I guess I had a question about your -- how you think about the sustainability of your market position in the data center? And I guess in my simplistic view, about 12 years ago, you made out a consensus call to invest in CUDA software, distribute it to universities. Neural networking took off and you were the de facto standard, and here we are right now. And for me, what's interesting to hear is that the demand that you're seeing today for your products is from markets that's just developed within the last year. And my question is like, how do you think about your investment, your R&D investment strategy to make sure that you are staying way ahead of the market, of the competition and even your customers who are investing in these markets, too? PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
NVDA,Q1 2021,3442,7806,1777,"Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2021. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2021. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may vary materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 21, 2020, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Jensen. Thanks, Simona. Before Colette describes our quarterly results, I'd like to thank those who are on the front lines of this crisis, first responders, health care workers, service providers, who inspires every day with their bravery and selflessness. I also want to acknowledge the incredible efforts of our colleagues here at NVIDIA. Despite many challenges, they have barely broken stride during one of the busiest periods in our history. Our efforts related to the virus are focused in 3 areas. First, we're taking care of our families and communities. We've pooled in raises by 6 months to put more money in our employees' hands, and NVIDIA and our people have donated thus far more than $10 million to those in need. Second, we're using NVIDIA's unique capabilities to fight the virus. A great deal of science being done on COVID-19 uses NVIDIA technology for acceleration when every second counts. Some of the many examples, including sequencing the virus, analyzing drug candidates, imaging the virus at molecular resolution with cryo-electron microscopy and identifying elevated body temperature with AI cameras. And third, because COVID-19 won't be the last killer virus, we need to be ready for the next outbreak. NVIDIA technology is essential for the scientific community to develop an end-to-end computational defense system, a system that can detect early, accelerate the development of a vaccine, contain the spread of disease and continuously test and monitor. We are racing to deploy the NVIDIA Clara computational health care platforms, Clara Parabricks can accelerate genomics analysis from days to minutes. Clara Imaging will continue to partner with leading research institutes to develop state-of-the-art AI models to detect infections, and Clara Guardian will connect AI to cameras and microphones and hospitals to help overloaded staff watch over patients. We completed the acquisition of Mellanox on April 27. Mellanox is now NVIDIA's networking brand and business unit and will be reported as part of our data center market platform, and Israel is now one of NVIDIA's major technology centers. The new NVIDIA has a much larger footprint in data center computing, end-to-end and full-stack expertise in data center architectures and tremendous scale to accelerate innovation. NVIDIA Mellanox are a perfect combination and position us for the major forces shaping the IT industry today, data center scale computing and AI. From micro service cloud applications to machine learning and AI, accelerated computing and high-performance networking are critical to modern data centers. Previously, a CPU compute node was the unit of computing. Going forward, the new unit of computing is an entire data center. The basic computing elements are now storage servers, CPU servers and GPU servers, and are composed and orchestrated by hyperscale applications that are serving millions of users simultaneously. Connecting these computing elements together is the high-performance Mellanox networking. This is the era of data center scale computing. And together, NVIDIA Mellanox can architect end to end. Mellanox is an extraordinary company, and I'm thrilled that we're now one force to invent the future together. Now let me turn the call over to Colette. Thanks, Jensen. Against the backdrop of the extraordinary events unfolding around the globe, we had a very strong quarter. Q1 revenue was $3.08 billion, up 39% year-on-year, down 1% sequentially and slightly ahead of our outlook, reflecting upside in our data center and gaming platforms. Starting with gaming. Revenue of $1.34 billion was up 27% year-on-year and down 10% sequentially. We are pleased with these results, which exceeded expectations in the quarter, marked by the unprecedented challenge of the COVID-19. Let me give you some color. Early in Q1, as the epidemic unfolded, demand in China was impacted with iCafes closing for an extended period. As the virus spread globally, much of the world started working and learning from home, and gameplay surged. Globally, we have seen 50% rise in gaming hours played on our GeForce platform, driven both by more people playing and more gameplay per user. With many retail outlets closed, demand for our products has shifted quite efficiently to e-tail channels globally. Gaming laptops revenue accelerated to its fastest year-on-year growth in 6 quarters. We are working with our OEMs, channel partners to meet the growing needs of the professionals and students engaged in working, learning and playing at home. In early April, our global OEM partners announced a record new 100 NVIDIA GeForce-powered laptops with availability starting in Q1 and the most to ship in Q2. These laptops are the first to use our high-end GeForce RTX 2080 SUPER and 2070 SUPER GPUs, which have been available for desktop since last summer. In addition, OEMs are bringing to market laptops based on the RTX 2060 GPU at just $999, a price point that enables a larger audience to take advantage of the power and features of RTX, including its unique ray tracing and AI capabilities. These launches are well-timed as mobile and remote computing needs accelerate. The global rise in gaming also lifted sales of NVIDIA Nintendo Switch and our console business, driving strong growth both sequentially and year-over-year. We collaborated with Microsoft and Mojang to bring RTX ray tracing to Minecraft, the world's most popular game with over 100 million gamers monthly and over 100 billion total views on YouTube. Minecraft with RTX looks astounding with realistic shadows and reflections. Light that reflects, refracts and scatters through surfaces as naturalistic effects like fog. Reviews for it are off the charts. Ars Technica called it a jaw-dropping stunner, and PC World said it was glorious to behold. Our RTX technology stands apart, not only with our 2-year lead in ray tracing but with its use of AI to speed up and enhance games using the Tensor Core silicon on our RTX class GPUs. We introduced the next version of our AI algorithm called Deep Learning Super Sampling. In real time, DLSS 2.0 can fill the missing bits from every frame, doubling performance. It represents a major step function from the original, and it can be trained on nongaming-specific images, making it universal and easy to implement. The value and momentum of our RTX GPUs continue to grow. We have a significant upgrade opportunity over the next year with the rise and tide of RTX-enabled games, including major blockbusters like Minecraft and Cyberpunk. Let me also touch on our game streaming service, GFN, which exited beta this quarter. It gives gamers access to more than 650 games with another 1,500 in line to get onboarded. These include Epic Games, Fortnite, which is the most played game on GFN; and other popular titles such as CONTROL, Destiny 2 and League of Lighting in the fall. Since launching in February, GFN has added 2 million users around the world, with both sign-ups and hours of gameplays boosted by stay-at-home measures. GFN expands our market reach to the billions of gamers with underpowered devices. It is the most publisher-friendly, developer-friendly game streaming service with the greatest number of games and the only one that supports ray tracing. Moving to Pro Visualization. Revenue was $307 million, up 15% year-on-year and down 7% sequentially. Year-on-year revenue growth accelerated in Q1 driven by laptop workstations and Turing adoption. We are seeing continued momentum in our ecosystem for RTX ray tracing. We now have RTX support for all major rendering visualization and design software packages, including Autodesk Maya, Dassault's CATIA, Pixar's RenderMan, Chaos Group's V-Ray and many others. Autodesk has announced that the latest release of VRED, its automotive 3D visualization software, supports NVIDIA RTX GPUs. This enables designers to take advantage of RTX to produce more like-life designs in a fraction of the time versus CPU-based systems. Over 45 leading creative and design applications now take advantage of RTX, driving a sustained upgrade opportunity for Quadro-powered systems while also expanding their reach. We see strong demand in verticals, including health care, media and entertainment and higher education, among others. Higher health care demand was fueled in part by COVID-19 related research at Siemens, Oxford and Caption Health. Caption Health received FDA clearance for an update to its AI-guided ultrasound, making it easier to perform diagnostics-quality cardiac ultrasounds. And in media and entertainment, demand increased as companies like Disney deployed remote workforce initiatives. Turning to automotive and robotic autonomous machines. Automotive revenue was $155 million, down 7% year-on-year and down 5% sequentially. The automotive industry is seeing a significant impact from the pandemic, and we expect that to affect our revenue in the second quarter as well, likely declining about 40% from Q1. Despite the near-term challenges, our important work continues. We believe that every machine that moves someday will have autonomous capabilities. During the quarter, Xpeng introduced the P7, an all-electric sports sedan with innovative Level 3 automated driving features, powered by the NVIDIA DRIVE AGX Xavier AI compute platform. Our open, programmable, software-defined platform enables Xpeng to run its proprietary software while also delivering over-the-air updates for new driving features and capabilities. Production deliveries of the P7 with NVIDIA DRIVE begin next month. Our Ampere architecture will power our next-generation NVIDIA DRIVE platform called Orin, delivering more than 6x the performance of Xavier Solutions and 4x better power efficiency. With Ampere scalability, the DRIVE platform will extend from driverless robotaxis all the way down to in windshield driver assistant systems sipping just a few watts of power. Customers appreciate the top-to-bottom platform all based on a single architecture, letting them build one software-defined platform for every vehicle in their fleet. Lastly, in the area of robotics, we announced that BMW Group has selected the new NVIDIA as a robotics platforms to automate their factories, utilizing logistic robots built on advanced AI computing and visualization technologies. Turning to data center. Quarterly revenue was a record $1.14 billion, up 80% year-on-year and up 18% sequentially, crossing the $1 billion mark for the first time. Announced last week, the A100 is the first Ampere architecture GPU. Although just announced, A100 is in full production, contributed meaningful to Q1 revenue and demand is strong. Overall, data center demand was solid throughout the quarter. It was also broad-based across hyperscale and vertical industry customers as well as across workloads, including training, inference and high-performance computing. We continue to have solid visibility into Q2. The A100 offers the largest leap in performance to date over our 8 generations of GPUs, boosting performance by up to 20x over its predecessor. It is exceptionally versatile, serving as a universal accelerator for the most important high-performance workloads, including AI training and inference as well as data analytics, scientific computing and cloud graphics. Beyond its leap performance and versatility, the A100 introduces new elastic computing technologies that make it possible to bring rightsized computing power to every job. A multi-instance GPU capability allows each A100 to be partitioned into as many as 7 smaller GPU instances. Conversely, multiple A100 interconnected by our third-generation NVLink can operate as one giant GPU for ever larger training tasks. This makes the A100 ideal for both training and for inference. The A100 will be deployed by the world's leading cloud service providers and system builders, including Alibaba cloud, Amazon Web Services, Baidu Cloud, Dell Technologies, Google Cloud platform, HPE and Microsoft Azure, among others. It is also getting adopted by several supercomputing centers, including the National Energy Research Scientific Computing Center and the Jlich Supercomputing Centre in Germany and Argonne National Laboratory. We launched and shipped the DGX A100, our third-generation DGX and the most advanced AI system in the world. The DGX A100 is configurable from 1 to 56 independent GPUs to deliver elastic software-defined data center infrastructure for the most demanding workloads from AI training and inference to data analytics. We announced 2 products for edge AI: the EGX A100 for larger commercial off-the-shelf servers; and the EGX Jetson Xavier NX for micro-edge servers. Supported by full AI optimized cloud, native and secure software, the EGX platform is built for AI computing at the edge. With the EGX, hospitals, retail stores, farms and factories can securely carry out real-time processing of the massive amounts of data streaming from trillions of edge sensors. NVIDIA EGX makes it possible to securely, deploy and manage and update fleets of servers remotely. EGX is also ideal for the massive computational challenge of 5G networks, which we are working on with our partners like Ericsson and Mavenir. Additionally, we announced CUDA 11 and other important software harnessing the A100's performance and universatility (sic) [universality] to accelerate 3 of the most complex and fast-growing workloads: recommendation systems, conversational AI and data science. First, NVIDIA Merlin is a deep recommendator (sic) [recommender] application framework that enables developers to quickly build state-of-the-art recommendation systems, leveraging our pretrained models. With billions of users and trillions of items on the Internet, deep recommendators are the critical engine powering virtually every internet service. Second, NVIDIA Jarvis is a GPU-accelerated application framework that makes it easy for developers to create, deploy and run end-to-end real-time conversational AI applications that understand terminology unique to each company and its customers using both vision and speech. Demand for these applications are surging. Amid the shift to working from home, telemedicine and remote learning. And third, in the field of data science and data analytics, we announced that we are bringing end-to-end GPU acceleration to Apache Spark, an analytics engine for big data processing that uses more than 500,000 data scientists worldwide. Native GPU acceleration for the entire Spark pipeline, from extracting, transforming and loading the data to training to inference, delivers the performance and the scale needed to finally connect the potential of big data with the power of AI. Adobe has achieved a 7x performance improvement and a 90% cost savings in an initial test using GPU-accelerated data analytics with Spark. Our accelerated computing platform continues to gain momentum, underscored by the tremendous success of GTC Digital, our annual GPU technology conference, which shifted this spring to an online format. More than 55,000 online developers and AI research registered for the online event, which includes hundreds of hours of free content from AI practitioners and industry experts who leverage NVIDIA's platforms. Our ecosystem is now 1.8 million developers strong. Times like these truly test a computing platform's metal in the utility it brings to scientist racing for solutions. Researchers around the world are deploying our GPU computing platform in the fight against COVID-19. Scientists are combining AI simulation to detect changes in pneumonia cases, sequence, the virus and seek effective biomolecular compounds for a vaccine or treatment. The first breakthrough came from researchers at the University of Texas at Austin and National Institute of Health, who used the GPU-accelerated application to create the first 3D atomic scale map of virus using NVIDIA GPUs. This was followed by researchers at Oak Ridge National Laboratory who screened 8,000 compounds to identify 77 promising drug targets using the world's fastest supercomputer, Summit, which is powered by more than 27,000 NVIDIA GPUs. The V100 GPUs at Oak Ridge are in high demand as they can analyze 17 million compound protein combinations in a day. They'll help understand the virus spread pattern, the University of California, San Diego, researchers ported their microbiomic analysis software to GPUs in the San Diego supercomputing cluster of 500x analysis speed up from what some people are more susceptible to the virus. Okay. Moving to the rest of the P&L. Q1 GAAP gross margins was 65.1% and non-GAAP was 65.8%, up sequentially and year-on-year, primarily driven by GeForce GPU product mix and higher data center sales. Q1 GAAP operating expenses were $1.03 billion, and non-GAAP operating expenses were $821 million, up 10% and 9% year-on-year, respectively. Q1 GAAP EPS was $1.47, up 130% from a year earlier, and non-GAAP EPS was $1.80, up 105% from a year ago. Q1 cash flow from operations was $909 million. Before I turn to the outlook, let me make a few comments on our Mellanox acquisition. Beyond the strong strategic and cultural fit that Jensen has discussed, Mellanox has exceptionally strong financial profile. The company reported revenue of $429 million in its March quarter, accelerating to 40% year-on-year growth, with GAAP and non-GAAP gross margins in the mid- to high 60% range. We expect the acquisition to be immediately accretive to non-GAAP gross margins, non-GAAP earnings per share and free cash flow. We aim to retain the full Mellanox team and accelerate investments in our combined road map as we jointly innovate on our shared vision for the future of accelerated computing. With that, let me turn to the outlook of the second quarter of fiscal 2021, which includes a full quarter contribution from Mellanox. We have assumed in our outlook the potential ongoing impact from COVID-19. We expect our automotive platform sales to be down 40% on a sequential basis and Pro Viz to decline sequentially. In gaming, while we will likely see ongoing impact from the partial operations or closures of iCafes and retail stores, we expect that to be largely offset by a shift to e-tail channels. Overall, the precise magnitude of the impact is difficult to predict, given uncertainties around the reopening of the economy. Overall, we expect second quarter revenue to be $3.65 billion, plus or minus 2%. The contribution of Mellanox revenue is likely to be in the low teens percentage range of our total Q2 revenue. We are providing this breakout to help with comparability between Q1 and Q2. But going forward, it will become an integrated part of our data center market platform. GAAP and non-GAAP gross margins are expected to be 58.6% and 66%, respectively, plus or minus 50 basis points. The sequential decline in GAAP gross margins primarily reflects an increase in acquisition-related costs, most of which are nonrecurring. GAAP and non-GAAP operating expenses are expected to be approximately $1.52 billion, and $1.04 billion, respectively. The sequential change in GAAP operating expenses reflects an increase in stock-based compensation and acquisition-related costs. GAAP and non-GAAP operating expenses for the full year are expected to be approximately $5.7 billion and $4.1 billion, respectively. For the full year, stock-based compensation and acquisition-related costs also influence. GAAP and non-GAAP OI&E are both expected to be an increase of approximately $50 million and $45 million, respectively. GAAP and non-GAAP tax rates are both expected to be 9%, plus or minus 1%, excluding discrete items. Capital expenditures are expected to be approximately $225 million to $250 million. Further financial details are included in the CFO commentary and other information available on our IR website. New this quarter, we have also posted an investor presentation summarizing our results and key highlights. In closing, let me highlight upcoming events for the financial community. Next Thursday, May 28, we will webcast a presentation and Q&A with Jensen on our recent product announcement moderated by Evercore. We will also be at Cowen's TMT Conference on May 27; Morgan Stanley's Cloud Secular Winners Conference on June 1; BoFa's Technology Conference on June 2; Needham's Fourth Automotive Technology Conference on June 3 and Nasdaq Investor Conference on June 16. Operator, we will now open for questions. Can you please poll for questions, please.","Thanks, Will, for the question. You are correct. We have indicated a couple of quarters ago that we were starting to see improved visibility after we came out of the digestion period in the prior overall fiscal year. As we move into Q2, we still have visibility and solid visibility into our Q2 results for overall data centers. So at this time, I'd say they are relatively about the same of what we had seen going into the Q1 period. And we think that is a true indication of their excitement about our platform and most particularly our excitement regarding A100, and that's launched and its additional products. Now regarding the second half of the year, as you know, we have seen broad-based growth in both the hyperscale and the vertical industries, both of them in terms of at record levels. In our Q1 results. And we see in terms of inferencing continuing to grow as well, as well as we're also expanding in terms of edge AI. Our strong demand of the A100 products, including the Delta Board, but also in terms of our DGXs, was just starting an initial ramp. However, we do guide only 1 quarter at a time. So it's still a little bit too early for us to give a true certainty in terms of the macro situation that's in front of us. But again, we feel very good about the demand for A100. So let me start, and I'll see if Jensen also wants to add on to it. I think you're talking about our sequential between Q1 and Q2. Some of the... Right. Some of the pieces that we had seen related to COVID-19 in Q1 may carry over into Q2. COVID-19, in fact, had an impact in terms of our retail channels as well as our iCafes. However, as we discussed, efficiently, moved to overall e-tail. We have normally been seasonally down in desktop between Q1 and Q2, and that will likely happen. But we do see the strength in terms of laptops and overall consoles as we move for Q1 to Q2. So in summary, we do expect grow sequentially between Q1 and Q2 for our overall gaming business. And I'll turn it over to Jensen to see if he has additional commentary. No, that was great. That was fantastic. Yes. I think when we think about that sequential growth, we'll probably be in the low -- moving up to probably the mid-single digits in terms of -- that's what our guidance right now, and we'll just have to see how the quarter goes. Stacy, the thing that I would add is this. I would say, I think the guidance is exactly what Colette mentioned. But if you look at the big picture, there's a few dynamics that are working really well in our favor. First, of course, is that RTX and ray tracing is just the home run. Minecraft was phenomenal. We have 33 games in the pipe that has already been announced or shipping. Just about every game developers signed on to RTX and ray tracing, and I think it's a foregone conclusion that this is the next generation. This is the way computer graphics is going to be in the future. And so I think RTX is a home run. The second, the notebooks that we create is just doing great. We got 100 notebooks in gaming. We have 75 notebooks designed for either mobile workstations or what we call NVIDIA studio for designers and creators. And the timing was just perfect. With everybody needing to stay at home, the ability to have a mobile gaming platform and a mobile workstation, it was just perfect timing. And then, of course, you guys know quite well that our Nintendo Switch is doing fantastic. There are 3 -- the top 3 games in the world. The top games in the world today are Fortnite, Minecraft and Animal Crossing. All 3 games are NVIDIA platforms. And so I think we have all the dynamics working in our favor. And then we just got to see how it turns out. I wanted to ask about the rollout of Ampere how quickly does that roll in the various segments between hyperscale as well as on the DGX side as well as on the HPC side. And is it a smooth transition? Is there -- I remember when you launched Volta, there was a little bit of a transitional pause. Just can you tell us how you see that ramping up with the different customer segments? Yes. Thanks a lot, Joe. So first of all, taking a step back. Accelerated computing is now common sense in data centers. It wasn't the case when we first launched Volta. If you went back to Volta, Volta was the first generation that the deep learning training in a really serious way, and it was really focused on training. It was focused on training and high-performance computing. We didn't come until later with the inference version called T4. But over the course of the last 5 years, we've been accelerating workloads that are now diversifying in data centers. If you take a look at most of the hyperscalers, machine learning is now pervasive. Deep learning is now pervasive. The notion of accelerated deep learning and machine learning using our GPUs is now common sense. It didn't used to be. People still saw it as something esoteric. But today, data centers all over the world expect a very significant part of their data center being accelerated with GPUs. The number of workloads that we've accelerated since in the last 5 years have expanded tremendously, whether it's imaging or video or conversational AI or deep recommender systems that probably unquestionably, at this point, the most important machine learning model in the world. And so the number of applications we now accelerate is quite diverse. And so that's really -- that's contributed greatly to the ramp of Ampere. When we came -- when we started to introduce Ampere to the data center, it was very commonsensical to them that they would adopt it. They have a large amount of workload that's already accelerated by NVIDIA GPUs. And as you know, our GPUs are architecturally compatible from generation to generation. We're forward compatible or backwards compatible. Everything that runs on T4 runs on A100, everything that runs on V100 runs on A100. And so I think the transition is going to be really, really smooth. On the other hand, because V100 and T4 -- which, by the way, V100 and T4 had a great quarter. It was sequentially up. And then on top of that, we grew with the A100 shipment. A100 -- or excuse me, V100 and T4 are now quite broadly adopted in hyperscalers for their AI services, in cloud computing, in a vertical industries, which is almost roughly half of our overall HPC business. All the way out to the edge, which had a great quarter. Much smaller part, of course -- supercomputing is important, but it's a very small part of the high-performance computing. But that's also -- we also shipped A100 to supercomputing centers. And so I think the general sense of it -- the summary of it is that the number of workloads for accelerated computing has continued to grow, the adoption of machine learning and AI and all the cloud and hyperscalers has grown. The common sense of using acceleration is now a foregone conclusion. And so I think we're ramping into a very receptive market with a really fantastic -- with a really fantastic product. Congratulations on the strong growth and execution. Just a quick clarification. Colette, 66% kind of the new baseline for gross margin? And then the question, Jensen, for you, is give us a sense for how much inference as a workload and payer as a product are expected to contribute? I'm just curious where you are in terms of growing in the inference and edge AI market? And where are we kind of in the journey of Ampere penetration? So let me start on the first question regarding the gross margin and our gross margin as we look into Q2. We are guiding Q2 non-GAAP gross margins at 66%. This is -- would be another record gross margin quarter just as we finished an overall record level, even as we are continuing right now to ramp our overall Ampere architecture within that. The Q2 also incorporates Mellanox. Mellanox has a very similar overall margins to our overall data center margins as well. But we see this new baseline as a great transition and likely to see some changes as we go forward. However, it's still a little early to see where these gross margins will go. But we're very pleased with the overall guidance right now at 66% for Q2. Accelerated computing is just at the beginning of its journey. If you look at -- I would characterize it as several segments. First is hyperscaler AI microservices, which is all the services that we enjoy today that has AI. Whenever you shop on the web, it recommends a product. When you're watching a movie, it recommends a movie or it recommends a song. All of those -- or recommends news or recommends a friend or recommends a website, the first 10 websites that they recommend. All of these recommenders that are powering the Internet are all based on machine learning today. It's the reason why they're collecting so much data. The more data they can collect, the more they could predict your preference, and that predicting your preference is the core to a personalized Internet. It used to be largely based on CPU approaches. But going forward, it's all based on deep learning approaches. The results are much more superior and a few percentage change in preference prediction accuracy could result in tens of billions of dollars of economics. And so this is very, very big deal. And the shift towards deep learning in hyperscale micro services or AI micro services is still ramping. Second is cloud. And as you know, cloud is a $100 billion market segment of it today, growing about 40% into $1 trillion opportunity. This -- cloud computing is the single largest IT industry transformation that we have ever seen. The 2 powers that is really -- the force -- the 2 forces that is really driving our data center business is AI and cloud computing. We're perfectly, perfectly positioned to benefit from these 2 powerful forces. So the second is cloud computing. And that journey is -- has a long ways to go. Then the third is industrial edge. In the future, today -- it's not the case today. But the combination of IoT, 5G, industrial 5G and artificial intelligence, it's going to turn every single industry into a tech industry. And whether it's logistics or warehousing or manufacturing or farming, construction, industrial, every single industry will become a tech industry. And there'll be trillions of sensors, and they'll be connected to little micro data centers. And those data centers will be in the millions. They'll be distributed all over the edge. And that journey has just barely started. We announced 3 very important partners in 3 domains. And they're the lead partners that we felt that people would know, but we have several hundred partners that are working with us on edge AI. We announced Walmart for smart retail. We announced the U.S. Postal Service, the world's largest mail sorting service and logistics service. And then we announced this last quarter, BMW, who is working with us to transform their factory into a robotics, automated factory of the future. And so these 3 applications are great examples of the next phase of artificial intelligence and where Ampere is going to ramp into. And that is just really at its early stages. And so I think it's fair to say that we're really well positioned in the 2 fundamental forces of IT today, data center scale computing and artificial intelligence. And the segments that it's going to make a real impact are all gigantic markets. Hyperscale AI, cloud and edge AI. C.J., can you help me? You cut out in the middle of your sentence to me. Can you repeat the first part of it for me? At this time, it's a little early for us. And as you know, we generally just go 1 quarter out, and we're excited to bring the Mellanox team on board so we can start beginning the future of building products together. For the overall margin, their overall performance over the last couple of quarters, they had a great last year. They had a great March quarter as well. And we're just going to have to stay tuned to see equally with them what the second half of the year looks for them. Okay? Yes, C.J., thanks for the question. This pandemic is really quite tragic, and it's reshaping industries and markets. And I think it's going to be structural. I think it's going to remain. And I think your question is really good because now it's a good time to think about where to double down. There's a few areas that I believe are going to be structurally changed. And I think that once I say it, it'll be very sensible. The first is that the world's enterprise digital transformation and moving to the cloud, that is going to accelerate. Every single company can't afford to rely just on on-prem IT. They have to be much more resilient. And having a hybrid cloud computing infrastructure is  going to provide them the resilience they need. And so that's one. And when the world moves and accelerates into this $1 trillion IT infrastructure transformation, which is now $100 billion into that journey, it's growing 40% a year, I wouldn't be surprised to see that accelerate. And so cloud computing AI is going to accelerate because of that. The second is the importance of creating a computational defense system. The defense systems of most nations today are based on radar. And yet in the future, our defense systems are going to detect things that are unseeable. It's going to be infectious disease. And I think every nation and government and scientific lab is now gearing up to think about what does it take to create a national defense system for each country that is based on computational methods? And NVIDIA is an accelerated computing company. We take something that otherwise would take a year in the case of Oak Ridge, and they filter 1 billion compounds in a day. And that's what you need to do. You need to find a way to have an accelerated computational defense system that allows you to find insight, detect early warning ASAP. And then, of course, the computational system has to go through the entire range from mitigation to containment to living within the monitoring. And so scientific labs are going to be gearing up. National labs are going to be gearing up. The third part is AI and robotics. We're going to have to have the ability to be able to do our work remotely. NVIDIA has a lot of robots that are helping us in our labs. And without those robots helping us in our labs, we'll have a hard time getting our work done. And so we need to have remote autonomous capability for -- to handle all of these -- either dangerous circumstances to disinfect environments, to fumigate environments autonomously, to clean environments, to be able to interact with people where as little as possible in the event of an outbreak. All kinds of robotics applications are being dreamed up right now to help society forward in the case of another outbreak. And then lastly, I think more and more people are going to work permanently from home. There's a strong movement of companies that are going to support a larger percentage of people working from home. And when people work from home, it's going to clearly increase the single best home entertainment, which is video games. I think video games is going to represent a much larger segment of the overall entertainment budget of society. And so these are some of the trends, I would say. I would say cloud computing, AI. I would say national labs, a computational defense system, robotics and working from home are structural changes that are going to be here to stay. And these dynamics are really good for us. Thanks, Toshiya, for your question. So regarding our gross margins in the second quarter, our second quarter guide at 66% is up sequentially from even a record level in terms of what we had in terms of Q1. This next record that we hope to achieve with our overall guidance is even with including our overall Ampere architecture. So typically, when we transition to a new architectures, margins can somewhat be a little bit lower on the onset but tend to kind of move up and trend up over time. Additionally, as you articulated, our automotive is lower. But also, we're going to see growth in some of our platforms in gaming such as consoles, which may offset those 2. But overall, there's nothing structural to really highlight other than our mix in business and the ramp of Ampere and its transition. Let's see, the trade tension. We've been living in this environment for some time, Toshiya. And as you know, the trade tension has been in the background for coming up on a year, probably gotten longer. And China's high-performance computing systems are largely based on Chinese electronics anyhow. And so that's -- I think our condition won't materially change going forward. So Toshiya, let me respond to your second question that you had for me, which was regarding to our OpEx and our decision to pull forward our overall local into Q2. This is something that we've normally done later in the year. We felt it was prudent during the current COVID-19. Although our employees are quite safe. We just wanted to make sure that their family members also were safe and had the opportunity to have cash upfront. It is about a couple of months, about 4 months earlier than normal, and it is incorporated in our guidance for Q2. Yes. Thanks for the call, Mark. Good Question. I think the -- if you take a step back, currently in our data centers, the current setup in data centers, starting from probably all the way back, 6, 7 years ago, but really accelerating in the last 5 years and then really accelerating in the last couple of years, we learned our way into it. There are 3 classes of workloads, and they kind of came into acceleration over time. The first class of workload that we discovered was -- the major workload was deep learning training. Deep learning training. And the ideal setup for that today prior to Ampere or yesterday prior to Ampere is the V100 SXM with NVLink, 8 GPUs on one board, and that architecture is called scale up. It's like a supercomputer architecture. It's like a -- it's like a weather simulation architecture. You're trying to build the largest possible computing node you can for one operating system called scale up. And the second thing that we learned along the way was then cloud computing started to grow because researchers around the world needed to get access to an accelerated platform for developing their machine learning algorithms. And because they have a different degree of budget, and they want to get into it, a little bit more lightly and have the ability to scale up to larger nodes, the perfect model for that was actually a V100 PCI Express, not SXM, but PCI Express that allows you to offer 1 GPU all the way up to many GPUs. And so that versatility, V100 PCI Express, not as scalable in performance as the V100 SXMs, but it was much more flexible for rentals. Cloud renting was really quite ideal. And then we started to get into inference, and we're on our seventh generation of TensorRT, TensorRT 7.0. Along the way, we've been able to accelerate more and more. And today, we largely accelerate every deep learning inference computational graph that's out there. And the ideal GPU for that was something that has the reduced precision, which is called (inaudible), reduced precision not with electronics that is focused more for inference -- and because inference is a scale-out application, where you have millions of queries, and each one of the queries are quite small versus scale up, where you have 1 training job and that 1 training job is running for a day. It could be running for days and sometimes even weeks. And so scale-up application is for 1 user that uses it for a long period of time on a very large machine. Scale out, it's for millions of users, each one of them have a very small query and that query could last hundreds of milliseconds and where ideally, you like to get it done in hundreds of milliseconds. And so notice, I've said 3 different architecture in a data center today. Most data centers today has a storage server, has CPU servers, and it has scale-up acceleration service with Voltas has scaled out servers with GeForce and then it has scale cloud computing, flexible servers based on V100. And so the ability to predict workload is so hard, and therefore, the utilization of these systems will be spiky. And so we created an architecture that allows for 3 things. So things -- the 3 characteristics of Ampere are: number one, it is the greatest generational leap in history. I mean I don't remember a generation where we increased throughput for training and inference by 20x. And it's just a gigantic -- for training and for inference, it is a gigantic leap forward. The second, it's the first architecture that is unified. We could use this to the computational -- the computation engine of Ampere accelerates the moment the data comes into the data center. From data processing, it's called [ETO]; the engine, which many of you probably know, it's the single most important computational engine in the world today for big data. It used to be Hadoop, but now it's Spark. Spark is used all over the world, 16,000 customers. We finally have the ability to accelerate that. And then it's -- Ampere is also good for training, deep learning, machine learning, extra boost as well as deep learning, all the way out to inference. And so we now have a unified acceleration platform for the entire workload. And then the third thing is it's the first GPU ever, the first acceleration platform ever that's elastic. You could reconfigure it. You could configure it for either scale up or you can configure it for scale out. When you configure it for scale up, you're gaining a whole bunch of GPUs together using NVLink, and it creates this 1 gigantic GPU. When you want to scale it out, that same computation node becomes 56 small GPUs. Each one of those 56 partitions, each 1 is more powerful than Volta. I mean it's really quite extraordinary. And so Ampere is a breakthrough on all of these fronts for performance for the fact that it unifies the workload, and you can now have 1 acceleration cluster; and then number three, it's elastic. You could use in the cloud, you could use for inference, you could use it for training. And so the versatility of Ampere is the thing that I'm most excited about. And now you could have 1 acceleration cluster that serves all of your needs. That's very helpful. Colette, did you want to handle that first? And then I'll do the... Sure. So let me help you out on the overall GAAP adjustments, so the delta between our GAAP OpEx and our non-GAAP OpEx. If you look at it for the full year and what we guided, we probably have about $1.55 billion associated with GAAP level expenses. Keep in mind, there is more in there than just our stock-based compensation. We have also incorporated the accounting that we will do for the overall Mellanox, and a really good portion of those costs are associated with the amortization of intangibles and also in terms of acquisition-related costs and deal fees and onetime items. So our stock-based compensation includes what we need for NVIDIA and also the onboarding of Mellanox. There is some retention with the overall onboarding of Mellanox. But for the most part, it is just working them in to the year for 3 quarters, which is influencing the stock-based compensation. Tim, there are several differences between our condition then and our condition today. So the first -- the first difference is the diversity of workload we now accelerate. Back then, we were early in our inference. We were still early in our inference, and most of the data center acceleration was used for deep learning. And so today, the versatility stands from data processing to deep learning and the number of -- the number of different types of AI models that's being trained for deep learning is growing tremendously from detecting, from training video, from training a model to detecting unsafe video. The natural language understanding the conversational AI to now a gigantic movement towards deep recommender systems. And so the number of different models that are being trained is growing. The size of the models are gigantic. Recommendation systems are gigantic. They're training on models that are hundreds. The data sizes, hundreds of terabytes. Terabytes, hundreds of terabytes. And it would take tens of -- hundreds of servers to hold all of the data that is needed to train these recommender systems. And so the diversity of -- from data analytics to training all the different models to the influence of all different models. We didn't inference recurring elements at a time, which is probably the most important model today. Text language models, speech models are all recurrent, Euronet models. And so those models were early for us at the time. So number one is the diversity of workload. The second is the acceleration of -- to cloud computing. I think that accelerated cloud computing is a movement that is going to be a multiyear if not a decade-long transition. From where we are today, it's only $100 billion industry segment of the IT industry. It's going to be $1 trillion someday, and that movement is just starting. We're also much more diversified out of the clouds. At the time, cloud was largely where our acceleration went for deep learning. And today, hyperscale only represents about half. And so we've diversified significantly out of cloud, not out of cloud, but including vertical industries. And a lot of that has to do with edge AI and inference. And as I mentioned earlier, we're working with Walmart and BMW and USPS, and that's just the tip of the iceberg. And so I think the conditions are a little different. And then what I would say lastly is Ampere. I mean we are -- we've ramped a few weeks. Even though it was quite significant, it was a great ramp. The demand is fantastic. It is the best ramp we've ever had. The demand is the strongest we've ever had in data centers. And we're starting to ramp of a multiyear ramp. And so -- those are some of the differences. I think the conditions are very different. Yes. Great. Thanks a lot, Harlan. I appreciate the question. So DGX, you know this is our third-generation DGX and it's really successful. People love it. It's the most advanced AI instrument in the world. If you're a serious AI researcher, this is your instrument. And in the DGX, there are 8 A100s and there are 9 Mellanox NICs, the highest speed NICs they have. And so we have a great appreciation for high-performance networking. High-performance networking and high-performance computing go hand-in-hand. And the reason for that is because the problems we're trying to solve no longer fit in one computer, no matter how big it is. And so it has to be distributed. And when you distribute a computational workload of such intense scale, the communications overhead becomes one of its greatest bottlenecks, which is the reason why Mellanox is so valuable. There's reason why this company is so precious and really a jewel and one of a kind. And so -- and it's not just about the link speed. It's not mostly. I mean we just have a deep appreciation for software. It's a combination of architecture and software and electronics design, chip design. And in that combination, Mellanox is just world-class. And that's the reason why they're in 60% of the world's supercomputers. That's why they're in 100% of the AI supercomputers. And their understanding of large-scale distributed computing is second to none. Now in the world of -- and I just talked about scale up. And you're absolutely right. Now the question is why scale out? And the reason for that is this. This is the reason why they're doing so well. The movement towards disaggregated microservice applications where containers, microservice containers are distributed all over the data center and orchestrated so that the workload could be distributed across a very large hyperscale data center. That architecture -- and you probably know the 3 most important application in my estimation in the world today, number one, would be TensorFlow and PyTorch; number two would be Spark; and number three would be Kubernetes. And you could rank it however you desire. And these 3 applications, in the case of Kubernetes, it's a brand-new type of application where the application is broken up with a small pieces and orchestrated across an entire data center. And because it's broken up into small pieces and orchestrate across the entire data center, the networking between the compute nodes becomes the bottleneck again. And that's the reason why they're doing so well. By increasing the network performance by offloading the communications of the CPUs, you increase the throughput of a data center tremendously. And so it's the reason why they had a record quarter last quarter. It's the reason why they've been growing 27% per year. And their stock was back, their integration into the hyperscale cloud companies, they're low latency, they're incredibly low latency of their link makes them really unique, even whether it's Ethernet or InfiniBand in both cases. And so they're -- it's a really fantastic stack. And then lastly, Cumulus, we would like to integrate -- we would like to innovate in this world where the world is moving away from just a CPU as a compute node. The new computing unit, a software developer is writing a piece of software that runs on the entire data center. In the future, going forward, the computing, the fundamental computing unit is an entire data center. It's so incredible. It's just utterly incredible. You write an application, 1 human could write an application, and it would literally activate an entire data center. And in that world, we would like to be able to innovate from end to end, from networking storage, security. Everything has to be secured in the future so that we can reduce the attack surface down to practically nothing. And so networking storage, security are all completely offloaded, all incredibly low latency, all incredibly high performance and all the way to compute, all the way through the switch. And then the second thing is we'd like to be able to innovate across the entire stack. You know that NVIDIA is just supremely obsessed about software stacks. And the reason for that is because software creates markets. You can't create new markets like we're talking about, whether it's computational health care or autonomous driving or robotic or conversational AI or recommender systems or edge AI. All of that requires software stacks. It takes software to create markets. And so our obsession about software and creating open platforms for the ecosystem and all of our developer partners, Cumulus plays perfectly into that. They are -- they pioneered the open networking stack. And they pioneered, in a lot of ways, software-defined data centers. And so we're super, super excited about the team. And now we have the ability to innovate in a data center scale world from end to end and then from top to bottom of the entire stack. Yes. I appreciate the question. And this, for computer architecture geeks and people who follow history, you know well that in the entire history of time, there are only 2 computing architectures that has made it so far, which is one of them is x86. The other one's ARM in any reasonable way. And if you get an ARM computer, you get an x86 computer, you can program it. And  in fact, there's no such thing as an accelerated computing platform until we came along. And today, we're the only computing -- accelerated computing platform that you could really largely address. We're in every cloud. We're in every computer company. We're in every country. We have every single size, and we accelerate applications from computer graphics to video games to scientific computing to workstations to machine learning to robotics. This journey took 20-some-odd years. Inside our company, it took 20-some-odd years. And we've been focused on accelerated computing since the beginning of our company. And we made a general purpose. We made a general purpose really starting with an endeavor cost Cg, C for graphics, and then it became CUDA. And we've been working on accelerated computing for quite a long time. And I think at this point, it's a foregone conclusion that accelerated computing has reached the tipping point and is well beyond it. The number of developers this year that support -- that we supported was almost 2 million developers around the world, and it's growing what appears to be exponentially. And so I think accelerated computing is now well established. NVIDIA-accelerated computing is well established. It's common sense, and people who are designing data centers expect to put accelerated computing in it. The question is how much? How much accelerated computing do you use? And what part of the date in your pipeline do you do it? And the big -- the gigantic breakthrough, of course, we know well now, and NVIDIA is recognized as one of the 3 pillars that ignited the modern AI, the big bang of modern AI. And the other 2 pillar, of course, is deep learning algorithm and the abundance of data. And so the 3 -- these 3 ingredients came together and was -- people use NVIDIA accelerated computing largely for training. But over time, we expanded training to have a lot more models. And as I mentioned earlier, the single most important model of machine learning today is the recommender system. It's the most important model because it's the only way that you and I could use the Internet in any reasonable way. It's the only way that you and I could use a shopping website or a video web -- a video app or a music app or book or news or anything. And so it is the engine of the Internet from the consumer's perspective. On the company perspective, it is the engine of commerce. Without the recommender system, there's no way they could possibly make money. And so their accuracy in predicting user preferences is core to everything they do. You just go up and down the list of every company. And that engine is gigantic. It is just a gigantic engine. And from the data processing part of it, which is the reason why we went and spent 3 years on Spark and RAPIDS, which made Spark possible and all the work that we did on NVLink and all that stuff was really focused on big data analytics. The second is all of the training of the deep learning models and the inference. So the number of applications, the footprint, the accelerated computing has grown tremendously, and its importance has grown tremendously because of the applications are the most important applications of these companies. And so I think when I mentioned -- when I said that, that acceleration is still growing, it is. But the major workloads, the most important workloads of the world's most important companies are now -- solidly require acceleration. And so I'm looking forward to a really exciting ramp for Ampere for all the reasons that I just mentioned. Okay. Thanks, John, for the question. Let's start from the first perspective on the overall OpEx for the year. We've guided the non-GAAP at approximately $4.1 billion for the year. Yes, that incorporates 3 full quarters of Mellanox, Mellanox and its employees. We have about close to 3,000 Mellanox employees coming on board. You are correct. We have a 53rd week in this quarter -- excuse me, not this quarter, this year. And that is -- has been outlined in SEC filings, and you should expect that as well. We pulled forward a little bit our focal by several months in order to take care of our employees. And then lastly though, we are investing in our business. We see some great opportunities. You've seen some good results from our investment, and there's more to do. We are hiring and investing in those businesses. So there's nothing different structurally, but just this onset of Mellanox and are investing together, I think, we'll produce long-term great results. And as usual, John, you know that we're investing into the IT industry's largest opportunities, cloud computing and AI. And then after these 2 opportunities is edge AI. And so we're looking down the fairway with some pretty extraordinary opportunities. But as usual, we're thoughtful about the rate of investment, and we're well-managed. And NVIDIA's leadership team are excellent managers, and you could count on us to continue to do that. Simona, what was John's question? Could you just give me one hint? I haven't... Oh yes. Right. Yes, right. Yes, right. A few -- some of the industries have been affected. We already mentioned automotive industry. The automotive industry has been grounded to a halt. Manufacturing hasn't largely stopped. And you saw that in our guidance. We expect automotive to be down 40% quarter-to-quarter. It's not going to remain that way. It's going to come back. And nobody knows what level is going to come back to you and how long, but it's going to come back. And there's no question in my mind that the automotive industry, they're hunkered down right now, but they will absolutely invest in the future of autonomous vehicles. They have to, or they'll be extinct. It's not possible not to have autonomous capability in the future of everything that moves. Not so that it could just completely drive without you. That a nice benefit, too. But mostly because of safety and comfort and just a joy of what seems like the car is reading your mind. And of course, you're still responsible for driving it and -- but it just seems to be coasting down the road, reading your mind and helping you. And so I think the future of autonomous vehicles is a certainty. People recognize the incredible economics that the pioneer, Tesla, is enjoying. And the industry is going to go after it. The future car companies are going to be software-defined companies and technology companies. And they would love to have an economic that allows them to enjoy the installed base of their fleets. And so they're going to go after it. And so this is -- I'm certain that this is going to come back. And well, I have every confidence is going to come back. And let's see, the energy sectors are -- have been impacted. The retail sector has been impacted. There's -- those aren't large industries for us. The impact in some of the industries is accelerating their focus in robotics. Like, for example, on the one hand, BMW has obviously impacted in manufacturing, which is the reason why they're moving so rapidly towards robotics. They have to figure out a way to get robotics into their factories. So same thing with retail. You're going to see a lot more robotic support in retail, you're going to see a lot more robotic support in warehouses, in logistics. And so during this time, when the market -- when the industry is disrupted and impacted, it allows the market leaders to really lean into investing into the future. And so when they come back, they'll be coming back stronger than ever. Thank you, Matt. I'm so proud. Yes. Thanks a lot, Matt. I appreciate your questions. I'll go backwards because it's kind of cool. On the one hand, I do miss that we can't engage the developers face to face. It's just so much fun. GTC is doing all their work and the hundreds of papers that are presented, I learned so much each time. And frankly, I really enjoyed the analyst meeting that we have. And so there's all kinds of stuff that I missed about the physical GTC, but here's the amazing thing. We had almost -- the GTC kitchen keynote. I did it from my kitchen just right behind me, and the kitchen keynote has been viewed almost 4 million times. And the video is incredible. And so I think our reach is -- could be quite great. And so I'm not too -- we've got an amazing marketing team, and we just -- we've got great people. They're going to find a way to reach our gamers. And whenever we launch something next, you know that gamers are going to be and our customers are going to be -- our end markets are going to be really excited to see it. And so I'm very confident that we're going to do just fine. Matt, what was the question before? I should never do backwards. I see. Okay. Yes. There's -- we work so closely with Mellanox over the years. And on the day that we announced GTC, you could see the number of products that we have working together. The product synergies are really incredible, and the product synergies include a lot of software development that went in and a lot of architectural development that went in. DGX comes with 9 Mellanox, Matt, as I mentioned. If you look at our data center, we ship -- before we ship DGXs to the customers, we ship it to our own engineers. And the reason for that is because every single product in our company has AI in it. From Jarvis to Metropolis to Merlin to DRIVE to Clara to Isaac to -- right? All of our products has AI in it, and we're accelerating frameworks for all of the AI industry. And Ampere comes with a brand-new numerical format called Tensor Float 32. And TF32 is just a fantastic [medium medical] format and the performance is incredible. And we had to get it integrated in with the industry standard frameworks. And now Tensor Float comes standard with Tensor Flow -- with TF -- NVIDIA TF32, and PyTorch come standard with TF32. And so we need our own large-scale data center. And so the first customer we ship to was ourselves. And then we started shipping as quickly as we could to all of the customers. You saw that in our data center, in our supercomputer. We have 170 -- 170 state-of-the-art, brand-new Mellanox switches. And almost 1,500 -- 200 gigabit per second Mellanox mix. And 15 kilometers of cables, fiber optic cables. And that is one of the most powerful supercomputers in the world today, and it's based on Ampere. And so we have a great deal of work that we did there together. We announced our first edge computer between us and Mellanox in this new card, we call it the EGX A100. It integrates Ampere and it integrates Mellanox' CX-6 Dx, which is designed for 5G telcos and edge computing. It's incredible security and has a single route of trust, and it's virtualized. And so basically, we -- this EGX A100, when you put it into a standard center x86 server, turns that server into a cloud computer in a box. The entire capability of a cloud, of a state-of-the-art cloud, which is cloud native, it's secure, it has incredible AI processing, it's now completely hyperconverged inside 1 box. The technology that made EGX A100 is really quite remarkable. And so you could see all the different product synergies that we have in working together. We could have done Spark acceleration without the collaboration with Mellanox. They worked on this piece of networking software called UCX. We worked on [nickel] together. It made possible the infrastructure for large-scale distributor computing. I mean just the list goes on and on and on. And so we -- the 2 teams have great chemistry. The culture -- it's a great culture fit. I love working with them. And right out of the chute, you saw all of the great product synergies that are made possible because of the combination. It's coming. Let me just -- thank you. We had a great and busy quarter. With our announcements, we highlighted several initiatives. First, computing is moving to data center scale where computing and networking go hand in hand. The acquisition of Mellanox gives us deep expertise and scale to innovate from end to end. Second, AI is the most powerful technology force of our time. Our Ampere generation offers several breakthroughs. It is the largest ever generational leap 20x in training and inference throughput; the first unified acceleration platform for data analytics, machine learning, deep learning, training and inference; and the first elastic accelerator that can be configured for scale-up applications like training to scale-out applications like inference. Ampere is fast, it's universal and it's elastic. It's going to re-architect the modern data center. Third, we are opening large new markets with AI software application framework, such as Clara for health care, DRIVE for autonomous vehicles, Isaac for robotics, Jarvis for conversational AI, Metropolis for edge IoT, AERIAL for 5G and Merlin with the very important recommender systems. And then finally, we have built up multiple engines of accelerated computing growth. RTX computer graphics, artificial intelligence, and data center scale computing from cloud to edge. I look forward to updating you on our progress next quarter. Thanks, everybody.","Congratulations on a solid quarter. Colette, I'm curious of your commentary around visibility in the data center side, that that's comments over the last couple of quarters, how would you characterize your visibility today relative to maybe what it was last quarter? And how do we think about the visibility in the context of trends maybe into the back half of the calendar year. I first wanted to follow-up on your gaming commentary. You sort of mentioned a couple of offsets. COVID potentially still a headwind, e-tail or tailwind and maybe offsetting each other. Were you trying to suggest that those did offset completely and gaming was kind of flattish into Q2? Because I know it has a typical seasonal pattern switches typically up. I guess what were you trying to say with those kind of factors? And what are the kinds of things we should be thinking about when it comes to seasonality, Colette, into Q2 around that business segment? Yes. That's right. Yes. I guess just to follow up on that, though, if it's growing. I mean like in prior years, we've seen it grow like very strong double digits. Obviously, the mix of the business was different back then. But do you think that the kind of -- I mean are we thinking kind of it's up somewhat? You don't -- is there any chance that it could be up like on -- for what we've seen in terms of like typical levels in the past? Like can you give us any sense of magnitude, that would be really helpful? Yes. That's very helpful. I guess if I could ask 2. Colette, can you help us with what you think the growth rate for Mellanox could look like in calendar '20? And then Jensen, a bigger picture question for you and really not specific to health care, more broad-based. But how do you think about the long-lasting impact of COVID on worldwide demand for AI? No, sorry about that. I'm curious if you could provide a little handholding on what we should think about for growth for Mellanox in calendar '20? I had one for Colette and then one for Jensen as well, if I may. Colette, I wanted to come back to the gross margin question. You're guiding July essentially flat sequentially, despite what I'm guessing is better mixed with non-ops coming in and automotive guided down 40% sequentially. I guess the question is, what are some of the offsets that are pulling down gross margins in the current quarter? And sort of related to that, how should we be thinking about the cadence and OpEx going forward, given the 6-month pull in that you guys talked about on the compensation side? And then one quick one for Jensen. I was hoping you could comment on the current trade landscape between the U.S. and China. I feel like you guys shouldn't be impacted in a material way directly nor indirectly. But at the same time, given the critical role you play in scientific computing, I can sort of see a scenario where some people may claim that you guys contribute to efforts outside of the U.S. So if you can kind of speak on that -- speak to that, that will be helpful. A question coming back to the A100. I'm trying to understand how this kind of fits into the evolution of your solution set over time and the evolution of the demand for the applications. Is -- and I guess if I think about it going back, you had a solution, which is largely training based. And then you kind of introduced solutions that were targeted more inferencing. And now you have a solution, it sounds to my understanding that it solves both inferencing and training efficiently. And so I guess I'm wondering is 3 years, 5 years, 10 years down the line, is this part of the kind of general purpose computing or acceleration framework that you had talked about in the past, Jensen, where Ampere is kind of like an Ampere-class product? Or is this -- would you still -- should we still expect to see inferencing-specific solutions in the market and then training-specific solutions and then an Ampere solution for a different class application? If you could provide a framework for thinking about Ampere in those context, I think that would be helpful. Actually I had 2, I guess, Jensen, first for you. Just on the data center business, things have been very strong recently. Obviously, there's always concerns that customers are pulling in CapEx, but it sounds like you have pretty good visibility into July. But I guess last time, most folks also thought that your kind of attrition really was so low that you would be immune into any digestion, but that wasn't the case. So I guess I'm wondering, if things are different now with A100 and whatnot, but my question is how do you handicap your ability to this time, maybe get through any digestion on the CapEx side? And then I guess, second question, Colette, stock comp had been running like 220 a quarter, and the guidance implies that it goes to like 460 a quarter. So it goes up a lot. Is that all executive retention? And is that sort of the right level as you look into 2021? Jensen, the team has showed the importance of networking, networking fabric and the Mellanox acquisition, like, for example, when you guys move from Volta DGX-1 to Volta DGX-2, you guys didn't change the GPU chipset. But by adding a custom networking fabric chip and more Mellanox network interface cards, among other things, you guys drove a pretty significant improvement in performance per GPU. But now when we think about scaling out compute acceleration to data center skilled implementation, how does Mellanox' Ethernet switching platforms differ from those provided by other large networking OEMs, some of whom have been your long-term partners? And then how does the Cumulus acquisition fit into the switching and networking strategy as well? Jensen, I'd like to focus on something you said. I think it was in one of your earlier responses, you said something about a very significant part of data centers are now accelerated with GPUs. I'm sort of curious how to interpret that. If we think about sort of the evolution of compute architecture going from almost entirely, let's say, [REX and REXs] CPUs to some future day where we have many more accelerators and maybe a much smaller number of CPUs relative to those. Maybe you can talk to us about where we are in terms of that architectural shift and where you think it goes sort of longer term, where we are in the position of that? Just 2 quick ones. Colette, I hate to ask something as mundane as OpEx, but just given the full year guide, there's sort of a lot to unpack. And you talked about some of it like the raises. I mean I think you also probably have some COVID plus or minuses in that. I think there's an extra week this year as well. And then, of course, there's Mellanox and how you're thinking about investing in that asset. I guess I'm just kind of curious, when we look at the full year guide, is there something structural going on OpEx as you try to take advantage of all these opportunities? Or can we use it as sort of a guidepost to how you're thinking about revenue for the back half of the year as well? How do I understand that? And then, Jensen, just a quick one for you. It kind of makes sense to me that COVID is accelerating activity in sort of HPC and hyperscale and maybe even in certain verticals like health care. But in the other verticals, has the sort of shelter in place kind of hurt engagement? And could we actually come out of COVID with some pent-up demand in those vertical markets? Just the idea of engagement levels in verticals, just with shelter in place. Has that hampered... Two different topics, Jensen. Well, first of all, congrats on Ampere. It's a heck of a product. The first question... The first question is it might have been a little bit hard to talk when the deal was pending about this topic, but now that it's closed, maybe you could talk a little bit about opportunities to innovate on and customize the Mellanox stack and the balance of having an industry standard. And the second one is E3 canceled, Computex moved around. At the same time, there's obviously stay-at-home gaming demand. Just how you think about gaming product, launch logistics? And any comments on there would be really helpful. Just the industry standard versus customization of Mellanox opportunity. PRELIMINARY TRANSCRIPT: ""Preliminary Transcript"" indicates that the Transcript has been published in near real-time by an experienced professional transcriber.  While the Preliminary Transcript is highly accurate, it has not been edited to ensure the entire transcription represents a verbatim report of the call. EDITED TRANSCRIPT: ""Edited Transcript"" indicates that a team of professional editors have listened to the event a second time to confirm that the content of the call has been transcribed accurately and in full. information on this web site without obligation to notify any person of such changes. In the conference calls upon which Event Transcripts are based, companies may make projections or other forward-looking statements regarding a variety of items. Such forward-looking statements are based upon current expectations and involve risks and uncertainties. Actual results may differ materially from those stated in any forward-looking statement based on a number of important factors and risks, which are more specifically identified in the companies' most recent SEC filings. Although the companies may indicate and believe that the assumptions underlying the forward-looking statements are reasonable, any of the assumptions could prove inaccurate or incorrect and, therefore, there can be no assurance that the results contemplated in the forward-looking statements will be realized. THE INFORMATION CONTAINED IN EVENT TRANSCRIPTS IS A TEXTUAL REPRESENTATION OF THE APPLICABLE COMPANY'S CONFERENCE CALL AND WHILE EFFORTS ARE MADE TO PROVIDE AN ACCURATE TRANSCRIPTION, THERE MAY BE MATERIAL ERRORS, OMISSIONS, OR INACCURACIES IN THE REPORTING OF THE SUBSTANCE OF THE CONFERENCE CALLS. IN NO WAY DOES THOMSON REUTERS OR THE APPLICABLE COMPANY ASSUME ANY RESPONSIBILITY FOR ANY INVESTMENT OR OTHER DECISIONS MADE BASED UPON THE INFORMATION PROVIDED ON THIS WEB SITE OR IN ANY EVENT TRANSCRIPT. USERS ARE ADVISED TO REVIEW THE APPLICABLE COMPANY'S CONFERENCE CALL ITSELF AND THE APPLICABLE COMPANY'S SEC FILINGS BEFORE MAKING ANY INVESTMENT OR OTHER DECISIONS."
